{"id": "Xxw1L7H3fz", "number": 22675, "cdate": 1758334357756, "mdate": 1759896852975, "content": {"title": "CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency", "abstract": "Large language models (LLMs) are often queried multiple times at test time, with predictions aggregated by majority vote. While effective, this self-consistency strategy (Wang et al., 2023) requires a fixed number of calls and fails when the correct answer is infrequent. We introduce Confidence-Guided Early Stopping (CGES), a Bayesian framework that forms posteriors over candidate answers from scalar confidence signals—derived from token probabilities or reward models—and adaptively halts sampling once posterior mass exceeds a threshold. We provide theoretical guarantees in both the ideal case of perfectly calibrated confidences and the realistic regime with noisy confidences. Averaged over five reasoning benchmarks, CGES reduces the average number of calls by 69.4% (e.g., from 16.0 to 4.9) while maintaining accuracy within 0.06 percentage points of self-consistency.", "tldr": "", "keywords": ["Large Language Models", "Self-Consistency", "Test-Time Scaling", "Efficiency", "Reasoning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3841b36eaa9b72e93143fa36456f9bf322cb9603.pdf", "supplementary_material": "/attachment/ee03354ea9eaccf08d5316e993ee66cebb652c56.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Confidence-Guided Early Stopping (CGES), a Bayesian framework for aggregating multiple sampled reasoning trajectories in self-consistency (SC) inference of large language models. Instead of relying on a fixed number of samples or heuristic early stopping rules, CGES models the likelihood of each candidate answer given confidence scores of sampled trajectories and terminates sampling once the posterior probability of any candidate exceeds a threshold (\\gamma). The paper establishes consistency guarantees for CGES in both ideal confidence calibration settings and more realistic noisy conditions. Empirical evaluations across multiple reasoning benchmarks (e.g., AIME24, MATH500, GSM8K, MMLU-Pro, GPQA) demonstrate that CGES can notably reduce the number of calls to the base model while maintaining comparable accuracy compared to standard Self-Consistency. Experiments also show the compatibility of CGES with different confidence estimators (token-level, MARS, reward-based PRMs)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**\n\n* The paper formulates SC aggregation and early stopping within a unified Bayesian perspective, which is theoretically grounded and conceptually elegant.\n* The theoretical analysis addresses both ideal and noisy confidence regimes, contributing rigorously to the understanding of test-time scaling.\n\n**Quality**\n\n* The overall methodology is well-motivated with mathematically sound derivations.\n* Experiments span multiple benchmarks, models, and confidence sources, providing evidence of generality."}, "weaknesses": {"value": "1. **Missing comparison to strong baselines.**\n   This paper lacks comparison with following strong baselines:\n\n   * *Let’s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs* (EMNLP2023)\n   * *Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning* (NAACL 2025)\n     \n     Without comparison against ASC and DSC, it is unclear how CGES improves over state-of-the-art adaptive SC methods.\n\n2. **Insufficient robustness evaluation.**\n   Results are averaged over only three random seeds. SC sampling can be performed in an **offline** manner: generate ≥100 trajectories once, then repeatedly subsample (K) trajectories (M) times (with large (M)) to estimate robustness. This is standard and statistically stronger. Three seeds do not adequately characterize variance.\n\n3. **No wall-clock latency analysis.**\n   Although sample count is reduced, the overall inference latency may not improve due to:\n\n   * posterior update overhead,\n   * confidence extraction,\n   * external reward model evaluation (e.g., PRM-72B).\n     End-to-end latency is essential for cost-effectiveness claims.\n\n4. **High dependence on confidence calibration.**\n   Miscalibration is common in math reasoning tasks. The framework is sensitive to over- or under-confident trajectories, potentially causing premature or delayed stopping. The paper lacks sensitivity analysis.\n\n5. **No ablations on the stopping threshold (\\gamma).**\n   (\\gamma) directly controls the accuracy-efficiency trade-off. Practitioners need guidance on how to choose it. Robustness to (\\gamma) is not evaluated."}, "questions": {"value": "1. Can the authors compare CGES to ASC and DSC [1,2] to position the method against the strongest adaptive SC baselines?\n\n2. Why report only three seeds when SC sampling can be replayed offline? Can the authors:\n\n   * oversample more trajectories,\n   * simulate many (large (M)) independent trials by random subsampling,\n   * report confidence intervals?\n\n3. Can the authors report wall-clock latency (not just sample count) for:\n\n   * base sampling,\n   * Bayesian aggregation overhead,\n   * reward-model scoring?\n\n4. Can the authors provide ablations sweeping (\\gamma) (accuracy vs. sample count vs. stopping time)?\n\n5. How robust is CGES to miscalibrated confidence? Can synthetic noise injection experiments be provided?\n\n6. Other related work that should be cited and discussed: [3-5].\n\n\n[1] Let’s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\n\n[2] Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning\n\n[3] Scaling test-time compute with open models. Arxiv 2024\n\n[4] Inference scaling laws: An empirical analysis of compute-optimal inference for llm problem-solving. ICLR 2025\n\n[5] Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling. NeurIPS 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bywwB25vcD", "forum": "Xxw1L7H3fz", "replyto": "Xxw1L7H3fz", "signatures": ["ICLR.cc/2026/Conference/Submission22675/Reviewer_5UEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22675/Reviewer_5UEf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721182402, "cdate": 1761721182402, "tmdate": 1762942330558, "mdate": 1762942330558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new approach for test-time scaling and self-consistency that leverages a confidence measure to generate fewer candidate responses. The idea is to use Bayesian inference to compute the posterior probability of the next candidate sequence and stop the sampling process as soon as the posterior probability is below a given threshold. The experimental evaluation is carried out on standard benchmark datasets, using open models. The results demonstrate improved performance compared to existing self-consistency approaches, namely the proposed approach generate fewer candidate responses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Test-time scaling and self-consistency for LLMs are topics that received considerable interest recently and therefore advances in this area are definitely warranted.\n\n- The paper a simple yet efficient self-consistency scheme that leverages a combination of confidence estimation and Bayesian probabilistic inference to rank and generate fewer candidate responses compared to existing self-consistency methods."}, "weaknesses": {"value": "- I was not able to find any major weaknesses with the paper."}, "questions": {"value": "No questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HhxNqL4qsm", "forum": "Xxw1L7H3fz", "replyto": "Xxw1L7H3fz", "signatures": ["ICLR.cc/2026/Conference/Submission22675/Reviewer_er2g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22675/Reviewer_er2g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941157985, "cdate": 1761941157985, "tmdate": 1762942330302, "mdate": 1762942330302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an approach that is generally based on the idea of leveraging consistency – something that is very popular in test-time scaling for LLMs and for confidence estimation. The approach is called confidence-guided early stopping (CGES) and is primarily a particular Bayesian formulation that relies on assumptions about responses and confidences from a confidence estimation module. The main method that is based on the formulation is quite simple and outlined via Algorithms 1 and 2. Experiments are performed on some mathematical and reasoning datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I find the overall idea of using confidences of LLM responses to inform test-time scaling useful. This is of course not a new idea by any means, but this paper proposes a scoring approach that uses a particular Bayesian formulation that effectively aggregates confidences from samples. The idea of minimizing the number of samples for test-time scaling is also useful."}, "weaknesses": {"value": "A major weakness of the paper is the problematic formulation. I have several issues about the assumptions of the paper and find them to be problematic. The paper gives the air of theoretical basis/justification for the CGES approach, but from my understanding, some of the assumptions do not make sense and the formulation feels flawed. I will describe some concerns in my detailed comments. \n\nAnother weakness is the lack of sufficient literature around confidence estimation in general. Confidences have been used in test-time scaling in prior work. I will highlight a few references for the authors to consider citing in their revised work."}, "questions": {"value": "Here are some additional comments and questions:\n\nI have several questions and concerns about Section 3. Please let me know if I have misunderstood anything:\n\n-\tConfidence is never fully explained or defined. I assume it is a score between 0 and 1 that represents a score for correctness of an answer. \n-\tThe authors assume that the correct answer is within the candidate set -- this is clearly not always true in practice and is an absurd assumption. In fact, it is common to have cases where there is no correct answer, particularly when there is high epistemic uncertainty. What are the ramifications if this assumption is not true?\n-\tPlease clarify that R refers to the textual response. Also, clarify what exactly P(R|…) means – based on the description, I understand this to be a product of conditional probabilities for tokens in the response, so this is likely to be a small number.\n-\tWhy does P(R|…) depend on A and I? If this is the likelihood of the response and generation is from an LLM, shouldn’t it depend only on Q? This did not make any sense to me.\n-\tHow are the samples generated? What temperature?\n-\tIt is unclear to me if Assumption 1 is necessarily true. Any response is a sequence of tokens, so it is not obvious that these are i.i.d. My guess is that they may not be i.i.d. due to potentially varying lengths. Confidences are also likely not i.i.d., and if they are, it depends on how they are obtained. Not enough is mentioned about this assumption.\n-\tAssumption 2 depends on the sampling procedure.\n-\tAssumption 3 does not make sense to me.\n-\tAssumption 4 is not true – the entire point of confidence is that it should be higher when the answer is correct. \n-\tThe equation in line 234 can’t be true. Confidence should be higher than the probability of a response, which might be a very small number due to multiplication of probabilities.\n-\tIt is not sufficient to just make assumptions – you need to defend them.\n\nI find lines 113 and 114 to be somewhat misleading – the theoretical guarantees don’t matter if the formulation is flawed or if the assumptions don’t make sense.\n\nWhy are only 7B models used for experiments? Only 1 model per dataset is not sufficient as demonstration of value, in my view. Also, why were these datasets chosen for the experiments?\n\nI recommend looking at other methods to estimate confidence (see some references below).\n\nSince this paper discusses the notion of self-consistency in various places, I suggest the authors cite more papers that perform uncertainty or confidence estimation using consistency-based methods. The following paper provides an empirical justification for such methods: https://arxiv.org/abs/2506.21849. \n\nAlso, the following paper can be useful for modeling probability distributions of responses: https://arxiv.org/abs/2406.02543. \n\nThe following paper reviews various confidence estimation approaches and could be good to cite: https://aclanthology.org/2024.naacl-long.366/. I recommend citing many more consistency-based papers in general, like some mentioned here. \n\nThe following paper has experimented with using confidences for test-time scaling: https://www.arxiv.org/abs/2510.13836. \n\nThe authors claim that a core part of their work is “uncertainty estimation”. I believe “confidence estimation” is actually what they rely on – see this paper for appreciating the difference: https://arxiv.org/abs/2305.19187."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BshytngQtc", "forum": "Xxw1L7H3fz", "replyto": "Xxw1L7H3fz", "signatures": ["ICLR.cc/2026/Conference/Submission22675/Reviewer_efJq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22675/Reviewer_efJq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952022568, "cdate": 1761952022568, "tmdate": 1762942330119, "mdate": 1762942330119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new method to adaptively stop sampling outputs from LLMs for Self-consistency. The method uses both the output and any signal for the output’s confidence to maintain a posterior distribution over the correct answer. The sampling is stopped once the probability of some answer in the posterior reaches a threshold. Also, the final answer is chosen as the mode of the posterior instead of just the most frequent answer. Experiments show drastic decrease in sampling cost and minor change in accuracy compared to Self-consistency."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very well written and does an excellent job explaining the concepts in an accurate and understandable language. I really enjoyed the formal statements for the assumption and the graphical model. \n\nThe idea to use both the output and an auxiliary signal is novel, and allows the method to utilize the progress in uncertainty quantification of LLMs in the future. \n\nThe experiments are diverse and extensive enough for confident evaluations. The method performs very well in terms of cost."}, "weaknesses": {"value": "I think the main weakness of the method is that we need to know at least a tight upper bound on the number of final answers. It works well for multiple choices questions, but I expect it to be more challenging in short answer questions where no predefined set of answers is known. I suspect using a loose upper bounds as the upper can drastically hinder the efficiency gains and force the algorithm for the non-existent answers to appear. I could not find the methodology used for the MATH dataset and would appreciate some details. \n\nThe paper is missing a critical similar work and baseline for early stopping in majority voting [1]. I think this other work provide a methodology for unknown number of final answers. The submission is different due to use of an external signal and the change in the selection mechanism. Nonetheless, I think this paper should be added as a baseline. \n\nMinor suggestion in writing is to provide an example of $C_t$ before the assumption statements. Without an example, what $C_t$ is was not clear, and assessing the assumptions was too difficult as a reader. The example for ideal case might come from an oracle with unrealistic information. \n\nIn contrast to the paper, I think a larger reward function can be considered realistic as the cost of reward calculations is just a forward pass of the model but the generation of outputs requires more compute. \n\n[1] [Let’s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs](https://aclanthology.org/2023.emnlp-main.761/) (Aggarwal et al., EMNLP 2023)"}, "questions": {"value": "Please provide more details on handling datasets with unknown number of final answers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GW4ZbwIUL1", "forum": "Xxw1L7H3fz", "replyto": "Xxw1L7H3fz", "signatures": ["ICLR.cc/2026/Conference/Submission22675/Reviewer_8J5r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22675/Reviewer_8J5r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762331188621, "cdate": 1762331188621, "tmdate": 1762942329853, "mdate": 1762942329853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}