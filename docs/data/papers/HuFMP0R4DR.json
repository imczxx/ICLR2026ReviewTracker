{"id": "HuFMP0R4DR", "number": 21237, "cdate": 1758315264716, "mdate": 1759896932832, "content": {"title": "QueST: Incentivizing LLMs to Generate Difficult Problems", "abstract": "Large Language Models have achieved strong performance on reasoning tasks, solving competition-level coding and math problems. However, their scalability is limited by human-labeled datasets and the lack of large-scale, challenging coding problem training data. Existing competitive coding datasets contain only thousands to tens of thousands of problems. Previous synthetic data generation methods rely on either augmenting existing instruction datasets or selecting challenging problems from human-labeled data. In this paper, we propose QueST, a novel framework which combines difficulty-aware graph sampling for prompt and difficulty-aware rejection fine-tuning that directly optimizes specialized generators to create challenging coding problems. Our trained generators demonstrate superior capability at creating challenging problems compared to even proprietary models such as GPT-4o. We leverage this method to generate large-scale synthetic coding problems, which we then use to distill from long Chain-of-Thought (CoT) models or conduct reinforcement learning for smaller models, proving effective in both scenarios. Our distilled model achieves the best performance compared to similarly sized models trained on previous long CoT SFT datasets. By training generators to create more difficult problems, QueST pushes the boundaries of reasoning abilities in large language models.", "tldr": "", "keywords": ["reasoning", "code generation", "large language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ca9c907fa774b034ed2affa2c86c0844fde5962.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Large language models show strong capabilities in complex reasoning tasks. Training with difficult problems can further improve the model's performance. Based on this, the authors proposed a data synthesis method to generate more challenging problems. Specifically, they combined difficulty-aware graph sampling for prompts and difficulty-aware rejection fine-tuning to create high-difficulty training data. Training the model with this synthetic data led to some improvement in its performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The authors generated a set of training data related to coding tasks, totaling 100,000 examples.\n\n+ The authors trained the model using the synthetic data, which led to improvements."}, "weaknesses": {"value": "+ In Section 3.1 of the paper, the data synthesis process requires a large number of external models for assistance, but the authors do not explain the strategy for selecting these models, which makes it unconvincing.\n\n+ The authors cannot explain the reason for the low accuracy in the synthetic problems. On one hand, it may be due to higher difficulty, and on the other hand, it may be because the answers contain errors.\n\n+ How well does the problem generator perform on coding tasks? The paper does not explain this. If the trained model cannot outperform the problem generator, then the synthetic data is meaningless, especially since the authors claim in the abstract that \"QueST pushes the boundaries of reasoning abilities in large language models.\""}, "questions": {"value": "+ Please explain the selection strategy for the various models in Section 3.1.\n\n+ Please provide the performance of the problem generator on coding tasks.\n\n+ Why not use OCR-Qwen-7B-Instruct as the backbone model? It has stronger reasoning capabilities.\n\n+ How does QueST perform compared to TACO when the data volume is the same? If QueST cannot significantly outperform TACO, it cannot be concluded that QueST's problems are more difficult or of higher quality.\n\n+ The performance of the three models with RL in Table 4 shows no significant improvement and is unstable. Does this indicate that the quality of QueST's data is normal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cCaahsgRFk", "forum": "HuFMP0R4DR", "replyto": "HuFMP0R4DR", "signatures": ["ICLR.cc/2026/Conference/Submission21237/Reviewer_HaRZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21237/Reviewer_HaRZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760935519091, "cdate": 1760935519091, "tmdate": 1762941651040, "mdate": 1762941651040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a difficulty-aware framework for generating coding problems, integrating difficulty-guided graph sampling with rejection-based fine-tuning. Additionally, the authors release a 100K dataset, which contributes valuable resources for future research and community use."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Difficulty Estimation via Self-Consistency: The authors estimate problem difficulty using self-consistency across multiple model outputs.\n2. Difficulty-Guided Sampling: For each prompt, multiple candidate problems are generated, and only the most difficult one (based on the proposed difficulty metric) is retained for training.\n3. Instead of letting the model generate simple problems repeatedly, the method continuously selects and trains on the most challenging problems, thus enhancing the generator’s reasoning and problem-design ability. A “difficulty label” is introduced such that concept pairs frequently co-occurring in hard problems are more likely to be sampled together during generation."}, "weaknesses": {"value": "1. The paper does not introduce a fundamentally new data synthesis approach but rather extends MathScale with heuristic sampling improvements based on self-consistency. Such heuristics are intuitive but may not lead to substantial long-term impact.\n\n2. The proposed method for measuring problem difficulty mainly relies on self-consistency within rollouts, which appears heuristic and lacks deeper theoretical justification to confirm its validity.\n\n3.The synthetic data do not show clear superiority in experimental results: for example, in Table 3, QueST-100K-7B underperforms OCR-Qwen-7B-Instruct. The best results rely on Qwen3-8B, which also raises concerns about fairness and comparability due to missing baselines.\n\n4. Although the method does not require human annotation, it still depends on TACO as the seed dataset, meaning it is not a fully self-synthesized approach."}, "questions": {"value": "In lines 118–120, how are semantically similar but different concepts handled? Given that generated concepts may exhibit such redundancy, could this affect sampling consistency or downstream results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0OEYItH15A", "forum": "HuFMP0R4DR", "replyto": "HuFMP0R4DR", "signatures": ["ICLR.cc/2026/Conference/Submission21237/Reviewer_jBTM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21237/Reviewer_jBTM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466963965, "cdate": 1761466963965, "tmdate": 1762941650693, "mdate": 1762941650693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a difficulty-aware coding problem generation framework, as well as the corresponding dataset (100K examples). In particular, a teacher / problem generator is trained to produce hard coding problems via (i) a difficulty-aware concept graph and (ii) rejection FT that keeps the hardest-of-K candidates according to a solver-disagreement proxy. Training a student model on the resulting data yields some improvement on hard and average performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper proposes training a specialized teacher / problem generator model, rather than prompting a fixed model, to create synthetic code data.\n2) The paper includes interesting ablations that anticipate and answer likely reader questions (e.g., Table 3)."}, "weaknesses": {"value": "1) The primary novelty of the framework arises from the fine-tuned teacher. Yet, Table 5 shows that the trained teacher performs roughly the same as a fixed teacher (gpt-4o), so the added value (and claimed flexibility) is unclear. Without gains from training a teacher model, the rest of the pipeline/framework  (concept extraction, synthetic data generation, and filtering), largely mirrors prior works. \n2) The magnitude of improvement seems small / possibly noisy. In Table 4, RL improves average performance marginally mostly via hard (easy + medium dip in performance when compared to TACO-RL).\n3) The paper is missing references/comparisons to other synthetic data generation pipelines that make use of concept extraction (e.g., see the following line of work: https://arxiv.org/abs/2405.12205, https://arxiv.org/abs/2407.21009, https://arxiv.org/abs/2408.14774)"}, "questions": {"value": "1) In Table 5, what is the baseline performance for the Qwen model? (In other words, the first row for the gpt-4o as problem generator, but with qwen as the generator.)\n2) Teacher and student models are often from the same model family (and even architecture). Could the authors comment on cross-family performance, and how gains reflect the benefits of the framework over the in-family inductive bias? \n3) The framework uses an LLM to extract each concept c. How sensitive is the framework to the use of different LLMs for concept extraction?\n4) How does the difficulty metric ($\\delta$) correlate with human rated difficulty (even on a small sample of the framework data/pipeline)? TACO (the seed dataset) also has human-annotated difficulty ratings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gkQqj16ZjX", "forum": "HuFMP0R4DR", "replyto": "HuFMP0R4DR", "signatures": ["ICLR.cc/2026/Conference/Submission21237/Reviewer_CXvL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21237/Reviewer_CXvL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682749706, "cdate": 1761682749706, "tmdate": 1762941650365, "mdate": 1762941650365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes QueST, a framework to train problem-generating LLMs that produce hard competitive-programming questions at scale. Two core ideas:\n\n1) a difficulty-aware concept graph that biases prompt construction toward concept pairs associated with higher human difficulty labels in the seed set (TACO), implemented by blending co-occurrence with average difficulty in the edge weights (Eq. 10; α=0.2) ; and\n\n2) difficulty-aware rejection fine-tuning (RFT) that, for each prompt, samples multiple questions and keeps the hardest one under a difficulty proxy δ defined as one minus the average majority-vote rate across M candidate solutions over T test inputs (Eq. 6) .\n\nQueST uses GPT-4o to generate test inputs and candidate solutions while computing δ, filters low-quality cases, and trains a specialized generator (Qwen2.5-14B-Instruct) with RFT. The resulting synthetic questions (20k–100k) are paired with long-CoT solutions from Qwen3-8B to SFT smaller students (Qwen2.5-Coder-7B-Instruct, Qwen3-8B-Base), and also support RLVR training (GRPO). On LiveCodeBench-V5 and USACO, QueST variants outperform baselines of similar size, with the 8B student showing the strongest average gains (Table 3)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Clear, modular pipeline with explicit math and algorithms. The $\\delta$ metric and RFT selection are precisely defined (Eqs. 5–9), with a practical filtering step for invalid executions\n\n2) The edge-weighting scheme combining co-occurrence and average difficulty is easy to implement and justified by seed annotations in TACO\n\n3) Training on QueST-generated data with Qwen3-8B as teacher matches or exceeds prior SFT datasets that used larger/stronger teachers, especially on harder USACO levels (Table 3)\n\n4) Contamination check reported. 50-gram Jaccard similarity is 0 across used datasets/benchmarks (per their method), which is appreciated"}, "weaknesses": {"value": "1) Difficulty proxy $\\delta$ may conflate “hardness” with generator/judge idiosyncrasies; causal link not isolated.\n\n2) The paper does not evaluate $\\delta$ stability across different judge models.\n\n3) The paper itself cites difficulty-aware/rejection methods and concept-graph generation (e.g., MathScale; DART-math; \"weakness-driven\" synthesis) in related work; QueST’s novelty is the combination plus code-specific engineering, not the first instance of difficulty-aware synthetic problem generation (Sec. 4)\n\n4) Table 3 shows the 7B model trained on QueST-100K is slightly below OCR-Qwen-7B-Instruct on LiveCodeBench Avg (43.3 vs 51.3), though it performs relatively better on USACO-Hard. The narrative should be more calibrated: QueST helps, but it's not uniformly superior to the best existing 7B post-training recipe."}, "questions": {"value": "1) Does $\\delta$  correlate with human difficulty? Please report Spearman/Pearson between $\\delta$  and TACO’s human-labeled difficulty on held-out problems, and across judge models\n\n2) How do results change with different M (number of solutions), T (test inputs), temperature, and different solvers for computing $\\delta$?\n\n3) Can you provide a small human or symbolic audit to estimate the precision of high-$\\delta$ items (i.e., % well-posed with correct reference outputs)? Even 100 sampled problems would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WLFYehObHY", "forum": "HuFMP0R4DR", "replyto": "HuFMP0R4DR", "signatures": ["ICLR.cc/2026/Conference/Submission21237/Reviewer_vYVu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21237/Reviewer_vYVu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762122728444, "cdate": 1762122728444, "tmdate": 1762941650088, "mdate": 1762941650088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}