{"id": "YPNDGGgByQ", "number": 25080, "cdate": 1758363859694, "mdate": 1759896734968, "content": {"title": "Prototype Transformer: Towards Language Model Architectures Interpretable by Design", "abstract": "While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. \"woman\") during training. They provide the potential to interpret the model's reasoning and execute targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability.\nIn terms of computation scalability, ProtoT scales linearly with sequence length vs the quadratic scalability of SOTA self-attention transformers. Compared to baselines, ProtoT scales well with model and data size, and achieves good performance on downstream benchmarks (GLUE). ProtoT exhibits robustness to input perturbations on par or better than some baselines, but differs from them by providing interpretable pathways showing how robustness and sensitivity arises. Reaching close to the performance of state-of-the-art architectures, ProtoT paves the way towards creating well-performing autoregressive LMs interpretable by design.", "tldr": "We introduce ProtoT, a linear-compute prototype-based alternative to transformer LMs that forms nameable concepts via two-way sequence-prototype communication, enabling interpretability, targeted edits, and competitive performance and robustness.", "keywords": ["Prototype Transformer (ProtoT); prototype-based language models; interpretable reasoning; nameable concept discovery; targeted model editing; linear-time sequence modelling; transformer alternatives; robustness to input perturbations; causal effects; autoregressive LMs; language models; fine-tuning; downstream performance"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/53f9b928e7fb1645a1306bf3d65a33a5428c6a40.pdf", "supplementary_material": "/attachment/c5ae20d6671b963c78f508462bfb93fd560bf238.zip"}, "replies": [{"content": {"summary": {"value": "The presented paper proposes a transformer variant that integrates prototypes inspired from the prototype learning done in computer vision with the aim to build an LLM architecture that is more explainable by design than regular attention."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting approach to combine prototype learning from the computer vision space with modern LLM architectures\n- Insightful analysis on noise robustness of the prototypes\n- Competitive performance of the introduced architecture with linear computational complexity compared to standard attention-based architectures"}, "weaknesses": {"value": "###  Weaknesses\n\n- The intervention approach to the prototypes is not much different to what has been done to specific translation heads to determine their function e.g. there exist works that have specifically identified specific \"translation heads\" for multilingual mappings and especially crucial for the translation task. Similarly to the \"male\" and \"female\" prototypes, one could analyze translation heads in a similar fashion and we do not necessarily need the prototypes.\n- To understand the role of each prototype one needs to have a reasonable hypothesis and probing dataset e.g. as done in the paper, there should be a prototype somewhere that encodes the gender (male/female) attributes. To me this seems rather difficult to distill for a large number of prototypes and might also have issues with confirmation bias. In an ideal world, we would have some sort of \"prototypical\" visualization similar to how it is done in computer vision to assess each prototypes function more holistically. \n- I found the interpretability portion of the paper to be rather short and would've hoped for a more thorough discussion of the benefits compared to similar masking experiments on attention heads and/or more findings distilled from the prototypes. Appendix A. 8 is interesting as we can see that `L7 P31` seems to be closely related to diseases or `L10 P8` seems to be related to germany.\n\n### Minor Comments\n\n- l. 152: `They are R parameter vector` -> `There are ...`"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ybRczly9JC", "forum": "YPNDGGgByQ", "replyto": "YPNDGGgByQ", "signatures": ["ICLR.cc/2026/Conference/Submission25080/Reviewer_UAxr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25080/Reviewer_UAxr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761373828456, "cdate": 1761373828456, "tmdate": 1762943318488, "mdate": 1762943318488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Prototype Transformer (ProtoT), an autoregressive language model that replaces standard self-attention with a prototype-based mixer. Each layer contains a set of learnable prototype vectors that communicate bidirectionally with token embeddings through “read” and “write” gates. The architecture enforces strict causality (tokens can only aggregate information from previous positions) and achieves linear scaling in sequence length, compared to the quadratic cost of self-attention.\n\nThe authors argue that this design enhances interpretability, as prototypes act as semantic hubs capturing coherent, nameable concepts (e.g., “woman,” “verb,” “disease”). They show qualitative examples of interpretable prototypes and conduct targeted interventions (e.g., reinitializing gender-related prototypes) to illustrate functional specificity. ProtoT is evaluated on perplexity, scalability, and GLUE benchmarks, showing performance close to or slightly better than comparable transformer baselines. Additional experiments assess robustness under noise, semantic interventions, and prototype clamping, claiming that ProtoT offers more interpretable and stable behavior."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel architectural contribution: Replacing attention with prototype-based communication is an original and thought-provoking idea. The design draws on interpretability intuitions (discrete, nameable concept vectors) while maintaining competitive performance.\n\n2. Linear computational scaling: The causal prefix-based computation provides linear time complexity with respect to sequence length, which is valuable for long-context modeling.\n\n3. Empirical competitiveness: Despite removing self-attention, ProtoT achieves perplexity and GLUE scores comparable to transformer baselines, suggesting that the architecture is not just an interpretability toy model but a viable alternative.\n\n4. Prototype interventions: The paper demonstrates that disrupting individual prototypes can cause semantically targeted effects (e.g., lowering the probability of “woman” when a “female” prototype is reinitialized). This provides concrete, though limited, evidence of concept-level disentanglement.\n\n5. Comprehensive robustness analysis:\nThe study includes several robustness evaluations (noise perturbations, prototype clamping, semantic interventions), providing a broader view of how the model responds to changes than typical interpretability papers."}, "weaknesses": {"value": "1. Interpretability evaluation is not systematic. \nThe interpretability claims rest on a few qualitative examples and one prototype intervention. There is no quantitative evaluation or comparison to standard attention heads (e.g., using interpretability metrics or human judgment).\nFor example, attention heads in GPT-style models also show easily nameable functions (e.g., syntactic dependency heads, gender-related heads, or token-copying heads). Without such comparison, it is unclear whether ProtoT’s prototypes are inherently more interpretable or simply a different representation of similar phenomena.\n\n2. Lack of architectural clarity.\nThe model description is highly technical and equation-heavy, but lacks a schematic or clear visual illustration of the prototype mixer, read/write gates, and causal structure. This makes it difficult to grasp how information flows through the network and what distinguishes it conceptually from attention.\n\n3. Questionable link between robustness and interpretability.\nThe robustness experiments primarily assess stability under perturbations (noise, synonym replacement, etc.), which is not equivalent to interpretability. While “prototype-mediated robustness” is an interesting idea, it measures internal routing stability rather than human-understandable reasoning transparency.\n\n4. GLUE benchmark does not demonstrate inference or reasoning interpretability.\nGLUE mainly tests fine-tuned classification tasks, not generative or reasoning-heavy inference. Thus, it does not show whether the prototype-based routing would perform comparably to attention in open-ended language modeling or long-context reasoning.\n\n5. Arbitrary and under-justified hyperparameter choices.\nSome architectural design decisions appear ad hoc. For instance, the “local convolution across 4 past tokens” at layers 0 and 1 is introduced without justification or ablation beyond a brief perplexity mention. It is unclear why 4 tokens were chosen, how sensitive results are to this choice, or whether such local convolutions meaningfully relate to the prototype mechanism itself rather than compensating for architectural limitations.\n\n6. Unclear optimization target for low-rank/value projection.\nThe authors use a low-rank projection at half of the hidden size on the value path to reduce compute, claiming only minor perplexity cost. However, it is unclear whether this step prioritizes performance over interpretability. Since the paper’s main goal is interpretability, it is not justified how this choice affects prototype clarity or whether it interacts with the interpretability of the learned representations.\n\n7. Limited evidence and lack of quantitative validation for polysemanticity and half-life correlations.\nWhile the authors note that polysemanticity appears in some prototypes and suggest a correlation between half-life values and the types of encoded concepts (e.g., lower half-life capturing local elements like entities or punctuation), the evidence is largely qualitative. They do not provide:\n- A systematic overview of which prototypes are polysemantic or how prevalent this phenomenon is.\n- Quantitative measurements to substantiate the claimed correlation between half-life values and concept types.\n\nAs a result, the claims about prototype hubs being largely disentangled and interpretable remain suggestive rather than rigorously demonstrated, which weakens the interpretability conclusions.\n\n8. Cherry-picked interpretability examples.\nThe prototypes shown in the paper appear selected to illustrate the interpretability claim (e.g., a “female” prototype). The main paper includes only one such example, with more shifted to the appendix. There is no evidence that interpretable prototypes are common rather than rare.\n\n9. Limited experimental scope.\nExperiments are performed on small-scale models (6 layers, 256 hidden units, context 256), using a 250M-token dataset. It is unclear whether the interpretability and performance claims hold for larger models or longer contexts.\n\n10. Causality claim lacks direct validation.\nThe prefix-mean formulation is described as enforcing strict causality, but the paper does not empirically analyze whether this design changes the model’s information flow compared to attention.\n\n11. Presentation issues.\nThe paper is dense, with heavy algebraic exposition and limited conceptual framing. Important design motivations (e.g., why this form of bidirectional gating should yield interpretability) are obscured by implementation details. Visual aids and clearer methodology would make it much more accessible. Furthermore, the paper is not rigorously proofread, containing minor typos such as line 305 (wrong capitalization) and line 376 (misplaced comma). The writing also frequently mixes results into the methodology section, with justifications often phrased as “we observed that this works best,” which blurs the line between design rationale and empirical findings. In addition, references are inconsistently formatted, for instance, arXiv citations appear in two different styles."}, "questions": {"value": "1. How do you quantitatively evaluate interpretability? Do you have any statistics on how many prototypes correspond to coherent or nameable concepts, for example using automated mechanistic interpretability methods or human-based evaluation?\n\n\n2. Can you provide a comparison with interpretability in attention heads (e.g., showing analogous heads from LLaMA on similar examples) to support the claim that ProtoT is inherently more interpretable? Human-based evaluation could also help establish this.\n\n\n3. How does ProtoT perform on generative or long-context tasks beyond GLUE fine-tuning? Does the prefix-mean and time-discount mechanism maintain performance at longer contexts or larger model scales compared to self-attention?\n\n\n4. Some architectural/hyperparameter choices appear ad hoc (e.g., layer-0 4-token convolution, number of prototypes, read/write temperatures, alpha-gate initialization). How sensitive are the results to these choices, and what is the rationale behind them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ajFu62FdFf", "forum": "YPNDGGgByQ", "replyto": "YPNDGGgByQ", "signatures": ["ICLR.cc/2026/Conference/Submission25080/Reviewer_SbL4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25080/Reviewer_SbL4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678965312, "cdate": 1761678965312, "tmdate": 1762943317990, "mdate": 1762943317990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an alteration to standard self-attention-based transformers, titled Prototype Transformer, that enforces model computation to route through prototypes, aimed at increasing interpretability and controllability. The authors compare their ProtoTs to Llama, Mamba, and DeltaNet, finding that ProtoTs have much worse perplexity but perform relatively on par for downstream evaluations like GLUE finetuning. They also report results on robustness, finding that ProtoTs offer improved robustness. They qualitatively evaluate the interpretability of the learned prototypes, providing examples of prototypes corresponding to gender attributes that can be controlled."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The aim of creating an inherently interpretable language model is very interesting and a difficult and under-explored problem. Prototype learning is an intuitive way to go about this, particularly given its success in computer vision.\n- The authors perform extensive training experiments, including very interesting scalability analysis.\n- Furthermore, the evaluation comparing ProtoTs to Llama/SSMs finding improved robustness and on par benchmark performance despite worse perplexity is also quite interesting.\n- The proof of concept interpretability analysis seems promising for the few provided qualitative examples."}, "weaknesses": {"value": "- It is unclear to me how well the ProtoTs function as language models. While the perplexity is clearly worse, examples of text completions and general language modeling utility would be helpful to provide intuition as to how meaningful in the difference is. \n- How do the authors test the actual interpretability of the learned prototypes? An automated interpretability evaluation pipeline (for example, see SAEBench [1]) could be a way to do this.\n- The downstream utility of ProtoTs, and more generally inherently interpretable transformers, over “post-hoc” explanation methods like circuit tracing/SAEs/transcoders/etc was not made very clear. Are ProtoTs expected to be better for intervention and steering, debiasing, safety auditing, etc? Given the lack of sparsity for ProtoTs over alternative methods, the added utility is not immediately apparent to me. An exploration of any potential downstream application would significantly strengthen this work.\n\nWhile this work is very promising, given that interpretability is the main motivation, I feel that the interpretability analysis is not very thorough. No quantitative analysis is performed on the interpretability, nor any comparison to alternatives such as popular post-hoc methods. I recommend that the authors complete a more fleshed-out evaluation of the interpretability and downstream utility of ProtoTs to convince readers and reviewers of the benefits of using ProtoTs given the perplexity cost. \n\n[1] Karvonen, Adam, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David Chanin, Yeu-Tong Lau et al. \"Saebench: A comprehensive benchmark for sparse autoencoders in language model interpretability.\" *arXiv preprint arXiv:2503.09532* (2025)."}, "questions": {"value": "- What ensures that the learned parameters are actually “prototypes” of the data? Are they constrained to lie on the data manifold?\n- How are prototypes labeled?\n- How different would ProtoTs be from replacing all MLPs in a LM with transcoders?\n- Lines 120-121: “by treating proto-s as semantic routing vectors that create R distinct communication channels, each with separate read/write gates and learnable time-discount param-s.” These abbreviations decrease legibility. Similar for “ppl” in lines 251-252.\n- Line 363: “Polysemanticity is present in a few prototypes but remains limited overall.” Is this a qualitative analysis? Can more details be provided?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hOSBDeVrQM", "forum": "YPNDGGgByQ", "replyto": "YPNDGGgByQ", "signatures": ["ICLR.cc/2026/Conference/Submission25080/Reviewer_BgJp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25080/Reviewer_BgJp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949157640, "cdate": 1761949157640, "tmdate": 1762943317670, "mdate": 1762943317670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ProtoT, a prototype-based autoregressive language model architecture. ProtoT introduces a prototype communication channel to aggregate contextual information and explicitly exposes routing activations, enabling interpretable and intervenable concept representations. The authors conduct evaluations and further examine the model’s robustness under various conditions, including semantic-preserving noise, prototype clamping perturbation, and causal interventions. In addition, a series of interpretability experiments are performed to analyze the model’s internal behavior. Experimental results demonstrate that ProtoT achieves competitive task performance while offering additional tools for robustness analysis and a certain degree of model interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Introducing a prototype mechanism into black-box models is an interesting direction. It enhances interpretability and allows us to explicitly observe and selectively modify the concepts learned by the model.\n\n2. The paper examines model robustness from multiple intervention perspectives and provides interpretability insights based on the routing mechanism, offering a more transparent view of the model’s internal reasoning process."}, "weaknesses": {"value": "1. There is a noticeable performance gap with top-tier models. Except for RTE and WNLI, ProtoT still shows a significant performance gap compared to LLaMA, indicating substantial room for improvement in terms of model expressiveness and generalization ability.\n\n2. Insufficient baseline comparisons: The experimental section lacks a systematic comparison with a broader range of mainstream methods, primarily comparing with relatively weaker models such as LLaMA. \n\n3. Limited depth of interpretability analysis: Although the paper presents some activation visualizations, it does not provide a thorough explanation of the semantic meaning, formation mechanism of the prototype concepts during reasoning.\n\n4. The distinction from prior work is unclear. Previous studies have already explored incorporating prototype-based ideas into various neural architectures. To highlight its originality, this work should better articulate its unique contributions and the challenges it addresses compared to existing prototype-based interpretability approaches.\n\n5. Questionable practical significance of interpretable architectures. In the era of LLM, post-hoc interpretability may hold greater promise. Despite their theoretical transparency, many “interpretable architectures” have seen limited adoption in practice. In practice, both researchers and industry practitioners tend to prefer black-box yet highly effective models, suggesting that the practical value of explicitly interpretable architectures should be redefined—perhaps use post-hoc interpretability as diagnostic or auxiliary tools is better."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b0cjS9Ikne", "forum": "YPNDGGgByQ", "replyto": "YPNDGGgByQ", "signatures": ["ICLR.cc/2026/Conference/Submission25080/Reviewer_x6LY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25080/Reviewer_x6LY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144121557, "cdate": 1762144121557, "tmdate": 1762943317042, "mdate": 1762943317042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the ProtoT, an autoregressive language model that replaces self-attention with a prototype-based communication mechanism. Instead of pairwise token interactions, the model introduces a fixed set of learnable prototype vectors that bidirectionally communicate with input tokens through read and write gates. These prototypes act as semantic channels that aggregate contextual information over time, regulated by per-prototype decay parameters. The design aims to yield an interpretable model architecture that exposes explicit “concept hubs” while offering linear computational complexity in sequence length. The authors evaluate ProtoT on FineWeb-Edu for perplexity, GLUE for fine-tuned downstream performance, and several interpretability and robustness analyses (concept discovery, prototype interventions, and perturbation tests). They show that prototypes appear to capture semantically coherent patterns (e.g., gendered terms, functional words), enabling interpretable interventions with minimal degradation in task performance. ProtoT achieves comparable results to linear-attention baselines but trails behind standard transformers like LLaMA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper pursues an ambitious and timely goal: integrating interpretability directly into the LM architecture, rather than relying on post-hoc analyses. The introduction of prototype vectors as explicit representational channels is conceptually elegant, drawing inspiration from case-based reasoning and slot-attention architectures, and potentially bridging feature disentanglement and mechanistic interpretability."}, "weaknesses": {"value": "- ProtoT essentially replaces self-attention with a learned prototype mixer that uses fixed latent vectors and exponential decay. While this is a clean design, similar ideas have been explored extensively in slot attention, Perceiver IO/AR, and prototype networks in both NLP and vision. The main mathematical formulation (Eq. 1) is a direct adaptation of cross-attention with time-discounting, without introducing new theoretical mechanisms for interpretability. The paper’s claim that prototypes “capture nameable concepts” is intriguing but not demonstrated beyond anecdotal examples and small intervention cases.\n\n- The visualization and intervention analyses are compelling illustrations but not rigorous evidence of interpretability. For instance, the “female” and “male” prototypes (L9 P7 and L9 P18) are selected manually from hundreds of candidates; the effects shown in Table 6 (e.g., −17.8 % probability for women) are not statistically validated and could arise from spurious correlations in FineWeb-Edu. The concept discovery procedure lacks quantitative measures such as mutual information, sparsity metrics, or concept alignment scores that are standard in interpretability research. Moreover, the authors acknowledge that polysemanticity remains “limited overall” without defining how it is measured. \n\n- The model’s perplexity (Table 1) and GLUE results (Table 2) indicate clear performance gaps versus LLaMA and Mamba. ProtoT performs comparably to DeltaNet, but that baseline itself is weak. The large-scale setting reduces this gap somewhat, yet ProtoT still trails by several perplexity points, and its long-context scalability is explicitly acknowledged as “poor.” Given ICLR’s standards for architectural contributions, the empirical competitiveness is insufficient to justify acceptance. \n\n- Although the model claims linear complexity, the real-world throughput (Table 9) shows it is 2×–3× slower than LLaMA at equivalent width/depth, despite linear scaling. The efficiency claim is thus more theoretical than practical. Furthermore, the interpretability-by-design argument conflates readability of intermediate states with faithful reasoning transparency. ProtoT provides observable slots, but it is unclear whether these slots correspond to causal features used by the model during generation. \n\n- The paper does not explain why prototype-based routing should inherently yield disentangled or interpretable representations. The prefix-mean and time-discount mechanisms are heuristic, and the correlation between decay parameters and “concept timescales” is empirically observed rather than derived. While Appendix A.7 includes layer-0 ablations, these primarily measure perplexity changes, not interpretability effects."}, "questions": {"value": "- How are “concepts” operationally defined and verified? Are prototypes annotated or evaluated against external semantic taxonomies or concept datasets?\n\n- How reproducible are the concept-specific findings (e.g., gender prototypes) across random seeds and datasets? Could similar prototypes emerge by chance?\n\n- Since prototypes act as a fixed-size communication bottleneck, how does interpretability scale with model size or number of prototypes (R)? Does increasing R simply reintroduce entanglement?\n\n- Could the observed prototype activations be artifacts of token frequency or co-occurrence patterns (e.g., gendered words dominating local contexts)?\n\n- Have you compared ProtoT’s interpretability to recent mechanistic interpretability methods (e.g., sparse autoencoders on transformer residual streams)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wAaolVUEEV", "forum": "YPNDGGgByQ", "replyto": "YPNDGGgByQ", "signatures": ["ICLR.cc/2026/Conference/Submission25080/Reviewer_YZaN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25080/Reviewer_YZaN"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission25080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762191264536, "cdate": 1762191264536, "tmdate": 1762943316609, "mdate": 1762943316609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}