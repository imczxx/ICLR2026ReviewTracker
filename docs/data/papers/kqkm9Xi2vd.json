{"id": "kqkm9Xi2vd", "number": 19835, "cdate": 1758299843460, "mdate": 1759897016800, "content": {"title": "Core Advantage Decomposition for Policy Gradients in Multi-Agent Reinforcement Learning", "abstract": "This work focuses on the credit assignment problem in cooperative multi-agent reinforcement learning (MARL). Sharing the global advantage among agents often leads to insufficient policy optimization, as it fails to capture the coalitional contributions of different agents. Existing methods mainly assign credits based on individual counterfactual contributions, while overlooking the influence of coalitional interactions. In this work, we revisit the policy update process from a coalitional perspective and propose an advantage decomposition method guided by the cooperative game-theoretic core solution. By evaluating marginal contributions of all possible coalitions, our method ensures that strategically valuable coalitions receive stronger incentives during policy gradient updates. To reduce computational overhead, we employ random coalition sampling to approximate the core solution efficiently. Experiments on matrix games, differential games, and multi-agent collaboration benchmarks demonstrate that our method outperforms baselines. These findings highlight the importance of coalition-level credit assignment and cooperative games for advancing multi-agent learning.", "tldr": "", "keywords": ["Deep Learning; Reinforcement Learning; Multi-Agent Systems; Game Theory"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5aec25fe087d1cb3e9aa09519fec2aa07ab07ade.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles credit assignment in cooperative multi-agent reinforcement learning (MARL). It proposes a core-guided advantage decomposition method grounded in cooperative game theory, where individual agent advantages are derived to satisfy a $\\epsilon$-core solution concept. To reduce computational overhead, the method approximates the core via random coalition sampling. The authors provide lower bounds on coalition policy improvement and evaluate on matrix games, VMAS, and cooperative MuJoCo."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides theoretical analysis, including lower bounds on coalition policy improvement.\n\n2. Uses a principled cooperative game-theoretic framework for advantage decomposition.\n\n3. Demonstrates empirical effectiveness on matrix games, VMAS, and cooperative MuJoCo tasks."}, "weaknesses": {"value": "1. Related work and baselines appear outdated.\n\n2. Reported improvements over baselines are modest and not clearly statistically significant.\n\n3. Missing evaluations on widely used benchmarks such as SMAC/SMACv2 and Google Research Football.\n\n4. Assumes additivity (sum of individual advantages equals joint advantage), which may not hold in highly non-linear interactions or with strong coordination requirements.\n\n5. Coalition advantages are estimated for counterfactual coalitions that the critic may not have seen, raising out-of-distribution estimation concerns.\n\n6. Sensitivity to the $\\epsilon$-core hyperparameter may affect stability and reproducibility; its selection criteria are unclear.\n\n7. Scalability  of random coalition sampling with increasing agent counts are not fully characterized."}, "questions": {"value": "1. How are counterfactual coalition advantages computed when the critic has not seen those joint actions in the replay buffer? Are there measures to mitigate out-of-distribution bias (e.g., constraints, regularization, uncertainty)?\n\n2. How sensitive is performance to the $\\epsilon$ parameter of the $\\epsilon$-core? \n\n3. Do you evaluate on SMAC/SMACv2 and Google Research Football? If not, what prevents running on these benchmarks, and how do you expect the method to scale there?\n\n4. How does computational cost scale with number of agents and coalition samples (training/inference wall-clock, GPU hours)? What is the per-update overhead relative to standard CTDE methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G7fVC1HQXO", "forum": "kqkm9Xi2vd", "replyto": "kqkm9Xi2vd", "signatures": ["ICLR.cc/2026/Conference/Submission19835/Reviewer_yTEA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19835/Reviewer_yTEA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719273893, "cdate": 1761719273893, "tmdate": 1762932010968, "mdate": 1762932010968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CORA, a credit-assignment wrapper for cooperative MARL policy gradients. CORA (i) computes coalition-level advantages for every subset of agents, (ii) allocates individual credits by solving a quadratic program that respects the strong Œµ-core (coalition rationality + global budget), and (iii) approximates the exponential set of constraints via random coalition sampling. Theoretically, CORA guarantees that beneficial coalitions receive a lower-bound policy improvement even when global advantage is negative. Empirically, CORA consistently outperforms MAPPO, HAPPO and Shapley-based baselines on matrix games, differential games, VMAS and Multi-Agent MuJoCo."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. New granularity: First work to embed core solution (cooperative game theory) inside policy-gradient updates; bridges coalitional stability and MARL credit assignment.  \n2. Theoretical substance: Novel lower bounds on log-policy improvement for any coalition; shows Œµ-core ensures provable incentives for valuable sub-teams.  \n3. Scalable approximation: Random sampling reduces 2‚Åø QP constraints ‚Üí O(n/Œ¥¬≤) with controllable Œ¥-probable core guarantee (Theorem 4).  \n4. Strong empirical record: SOTA or near-SOTA on 12 tasks spanning discrete, continuous, dense and sparse-reward settings; ablations verify sample-efficiency and robustness to small coalition budgets.  \n5. Reproducibility: Complete pseudocode, hyper-parameters, seeds and anonymized code provided; experiments use public benchmarks."}, "weaknesses": {"value": "1. Computational Footprint  \n   Even with random coalition sampling, CORA requires hundreds of extra Q-value evaluations per step, significantly increasing training cost. The paper does not report wall-clock overhead relative to MAPPO for n = 6, 10, 15, limiting its practical deployment at scale.\n2. Scalability Ceiling  \n   The algorithm‚Äôs complexity is O(m¬∑|C| + QP). Current experiments only go up to n = 6 agents, with no results for n ‚â• 20, leaving unclear how performance degrades in larger systems.\n3. Insufficient Baseline Comparison  \n   The paper does not compare with recent Shapley-based policy gradient methods (e.g., SHAQ, SCCA) or role decomposition methods like RODE, limiting a full assessment of CORA‚Äôs relative strengths.\n4. Variance Regularization Issues  \n   Experiments show that ‚ÄúCORA w/o std‚Äù outperforms full CORA in some tasks (e.g., differential games), suggesting that variance regularization may suppress exploration. There is no adaptive mechanism or analysis of when to enable/disable this term.\n5. Strong Theoretical Assumptions  \n   - The provided lower bounds on policy improvement rely on compatible linear critics and small step size Œ±, but no discussion is given for deep neural network critics or non-linear policies.  \n   - Theorem 4 gives a Œ¥-probable core guarantee, but no rate is provided for how Œµ decreases with sample size m, lacking insight into the trade-off between approximation quality and sampling efficiency."}, "questions": {"value": "1. Computational Cost & Real-Time Feasibility  \n   What is the per-step training time and GPU memory usage of CORA compared to MAPPO when n = 10 or 15? Can further parallelization or approximation reduce this cost?\n2. Performance at Scale  \n   How does CORA perform degrade as n ‚â• 20? Does the policy improvement lower bound still hold? Are there QP solver failures or insufficient sampling issues?\n3. Comparison with Recent Credit Assignment Methods  \n   How does CORA compare with latest Shapley-based PG methods (e.g., SHAQ, SCCA) or RODE? Under what task structures does CORA offer clear advantages?\n4. Adaptive Variance Regularization  \n   Can an adaptive mechanism be designed to dynamically adjust the strength of the variance regularization term based on task exploration difficulty or training stage?\n5. Theory Extension to Deep Critics  \n   Do CORA‚Äôs policy improvement bounds still hold under deep neural network critics and non-linear policies? Can compatible function approximation or other techniques extend the theory?\n6. Empirical Œµ vs. m Relationship  \n   How does Œµ decrease as sample size m increases in practice? Can empirical curves or tighter theoretical bounds be provided to guide sampling strategy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xP6JvvdhJf", "forum": "kqkm9Xi2vd", "replyto": "kqkm9Xi2vd", "signatures": ["ICLR.cc/2026/Conference/Submission19835/Reviewer_f3j2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19835/Reviewer_f3j2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802481512, "cdate": 1761802481512, "tmdate": 1762932010617, "mdate": 1762932010617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CORA (Core Advantage Decomposition), a novel method for credit assignment in multi-agent reinforcement learning. CORA formulates advantage decomposition as an $\\epsilon$-core problem from cooperative game theory, ensuring coalition rationality and balanced credit allocation among agents. Each agent‚Äôs advantage $A_i$ is obtained by solving a quadratic program constrained by coalition-level advantages. The method is theoretically justified under natural policy gradient updates and empirically evaluated on matrix games, differential games, VMAS, and MaMuJoCo tasks, showing improved stability and performance over MAPPO and related baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a principled connection between cooperative game theory ($\\epsilon$-core) and multi-agent credit assignment, offering a novel theoretical perspective.\n- Cross-agent credit assignment is a fundamental problem in multi-agent reinforcement learning, and the paper provides a novel solution to this problem."}, "weaknesses": {"value": "- The method appears computationally expensive, yet the paper does not provide quantitative analysis or discussion on runtime efficiency or scalability.  \n- The experiments are limited to small and medium-scale environments; more complex benchmarks such as SMAC or Google Research Football are not tested.  \n- (Minor) Several figures have small fonts and are difficult to read."}, "questions": {"value": "1. Please compare CORA‚Äôs coalition-based advantage decomposition with the implicit credit assignment mechanisms in COMA and VDN, as well as with HAPPO‚Äôs explicit but globally shared advantage decomposition. How do these different decomposition paradigms differ in terms of stability, scalability, and theoretical grounding?\n\n2. How would you position CORA relative to recent explicit credit assignment approaches such as  \n   - She et al. (2022) ‚ÄúAgent-Time Attention for Sparse Rewards Multi-Agent Reinforcement Learning,‚Äù and  \n   - Chen et al. (2023) ‚ÄúSTAS: Spatial-Temporal Return Decomposition for Multi-Agent Reinforcement Learning‚Äù?\n\n3. Given that CORA currently shows promising results mainly in small to medium-scale environments, is there a principled way to improve its computational efficiency so that it can scale to larger agent populations while maintaining its sample-efficiency advantage? If the approach remains limited to few-agent settings, its practical impact might be constrained."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iSderbZTRN", "forum": "kqkm9Xi2vd", "replyto": "kqkm9Xi2vd", "signatures": ["ICLR.cc/2026/Conference/Submission19835/Reviewer_Skdm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19835/Reviewer_Skdm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980110542, "cdate": 1761980110542, "tmdate": 1762932010024, "mdate": 1762932010024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Core Advantage Decomposition (CORA), a credit-assignment scheme for cooperative MARL that decomposes the global advantage into per-agent credits by solving, at each update, a strong \n$\\epsilon$-core optimization over coalitions. Concretely, the authors define a coalitional advantage and allocate individual advantages via a quadratic program that enforces core constraints and penalizes deviation from uniform sharing. They also give a sampling-based approximation with a PAC-style guarantee using VC-dimension arguments. CORA is integrated into an actor-critic (PPO-style) training loop with two critics, and is evaluated on matrix games, differential games, VMAS, and MA-MuJoCo. Empirically, CORA reportedly improves returns over MAPPO/HAPPO/COMA in several tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Casting per-agent advantage allocation as a strong $\\epsilon$-core program (Eq. 7) is neat and leads to interpretable coalition rationality constraints and a variance-regularized objective.\n\n2. Theorems 2 and 3 relate NPG updates to (coalitional) improvement, clarifying when beneficial coalitions are amplified even if the global advantage is negative. \n\n3. Theorem 4 provides a simple sample-complexity bound for entering a ùõø-probable core using VC-dimension, which is rarely discussed in MARL credit assignment.\n\n4. Experiments span matrix/differential games, VMAS, and MA-MuJoCo, with ablations on coalition sampling and discussions of runtime/constraint violations in a synthetic setting."}, "weaknesses": {"value": "1. There is extensive prior work using cooperative-game concepts for credit (e.g., Shapley-based SQDDPG, Shapley Counterfactual Credits, SHAQ). The paper does not compare against these or thoroughly argue why the core yields better learning dynamics than Shapley-style alternatives. The theoretical pieces largely restate core feasibility rather than showing tighter improvement bounds or variance reductions over Shapley. Empirically, none of these Shapley baselines are included.\n\n2. The lower-bound results assume natural policy gradient updates with compatible function approximation, while the implementation uses PPO with clipping and two critics. The paper asserts first-order relations, but does not quantify how clipping, advantage normalization, or off-policy bootstrap in Q affect the guarantees. Rhis leaves the theoretical relevance to the practical algorithm unclear.\n\n3. While MAPPO/HAPPO/COMA are included, other strong contemporary PPO-style baselines and trust-region variants on MA-MuJoCo (e.g., HAPPO/HATRPO references) suggest nuanced performance differences. The paper does not situate its results in that evolving landscape or evaluate on common benchmarks like SMAC or MPE where credit-assignment is heavily studied.\n\n4. The baseline action is chosen as the modal/mean policy output. This may bias estimates and reduce exploration, yet the paper reports that removing the std term helps on differential games, suggesting sensitivity that is not analyzed. Moreover, solving Eq. 7 requires many Q evaluations per step. The cost is discussed only in a random-advantage toy experiment, not the real tasks."}, "questions": {"value": "1. Can you include direct comparisons to SQDDPG, Shapley Counterfactual Credits, and/or SHAQ on at least one VMAS and one MA-MuJoCo task? If not, please justify why these are out of scope and provide a discussion on when core vs. \n\n2. For VMAS and MA-MuJoCo, what is the average per-update time and memory overhead vs. MAPPO/HAPPO when (i) using all coalitions and (ii) using sampled coalitions at your recommended \n$m$? Please also report the number of ùëÑ forward passes per batch and the resulting throughput. \n\n3. Have you tried expectation baselines $Q(s,a_C,\\pi_{N\\backslash C})$ or action-masking variants (as mentioned in Remark 1)? A targeted ablation isolating the effect of the baseline actioncould clarify whether the gains stem from the core constraints or the baseline choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5mUcJNemlH", "forum": "kqkm9Xi2vd", "replyto": "kqkm9Xi2vd", "signatures": ["ICLR.cc/2026/Conference/Submission19835/Reviewer_g7Pg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19835/Reviewer_g7Pg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051512195, "cdate": 1762051512195, "tmdate": 1762932009169, "mdate": 1762932009169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}