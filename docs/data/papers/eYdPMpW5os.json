{"id": "eYdPMpW5os", "number": 423, "cdate": 1756739017247, "mdate": 1759898261650, "content": {"title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot", "abstract": "Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents \\textit{OBS-Diff}, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.", "tldr": "Provide the first efficient training-free pruning for diffusion model", "keywords": ["Pruning", "Diffusion Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf3d15a6fb27e7209e3a79ca94ed5e4874bcf7b9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a **training-free OBS-Diff framework** for pruning multi-step diffusion models to reduce computation cost. It extends the **Optimal Brain Surgeon (OBS)** algorithm to diffusion models, using a **timestep-aware Hessian** to reflect the varying importance of parameters across denoising steps. The authors also design **Module Packages** to group layers for batch pruning, reducing the cost of activation collection. OBS-Diff supports **unstructured**, **semi-structured**, and **structured** pruning. Experiments show that it outperforms existing methods in ImageReward and CLIP score. And the quantitative experiment shows that it perform much better than other methods at 30% and 40% sparsity level."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is **highly generalizable**, applicable to different diffusion architectures such as U-Net and MMDiT.\n2. It **extends OBS pruning** to diffusion models, introducing timestep-aware weighting that better preserves early-stage generation accuracy.\n3. Supports multiple pruning granularities (unstructured, semi-structured, structured) in a unified framework."}, "weaknesses": {"value": "1. The pruning ratio is fixed per layer (layer-wise), without dynamic or weighted adjustment based on layer importance. Different layers may contribute unequally to the final output, so uniform sparsity could be suboptimal."}, "questions": {"value": "1. Why doesn’t the paper include a **comparison of inference-time acceleration** between different pruning methods? Only pruning-time cost is reported.\n2. Is the **generalization ability limited by the calibration dataset**? Since pruning depends on calibration prompts, will prompts outside that distribution generate lower-quality results?\n3. If the **calibration dataset size and prompt diversity** increase, could neuron activations become more uniform, reducing contrast in neuron importance and potentially leading to over-pruning of useful neurons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A4kkrjUE0M", "forum": "eYdPMpW5os", "replyto": "eYdPMpW5os", "signatures": ["ICLR.cc/2026/Conference/Submission423/Reviewer_ePsz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission423/Reviewer_ePsz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878705629, "cdate": 1761878705629, "tmdate": 1762915517050, "mdate": 1762915517050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OBS-Diff, a training-free, one-shot pruning framework tailored for large text-to-image diffusion models, whose iterative denoising makes standard LLM-based pruning ineffective. Experiments show OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering notable inference speedups with minimal drop in visual quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- OBS-Diff adapts OBS with a timestep-aware Hessian that gives earlier steps more weight, which matches error build-up in denoising. It also supports many sparsity types (unstructured, N:M, and structured like MHA heads and FFN neurons), making the pruning precise and flexible without extra training.\n- It is a one-shot, training-free pipeline with a group-wise sequential strategy that cuts calibration cost. Experiments show clear speedups with little quality drop across common text-to-image backbones (e.g., SD/SDXL/DiT-style models), suggesting good practicality."}, "weaknesses": {"value": "- Most comparisons are against LLM-oriented, training-free heuristics (e.g., Wanda/DSnoT/magnitude) adapted to diffusion, plus simple L1 for structured cases. This risks overstating, because these baselines were not designed around DiT denoising dynamics. The paper should add results against diffusion-native pruning on Stable Diffusion/SDXL/DiT—e.g., LD-Pruner, EcoDiff, and TinyFusion.\n- The proposed timestep weighting and the group-wise pruning are tuned on a small prompt set and tested in limited settings. It’s not clear they hold for other samplers, step counts, CFG scales, or out-of-domain prompts/styles. Wider tests across these factors would better show robustness."}, "questions": {"value": "My major concern lies in the experimental comparison. This paper only compares with LLM-based method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ckqGKF81bj", "forum": "eYdPMpW5os", "replyto": "eYdPMpW5os", "signatures": ["ICLR.cc/2026/Conference/Submission423/Reviewer_RU2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission423/Reviewer_RU2d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900744897, "cdate": 1761900744897, "tmdate": 1762915516872, "mdate": 1762915516872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "OBS-Diff is a training-free, one-shot pruning framework for text-to-image diffusion models. OBS-Diff employs optimized OBS for diffusion architectures and supports multiple granularities (unstructured, N:M semi-structured, and structured pruning of MHA heads / FFN neurons). A timestep-aware Hessian with decreasing weights prioritizes early denoising steps to optimize error accumulation, and a group-wise sequential procedure optimizes calibration cost. Experiments on SD3/3.5/Flux models report minimal visual quality loss with notable speedups on a wide range of sparsity types."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Unified support for unstructured, N:M (e.g., 2:4), and structured sparsity in diffusion series models. \n\n- Efficient group-wise sequential pruning and effective weighted Hessian construction. \n\n- Qualitative and quantitative results across SD3-Medium/SD3.5-Large and Flux; Strong experimental metrics."}, "weaknesses": {"value": "- Figure 2 needs to be cited and introduced at the beginning of the methods section. \n\n- Wall-clock numbers are shown for a single MMDiT block; whole-model, sampler-inclusive speedups are not fully established. Efficiency gain under unstructured sparsity also needed to be reported.  \n\n- The comparison set is narrow, mainly Magnitude, Wanda, DSnoT for un/semi-structured pruning and an L1-norm baseline for structured pruning (with Diff-Pruning only on CIFAR-10). Please broaden or justify the baseline choice by including (or at least discussing) recent training-based / non-one-shot diffusion compression methods \n\n- What would be the result of implementing SparseGPT like wanda? This experiment might demonstrate the advantages of the proposed approach over standard OBS. \n\n- The log-decrease schedule is intuitive but needs stronger ablation/theory vs. alternatives (e.g., cosine, exponential, other weights).  \n\n- For CIFAR-10 DDPMs, the pruned models are fine-tuned (100k steps, C.3); clarify when fine-tuning is required vs. truly training-free regimes. Also, the enhancement of the proposed method seems to be very limited compared to other methods (even random pruning), please clarify. \n\n- In Table 8, the higher the sparsity, the better the FID metrics, explain why. \n\n- How sensitive is OBS-Diff to the timestep-weighting schedule and to the number/selection of timesteps used for Hessian estimation, and do the gains persist across samplers (DDIM/DPM++/EDM) and extend to video diffusion or cascaded/staged T2I? A qualitative explanation is sufficient; empirical results are optional if running new experiments would be time-consuming."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HBIf9sNDJX", "forum": "eYdPMpW5os", "replyto": "eYdPMpW5os", "signatures": ["ICLR.cc/2026/Conference/Submission423/Reviewer_p7Tk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission423/Reviewer_p7Tk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902765478, "cdate": 1761902765478, "tmdate": 1762915516742, "mdate": 1762915516742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OBS-Diff, a one-shot, training-free pruning framework designed to compress large-scale text-to-image diffusion models. The authors identify that existing methods fail because they don't account for the iterative nature of the denoising process, where errors can accumulate. OBS-Diff addresses this by adapting the classic Optimal Brain Surgeon (OBS) method. Its core innovations are a timestep-aware Hessian construction that gives more weight to earlier, more critical denoising steps, and a group-wise pruning strategy to make the process computationally efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed timestep-aware Hessian is an elegant and intuitive solution. Weighting the pruning criteria based on the temporal dynamics of the diffusion process is a clever way to mitigate performance degradation.\n\n2. The framework is explicitly designed to handle modern, complex architectures (like MMDiT) and supports a wide range of pruning granularities (unstructured, semi-structured, and structured). This makes it a potentially universal tool for diffusion model compression."}, "weaknesses": {"value": "1. the main result (Table 1) presents the unstructured pruning results. However, unstructured pruning have limited application and cannot provide memory reduction and speedup on conventional hardware. From the table, models can be pruned up to 50%. Similar results have been presented by baseline works (SparseGPT, Wanda), which shows the redundancy of transformer weights. Therefore, showing diffusion models can be pruned by 50% in an unstructured way has limited contribution.\n2. Comparison results are mainly for unstructured pruning. Authors should show comparison results in structured pruning and semi-structured pruning settings to support the advantage of OBS-Diff in a practical scenario. From Table 2, Wanda is better than OBS-Diff under semi-structured setting. Comparison under structured setting is missing.\n3. While many models are used under unstructured setting. Only SD3.5 is used under semi-structured and structured setting."}, "questions": {"value": "Same as above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L6Hx7pFoaa", "forum": "eYdPMpW5os", "replyto": "eYdPMpW5os", "signatures": ["ICLR.cc/2026/Conference/Submission423/Reviewer_L2zh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission423/Reviewer_L2zh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission423/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963428126, "cdate": 1761963428126, "tmdate": 1762915516615, "mdate": 1762915516615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}