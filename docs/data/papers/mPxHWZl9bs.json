{"id": "mPxHWZl9bs", "number": 10694, "cdate": 1758179816920, "mdate": 1759897634869, "content": {"title": "VideoExplorer: Boosting Long Video Understanding with Dynamic Temporal Grounding", "abstract": "Long-video understanding (LVU) is a challenging problem in computer vision. \nExisting methods either downsample frames for single-pass reasoning, sacrificing fine-grained details, or depend on textual reasoning over task-agnostic representations, hindering task-specific perception and exploration.\nIn this paper, we propose VideoExplorer, a framework grounded in the principle of ``thinking with video'', which naturally intertwines planning, temporal grounding, and scalable perception into a coherent reasoning process.\nRather than reasoning over a static context, VideoExplorer iteratively formulates sub-questions, locates relevant moments, and performs task-oriented, temporally scalable video understanding until reaching the final answer, enabling faithful, efficient, and interpretable reasoning.\nTo address the lack of LVU training resources, we construct a long-video reasoning dataset using difficulty-adaptive sampling to ensure high-quality trajectories on complex tasks.\nBuilding on this dataset, we design a two-stage training pipeline: supervised trajectory initialization followed by trajectory-level preference optimization, encouraging adaptive temporal grounding and iterative information integration guided by downstream rewards.\nExtensive evaluations on popular long-video understanding and reasoning benchmarks demonstrate VideoExplorer's significant advantage over existing baselines, highlighting its robustness, adaptability, and efficiency. \nOur code is available in this repository.", "tldr": "", "keywords": ["long video understanding", "agentic frameworks"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f305e3d691ea334d0c5a04faa311f5c9e10af18a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents VideoExplorer, an agentic framework for long-video reasoning that integrates a planner, a decoupled temporal grounder, and a scalable visual perception module under the principle of  “thinking with video.”  The method decomposes complex tasks into sub-queries, grounds them temporally, and iteratively retrieves relevant video segments. To enable training, the authors construct a reasoning-centric dataset with difficulty-adaptive sampling and design a two-stage optimization pipeline—structured imitation (SFT) followed by trajectory-level preference optimization (TDPO)—to teach multi-step reasoning. Experiments across multiple long-video benchmarks demonstrate improved performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly motivated by the challenges of long-video reasoning and proposes a solution that integrates planning, temporal grounding, and reasoning. The concept of “thinking with video” offers an intuitive and coherent metaphor that connects cognitive reasoning and perception. \n\nThe paper reports results on multiple benchmarks such as LVBench, MLVU, MH-NIAH to demonstrate its performance. \n\nThe decoupled planner–grounder structure and dynamic temporal scaling allow for interpretable, step-by-step reasoning. This may benefit future extensions or real-world agentic systems. \n\nThe reasoning-centric dataset and two-stage optimization (SFT + TDPO) are clearly presented, and the difficulty-adaptive sampling strategy is reasonable to curate non-trivial supervision signals. \n\nOverall, the paper is clearly written and provides quantitative results to demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "The novelty is incremental.  The key idea—iterative query decomposition, retrieval-based reasoning, and adaptive temporal localization—has been extensively explored in VideoAgent (Wang et al., 2024b), Ego-R1 (Tian et al., 2025), and VideoTree (Wang et al., 2025). The proposed concept of “thinking with video” largely parallels prior agentic frameworks with retrieval augmentation, which also decompose a global query into multiple sub-queries to iteratively access contextual evidence. The only difference is whether the clues are retrieved from pre-generated textual contexts (e.g., dense captions, object trajectories, audio transitions) or directly from raw video. The “decoupled temporal grounding” seems more like a system-level reorganization than a fundamentally new algorithmic or theoretical contribution. \n \nLack of quantitative analysis for efficiency and grounding mechanism. In lines 234–236, the authors claim that the temporal grounder reduces reasoning complexity. However, no quantitative analysis or efficiency benchmark is provided—only a qualitative statement (“reduces redundant searches”). It remains unclear how much retrieval complexity is actually reduced, or whether the proposed method is more efficient than baselines such as VideoRAG or VideoTree. Moreover, the temporal grounding process (“construction, retrieval, verification, summarization”) is never formally defined, ablated, or quantified, leaving the core mechanism somewhat ambiguous. \n \nPotential dataset bias and self-bootstrapping issue. The difficulty-adaptive sampling strategy, while intuitively reasonable, lacks empirical validation for its generality and fairness. Since the reasoning trajectories are generated using the authors’ own VideoExplorer model and subsequently used for model training, the dataset construction may introduce self-bootstrapping bias, potentially inflating performance due to data-model coupling. There is no discussion on how this bias is mitigated or evaluated. \n \nTDPO is under-specified and not clearly novel. The proposed Trajectory-level Direct Preference Optimization (TDPO) appears to be a straightforward adaptation of existing DPO-style alignment methods to multi-step trajectories. The paper does not describe how preference pairs are collected, how trajectory-level rewards are computed, or how TDPO differs substantially from prior RLHF/DPO variants. Without such clarification, the contribution of TDPO remains vague and incremental. \n \nMissing in-depth analysis. While quantitative results show consistent improvements, the paper does not analyze why the method works—e.g., whether gains arise from improved temporal localization, better reasoning decomposition, or dataset curation. There is also no qualitative or failure analysis beyond a single example, which weakens the interpretability and contribution of the approach. \n \nInconsistent performance under smaller LLM backbones. As shown in Table 1, when the LLM parameter size is 3B, the proposed VideoExplorer performs significantly worse in reasoning compared to both Agentic Frameworks (Qwen2.5-VL-7B) and (Qwen2.5-VL-32B). The paper did not provide an explanation on that, which raises concerns about the method’s robustness and scalability across different model sizes."}, "questions": {"value": "See Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oZdXMayWPR", "forum": "mPxHWZl9bs", "replyto": "mPxHWZl9bs", "signatures": ["ICLR.cc/2026/Conference/Submission10694/Reviewer_HMvR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10694/Reviewer_HMvR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871065211, "cdate": 1761871065211, "tmdate": 1762921937740, "mdate": 1762921937740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose VideoExplore, an agent-based long video reasoning framework, to address the \"task-agnostic, static and lossy representation\" issues in existing methods. The framework includes a planner to decompose questions into task-relevant sub-questions, a grounding module to localize key timestamps, and the understanding module to collect the answer to each sub-question. The authors also collect a large-scale training dataset for the 2-stage SFT-RL training strategies. The experimental results demonstrate its superior performance over existing methods. However, the writing in the methods section needs improvement for a better reading experience."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors proposed VideoExplore, which is a sophisticated and effective agent-based video reasoning framework for long video understanding and reasoning.\n2. To facilitate the 2-stage training, the authors collected a large-scale training dataset (including QA and reasoning chains) based on existing long video datasets and with the help of LLMs.\n3. The experiments demonstrate that the proposed methods outperform previous methods by a large margin.\n4. Ablation studies demonstrate the effectiveness of the components of the framework, the training strategies, and the training data. \n5. The visual token usage also indicates a high computation efficiency compared with other methods."}, "weaknesses": {"value": "Major:\n1. The writing in 3.2 is not so clear. In Eq. 4, the definition and format of P_t, and the format of T_t (temporal span), are missing. They can be somehow inferred from Figure 1, but a clear description in 3.2 will help reading. And also, if there is a function of the planner, it will be more complete for the task definition.\n2. In line 235, \"In the offline stage, videos are segmented into clips ...\". What is the specific method to segment these videos? Uniformly? Does this step only relate to semantic matching as described in line 236? \n3. With these segments, it looks like only text descriptions and timestamps can help the video understanding. What is the purpose and benefit of introducing multimodal queries?\n4. This paragraph, \"Decoupled Temporal Grounding Execution.\" is hard to understand. The function of the planner and the passing-back mechanism (line 241) is not illustrated in Figure 1. A simple \"Retrieving-->Verifying-->Summarizing\" in Figure 1 looks oversimplified for the complex framework.\n5. What is the timestamp representation/encoding in the framework? Is it the frame index or absolute time? It looks like there is a discrepancy in Figure 1 and Figure 5.\n\nMinor: \n1. In line 045, \"To address to the above problems, ...\" --> \"To address the above problems, ...\".\n2. In line 146, \"... downsampling the raw video video and ...\" --> \"\"\\... downsampling the raw video and ...\"."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PrQGOuKkf8", "forum": "mPxHWZl9bs", "replyto": "mPxHWZl9bs", "signatures": ["ICLR.cc/2026/Conference/Submission10694/Reviewer_iHcw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10694/Reviewer_iHcw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961559606, "cdate": 1761961559606, "tmdate": 1762921937400, "mdate": 1762921937400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an idea of “Thinking with video.” Analogous to “thinking with images,”  “thinking with video” treats reasoning as a dynamic process of temporally grounded exploration and decomposition. The model iteratively decides what to look for, where to watch, and at what temporal scale, flexibly combining both fine-grained inspection and coarse-grained temporal grounding for long video understanding. The paper also proposes a two-stage training framework with SFT and DPO on accepted temporal grounding trajectories to improve VQA on long video benchmarks and showcase great performance improvement with modest innovation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly extends the idea of Thinking with Images to the video domain from a temporal perspective.\n\n2. It constructs a reasoning-centric dataset with multi-step reasoning trajectories, supervised for a two-stage optimization pipeline that combines structured imitation learning (SFT) and trajectory-level preference alignment (DPO for video)\n\n3. The work demonstrates strong token efficiency in long-video understanding, outperforming uniform-sampling-based methods."}, "weaknesses": {"value": "1. The proposed components include question decomposition and agent planning, question-aware temporal grounding, agentic iterative reasoning, and temporally grounded preference optimization—have been explored in many existing works such as DrVideo [1], VideoINSTA [2], Traveler [3], TPO [4], and Video-R1 (T-GRPO) [5]. The authors either missed direct relevant comparisons or need to further justify the novelty and necessity of their proposed multi-step temporal grounding approach.\n\n[1] DrVideo: Document Retrieval-Based Long Video Understanding. CVPR 2025.\n\n[2] VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs. EMNLP 2024.\n\n[3] Traveler: A Modular Multi-LLM Agent Framework for Video Question Answering. EMNLP 2024.\n\n[4] Temporal Preference Optimization for Long-Form Video Understanding. arXiv:2501.13919.\n\n[5] Video-R1: Reinforcing Video Reasoning in MLLMs. arXiv:2503.21776.\n\n2. The annotation procedure for multi-step reasoning in the proposed dataset is unclear and less described. The definitions of step-wise reasoning and the quality assurance of VLM-based grounding annotations require more explanation. \n\n3. In Table 2, the QA accuracy shows significant improvement. However, the evaluation of Temporal Grounding Accuracy compared to non-grounding methods (e.g., Ego-R1, VideoAgent) needs additional validation, preferably against other temporal grounding-focused baselines such as [6]\n\n[6] ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos. CVPR2025.\n\nFormatting Errors:\n\nIn Section 2.1, the references following “…leading to the emergence of Multi-modal Large Language Models (MLLMs)” are improperly formatted.\n\nLine 146 contains a duplicate “video”."}, "questions": {"value": "In Section 2.2, regarding the statement: “This paradigm will inevitably lose the rich visual information in original long videos, leading to sub-optimal performance such as in egocentric videos…”\nThe reviewer finds this argument unconvincing, as captioning from image or video frames does not seem to differ significantly between egocentric and exocentric videos."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xP6HauLFjh", "forum": "mPxHWZl9bs", "replyto": "mPxHWZl9bs", "signatures": ["ICLR.cc/2026/Conference/Submission10694/Reviewer_F2uk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10694/Reviewer_F2uk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762354976213, "cdate": 1762354976213, "tmdate": 1762921937024, "mdate": 1762921937024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses long video understanding by introducing VideoExplorer, a framework that iteratively plans, grounds, and reasons over relevant video segments rather than processing a fixed downsampled input. A planner decomposes complex queries into sub-questions, while a temporal grounder retrieves and verifies supporting segments, adjusting frame granularity as needed. The system is trained using a new reasoning-centric dataset with difficulty-adaptive sampling and a two-stage process: supervised fine-tuning followed by trajectory-level preference optimization. This enables VideoExplorer to produce faithful, multi-step reasoning chains. The authors demonstrate state-of-the-art results on several long-video benchmarks, showing improved scalability, accuracy, and interpretability. Overall, the work introduces a dynamic and cognitively inspired paradigm for long video reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Methodology - VideoExplorer is built from well-justified components—planner, grounder, and a cognitive reasoning loop—and its training combines imitation learning with trajectory-level RL, well-suited for multi-step tasks. The decoupled grounding module effectively handles query ambiguity, and each design choice is validated through experiments, such as performance drops when TDPO is ablated.\n- Comprehensive Evaluation - The paper presents a thorough empirical evaluation across multiple benchmarks, where VideoExplorer consistently achieves state-of-the-art results in both accuracy and temporal grounding. Ablation studies, efficiency analysis, and qualitative case studies further support that the performance gains stem from the proposed innovations, reinforcing the method’s robustness and generality."}, "weaknesses": {"value": "- Baselines - The paper evaluates against agentic frameworks and vision-language models that either process uniformly sampled frames or use retrieval-augmented textual representations. However, it does not benchmark against methods that tackle long video understanding by selecting representative keyframes as a compression or reasoning strategy, e.g. [a,b,c,d]. Including such baselines could have strengthened the evaluation by clarifying how much of VideoExplorer’s performance gains come from temporal reasoning and grounding, versus simply improved frame selection. This should also be addressed in related work section.\n- Efficiency - The paper shows reduced token processing compared to baselines, but an analysis of runtime or memory overhead would be helpful. Any empirical or theoretical insight into the efficiency limits would strengthen understanding of the method’s practicality.\n- Generality - Not really a weakness, but the tasks in the paper are QA-style; it would be interesting to see how well the approach would generalize to other long-video understanding tasks.\n- Planner and Grounder Robustness - If the planner formulates off-target sub-questions or the grounder retrieves redundant segments, the model may overlook critical information or waste reasoning steps. The paper does not clearly address how such failures are detected or mitigated, raising concerns about the system’s robustness in complex or ambiguous scenarios.\n\nReferences:\n- [a] M-LLM Based Video Frame Selection for Efficient Video Understanding, CVPR 2025\n- [b] VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection, CVPR 2025\n- [c] Vila: Efficient videolanguage alignment for video question answering, ECCV 2024\n- [d] Self-Chained Image-Language Model for Video Localization and Question Answering, NeurIPS 2023"}, "questions": {"value": "- Have you considered comparing VideoExplorer to keyframe selection methods? This could help isolate how much of the performance gain stems from dynamic temporal grounding versus improved frame selection alone. \n- Can you share any measurements or analysis on runtime, memory usage, or compute overhead? How does the multi-turn framework scale with video length in practice? \n- Do you believe the VideoExplorer framework could extend to long video understanding tasks other than QA benchmarks? If so, what adaptations might be needed? \n- How does the system handle cases where the planner generates sub-optimal queries or the grounder retrieves redundant or irrelevant segments? Are there safeguards to prevent inefficient or incomplete reasoning paths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not Applicable."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5obeheRUZm", "forum": "mPxHWZl9bs", "replyto": "mPxHWZl9bs", "signatures": ["ICLR.cc/2026/Conference/Submission10694/Reviewer_BT9o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10694/Reviewer_BT9o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762415716183, "cdate": 1762415716183, "tmdate": 1762921936625, "mdate": 1762921936625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VideoExplorer, an agentic framework for long-video understanding that (1) plans sub-questions (2) temporally grounds relevant segments and, (3) performs temporally scalable perception before answering calling it \"thinking with video” instead of perception and reasoning over a fixed downsampled context. The training recipe combines SFT on expert trajectories and trajectory-level DPO (TDPO), and the authors build a small reasoning-centric dataset via difficulty-adaptive sampling (11.1k planner and 10.8k grounding trajectories). Results are reported LVBench, MLVU, and MH-NIAH."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear system decomposition (Planner, temporal grounder and perception) with a sensible interface.\n2. Two-stage alignment (SFT then TDPO) is appropriate for multi-step trajectories. Ablations show the importance of the two components.\n3. Token/segment budgeting is discussed, and evaluation caps (visual-only ≤ 32 frames per call) are used. Despite using far fewer visual tokens, VideoExplorer achieves better performance than baseline like VideoAgent and Ego-R1."}, "weaknesses": {"value": "1. Proposed framework is incremental. Planner and grounding is very close to the prior video agents and long context streaming VLMs. VideoAgent introduces controller to call tools, VideoTree iteratively selects and refines parts of the video. And  Ego-R1 uses RL to tune multi step tool use. Training-free methods like Video-RAG already use retrieval for reasoning.\n\n2. The new decoupled temporal grounder and trajectory-level (TDPO)- is more like an engineering refinement, the paper does not isolate specific capabilities that prior systems lack beyond the gains in the table. For example swap your grounder into VideoAgent/VideoTree and swap theirs into your planner to show method-specific gains.\n\n3. Missing comparisons to prior streaming VLMs and video agents like LVAgent which reports higher numbers than the proposed method, for better comparisons against the current SOTA results should be reported under similar settings. Or at least report their results with their settings mentioned. It’s unclear whether the proposed framework is competitive with current best systems.\n\n4. The text mentions evaluations on VideoMMMU but results are not reported. \n\n5. Grounding is reported at IoU 0.1 which is a very low bar."}, "questions": {"value": "1. What capability does decoupled grounder + TDPO enable that VideoAgent/VideoTree/Ego-R1/Video-RAG lack (e.g., multi-hop timestamp chaining, lower false-grounding at higher IoU)? Can you provide metrics demonstrating this.\n\n2. Can you swap your grounder into VideoAgent/VideoTree and theirs into your planner under identical budgets and backbones? Do your gains persist?\n\n3. Can you provide the missing VideoMMMU results that you mention in the introduction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QxERgBtf3b", "forum": "mPxHWZl9bs", "replyto": "mPxHWZl9bs", "signatures": ["ICLR.cc/2026/Conference/Submission10694/Reviewer_gPaM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10694/Reviewer_gPaM"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762791058769, "cdate": 1762791058769, "tmdate": 1762921936268, "mdate": 1762921936268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}