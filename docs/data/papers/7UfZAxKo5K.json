{"id": "7UfZAxKo5K", "number": 14269, "cdate": 1758231694140, "mdate": 1759897379799, "content": {"title": "Two-Way Is Better Than One: Bidirectional Alignment with Cycle Consistency for Exemplar-Free Class-Incremental Learning", "abstract": "Continual learning (CL) seeks models that acquire new skills without erasing prior knowledge. In exemplar-free class-incremental learning (EFCIL), this challenge is amplified because past data cannot be stored, making representation drift for old classes particularly harmful. Prototype-based EFCIL is attractive for its efficiency, yet prototypes drift as the embedding space evolves; thus, projection-based drift compensation has become a popular remedy. We show, however, that existing one-directional projections introduce systematic bias: they either retroactively distort the current feature geometry or align past classes only locally, leaving cycle inconsistencies that accumulate across tasks. We introduce bidirectional projector alignment during training: two maps, old$\\to$new and new$\\to$old, are trained during each new task with stop-gradient gating and a cycle-consistency objective so that transport and representation co-evolve. Analytically, we prove that the cycle loss contracts the singular spectrum toward unity in whitened space and that improved transport of class means/covariances yields smaller perturbations of classification log-odds, preserving old-class decisions and directly mitigating catastrophic forgetting. Empirically, across standard EFCIL benchmarks, our method achieves unprecedented reductions in forgetting while maintaining very high accuracy on new tasks, consistently outperforming state-of-the-art approaches.", "tldr": "", "keywords": ["continual learning", "exemplar free", "exemplar free class incremental learning", "class incremental learning", "exemplar-free"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/107e32bad80fcefb89b1fee43b46cbf077e6b5c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies exemplar-free class-incremental learning (EFCIL) and claims that one-directional projection of class prototypes from old to new embedding space introduces systematic bias that accumulates over tasks. The authors propose to use a bidirectional projector which aligns prototypes during training using a cycle-consistency objective function which ensures a near-bijection between the pairs. The authors further analyze that reducing the cycle loss better preserves old-class boundaries. The method finally uses a Gaussian Bayes classifier for inferene which uses current-task statistics for new classes and projected statistics for old classes. The proposed method outperforms several recent EFCIL baselines across multiple benchmarks achieving a better stability-plasticity trade-off."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is very well-written with a good introduction, motivation and theoretical formulations.\n2. The authors discuss and formulate the concept of prototype drift compensation very well including recent works.\n3. The extensive experiments with thorough ablations are appreciated. The evaluation with distribution similarity metrics is a valuable addition."}, "weaknesses": {"value": "1. It would be nice if the authors could discuss and analyze more on the claim that one-directional projection of class prototypes accumulates bias over tasks to better motivate the method.\n2. Some analysis with respect to the oracle prototype drift could be added. For instance, comparing the projected prototypes using different methods with the oracle prototypes (computed using all old class data) could be used to compare prototype estimations from different methods (see Fig. 6 in ADC [CVPR 24]). This would also highlight how much scope is there to further improve the prototype estimation.\n3. A more detailed discussion of the Bayes classifier for the inference stage (not mentioned in the pseudo-code) would add some clarity.\n4. Figure 2 visibility could be improved."}, "questions": {"value": "1. What is the impact of transporting covariances and estimating from a normal distribution instead of directly projecting the class means as done is LDC [ECCV 24]? Is it necessary to also transport covariances since storing class-wise covariances adds storage requirements? How much does this improve the estimation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kv17pPO7Nn", "forum": "7UfZAxKo5K", "replyto": "7UfZAxKo5K", "signatures": ["ICLR.cc/2026/Conference/Submission14269/Reviewer_A1WY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14269/Reviewer_A1WY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634122482, "cdate": 1761634122482, "tmdate": 1762924721532, "mdate": 1762924721532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the a new method for predicting the prototype drift in the exemplar-free class-incremental learning (EFCIL), especially for the setting of training from scratch. This paper specifies the problem of existing drift compensation methods, which only consider transforming old prototypes to the new distribution while the representation learning is conducted in a reverse direction, i.e., pulling new backbone to the old one. To address this problem, this paper utilizes a distiller to transform the new feature to the old and a adapter to adapt old features to new distribution. A cycle consistency loss is proposed to avoid rank loss and theoretical analysis is provided to validate the necessarity of it.  Comparative experiments with other methods and ablation studies are conducted to verify the effectiveness of proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well written and easy to follow. \n2. The perspetive of proposing bi-directional loss for the prototype drift compensation is novel.\n3. Theorectial analysis of the cycle loss is decent and sound."}, "weaknesses": {"value": "Overall, I think this paper is good, but I have several concerns concentrated on the experiments and implementation.\n1. The selected methods in the comparative studies are a bit old. The most up-to-date methods in the comparison are published in 2024. Since this paper is submitted to ICLR 2026, more recent methods should be included, for example, the DPCR [1].\n2.  The extra amount of parameters of the adapter and distiller should be elaborated. This version does not indicate what architecture of the adapter and distiller is used to provide the performance of the comparative studies. Also, from Table 5, it can be witnessed that a performance drop occurs when using achitecture other than MLP. However, from the Appendix A.10, for a ResNet-18 whose output dimension is 512, the parameters of the MLP itself is $512\\times 512\\times 32 \\times 2 \\approx 16.78M$, which is much more larger than the  linear $512\\times 512 \\times 2 \\approx 0.52M$ and even the backbone ResNet-18 (11.69M). As such, does the performance of this method only benefit from more parameters? Also, what will happen if a smaller MLP is used? It is not reasonable that a adapter can be larger than the backbone. \n3. The effect of different $\\lambda_{\\text{bi}}$ and $\\lambda_{\\text{cyc}}$ is not studied.\n4. Lack of results under the setting of training from half. Since another pipeline of EFCIL is training from half, it is interesting to validate the proposed method under that setting.\n5. Compatibility with other types of classifiers should be studied. For continual learning, there are other types of classifier other than the Bayesian classifier, for example, the most common fully connected network. It is interesting to discuss and validate the compatibility of proposed method with them.\n\n[1] Run He, et al. Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning. In ICML 2025."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XKcnTKfOhz", "forum": "7UfZAxKo5K", "replyto": "7UfZAxKo5K", "signatures": ["ICLR.cc/2026/Conference/Submission14269/Reviewer_tKrY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14269/Reviewer_tKrY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917528320, "cdate": 1761917528320, "tmdate": 1762924720442, "mdate": 1762924720442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of exemplar-free class-incremental learning, where a model must incrementally learn new classes without storing previous examples. The core challenge is \"prototype drift\" - as the feature extractor adapts to new classes, previously stored class prototypes (means/covariances) become outdated in the evolving feature space. \nThe authors identify a fundamental limitation in existing two-stage approaches: (1) training the new model with distillation from the old model, followed by (2) learning a post-hoc adapter to map old prototypes to the new feature space. They demonstrate that this creates systematic bias due to one-directional projections that either retroactively distort current feature geometry or leave cycle inconsistencies that accumulate across tasks.\nTheir solution introduces bidirectional alignment with cycle consistency during the main training stage. Specifically, they jointly learn two mappings: a distiller D (z_new → z_old) and an adapter A (z_old → z_new), with stop-gradient operations and a cycle-consistency objective (A∘D ≈ I and D∘A ≈ I). Theoretically, they prove this contracts the singular spectrum toward unity in whitened space and reduces perturbations in classification log-odds. Empirically, their method achieves state-of-the-art results across multiple benchmarks (CIFAR-100, TinyImageNet, ImageNet-100, and CUB-200), consistently reducing forgetting while maintaining high accuracy on new tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper makes a compelling case that existing one-directional projection approaches create systematic bias due to post-hoc mismatch. The insight that bidirectional alignmeœqqnt with cycle consistency should be integrated into the main training stage (rather than treated as a separate post-hoc step) is conceptually sound and well-motivated.\n\n2. The theoretical analysis in Sections 3.2 and Appendix A provides rigorous justification for the approach. Theorem 1 showing how cycle loss contracts the singular spectrum toward unity, and Corollary 2 linking transport errors to classification log-odds stability, provide solid mathematical grounding for the empirical observations.\n\n3. The evaluation across four standard benchmarks with multiple task splits (T=10, T=20) demonstrates consistent improvements over state-of-the-art methods. The ablation studies (Tables 4 and 5) effectively isolate the contributions of bidirectional alignment (L_bi) and cycle consistency (L_cyc)."}, "weaknesses": {"value": "1. While the method shows strong performance overall, there's minimal discussion of scenarios where gains are marginal or negative. For example, on CUB-200 with T=20, the method trails EFC by 2.4/3.4 pp in accuracy metrics, but this isn't thoroughly analyzed.\n\n2. The paper mentions the adapter/distiller is \"lightweight\" but doesn't quantify the additional computational overhead compared to baseline methods. Given that efficiency is critical in continual learning, concrete metrics on training time, inference time, and memory requirements would be valuable.\n\n3. The method introduces new hyperparameters (λ_bi, λ_cyc, α) that are set to fixed values across experiments. A sensitivity analysis showing how results vary with these parameters would strengthen the claims of robustness.\n\n4. The theoretical analysis assumes centered features and Gaussian class distributions. The paper doesn't sufficiently address how violations of these assumptions in practice might affect performance, nor does it quantify how well these assumptions hold in the experimental settings.\n\n5. While the paper compares against many recent methods, some recent approaches like those using test-time adaptation or more sophisticated covariance modeling aren't included in the comparisons."}, "questions": {"value": "- The paper doesn't sufficiently explain why the specific cycle loss formulation in Equation 7 (with stopgrad on targets) prevents degeneracies better than alternative cycle consistency implementations. A brief comparison to other possible formulations would strengthen the technical justification.\n\n- The motivation of using such a cycle consistency scheme is not very clear. Why can it bring performance gain?\n\n- Some theoretical sections (particularly Section 3.2) could benefit from more intuitive explanations alongside the formalism to improve accessibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XA4zNA59RG", "forum": "7UfZAxKo5K", "replyto": "7UfZAxKo5K", "signatures": ["ICLR.cc/2026/Conference/Submission14269/Reviewer_J5qi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14269/Reviewer_J5qi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923053627, "cdate": 1761923053627, "tmdate": 1762924719777, "mdate": 1762924719777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses exemplar-free class-incremental learning (EFCIL), where models must learn new classes sequentially without storing past data, making them vulnerable to catastrophic forgetting due to representation drift. The authors critique existing drift compensation methods that use one-directional projections (either post-hoc adapters mapping old-new features or distillers pulling new-old), showing these introduce systematic bias and cycle inconsistencies that accumulate across tasks. They propose a bidirectional alignment framework that jointly learns two projectors—an adapter A (old-new) and distiller D (new-old)—during each task's training, coupled with a cycle-consistency loss that enforces near-inverse behavior using stop-gradient gating to prevent retrograde interference with the evolving backbone. The authors provide theoretical analysis proving that minimizing cycle loss contracts the singular spectrum of the composed maps toward unity in whitened space, and that improved transport of class means and covariances yields smaller perturbations of classification margins, directly mitigating forgetting. Extensive experiments across CIFAR-100, TinyImageNet, ImageNet-100, and CUB-200 demonstrate state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper effectively motivates the work by clearly articulating limitations of prior one-directional approaches (systematic bias, local alignment, cycle inconsistencies), making the contribution well-grounded.\n- The paper provides theoretical analysis that formally connects cycle consistency to spectral contraction and classification stability. The proof that minimizing cycle loss contracts singular values toward unity and bounds perturbations of classification log-odds provides principled justification for the approach.\n- The bidirectional alignment with cycle consistency integrated during training (rather than post-hoc) is genuinely innovative. This addresses a clear gap in existing two-stage methods that optimize transport only after representation learning is complete, leading to accumulated cycle inconsistencies.\n- The evaluation is thorough, spanning four datasets (CIFAR-100, TinyImageNet, ImageNet-100, CUB-200) with multiple task splits (T=10, T=20), and comparing against 10 competitive baselines. The consistency of improvements across settings strengthens the claims."}, "weaknesses": {"value": "- The theoretical analysis and distribution transport (Algorithm 1, Stage II) heavily rely on modeling classes as Gaussians with full covariance matrices. This assumption may not hold for complex, multi-modal, or long-tailed distributions, and the paper doesn't validate whether this approximation is reasonable or explore robustness when it's violated.\n- On CUB-200 (where methods fine-tune from ImageNet pretrained weights), the improvements are modest or negative (Table 2). The authors acknowledge that low learning rates for pretrained backbones reduce drift, but this limits the method's applicability to an important practical scenario where pretrained models are standard."}, "questions": {"value": "On Gaussian Assumption and Distribution Modeling.  \nQ1: Validation of Gaussian Approximation.   \n\nCould you provide empirical evidence that the Gaussian approximation is reasonable for the datasets studied?  \n For example:  \n- Visualizations of class distributions in feature space (e.g., t-SNE/UMAP with confidence ellipses)\n- Quantitative goodness-of-fit tests (e.g., Mardia's multivariate normality test, or KL divergence between empirical class distributions and fitted Gaussians)\n- Analysis showing whether classes that deviate more from Gaussianity exhibit larger forgetting or worse transport\n\nThis would help assess whether the theoretical foundations align with the empirical reality of your experiments.\n\nQ2: Robustness to Non-Gaussian Distributions. \nHave you tested the method on datasets with known multi-modal or long-tailed class distributions?  \n\nFor instance:\n\n- What happens on fine-grained datasets where within-class variance is high (cars, aircraft)?\n- Can you provide ablations using mixture-of-Gaussians or non-parametric kernel density models instead of single Gaussians?\n- How does performance degrade when classes are artificially made more multi-modal (e.g., by merging semantically different subsets)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lBQTQobrBf", "forum": "7UfZAxKo5K", "replyto": "7UfZAxKo5K", "signatures": ["ICLR.cc/2026/Conference/Submission14269/Reviewer_zrCc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14269/Reviewer_zrCc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932320392, "cdate": 1761932320392, "tmdate": 1762924719101, "mdate": 1762924719101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}