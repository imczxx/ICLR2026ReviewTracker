{"id": "j8UGvohcpR", "number": 22389, "cdate": 1758330447100, "mdate": 1759896868830, "content": {"title": "Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making", "abstract": "Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit \"ask'\" action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.", "tldr": "A Bayesian approach for agents to learn when to trust action suggestions from advisors of unknown and varying quality.", "keywords": ["POMDPs; Bayesian inference; Human-agent collaboration; Adaptive trust calibration"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6abbfc3f3c76b3a1588ac799231f1a426464c4e3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a POMDP-based framework that allows an autonomous agent to (1) maintain a Bayesian belief over discrete “suggester types” that encode unknown and possibly time-varying reliability, and (2) actively request suggestions via an explicit, cost-bearing “ask” action. Belief updates are performed with a factored MOMDP representation to keep computation tractable. Extensive experiments on Tag and RockSample show that the agent quickly adapts its trust when suggester quality drifts, and strategically limits costly queries. A final heuristic-suggester ablation demonstrates robustness to model mismatch."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel integration of latent suggester reliability and agent-initiated queries inside a single Bayesian decision-theoretic framework.\nSound modeling choice: MOMDP factorization keeps the hidden state small, enabling off-the-shelf solvers (SARSOP) to scale to the augmented state space."}, "weaknesses": {"value": "Discrete-type assumption: real-world reliability is almost certainly continuous and context-dependent; the chosen five-point discretization may be too coarse and is not motivated by data.\nScalability concerns: experiments are limited to small toy domains; the hidden component Y×T is still |T| times larger, which will hurt solvers when |S| or the horizon grows.\nLimited novelty in ask mechanism: “query=information-gathering action” is well-known in POMDP sensor management; the paper does not theoretically analyze value-of-information or provide new solver tricks."}, "questions": {"value": "1. How sensitive are the policies to the granularity of T and to the exact numeric ask-cost used? Any theoretical analysis?\n2. Experiments are limited to small toy domains, which is not convincing. Please outline how the same POMDP formulation would be instantiated when the action space is continuous (e.g., 2-D mouse drag, 6-DOF robot joint commands) and suggestions arrive as natural-language or GUI-event streams. Would you discretize the continuous space, or move to a continuous-state POMDP / POMDP-lite solver?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Dgr2HVajMB", "forum": "j8UGvohcpR", "replyto": "j8UGvohcpR", "signatures": ["ICLR.cc/2026/Conference/Submission22389/Reviewer_Qnk9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22389/Reviewer_Qnk9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896004479, "cdate": 1761896004479, "tmdate": 1762942196044, "mdate": 1762942196044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers integrating external suggestions (e.g. from humans) into autonomous decision-making. In this context, the paper proposes modelling (i) suggester’s suggestion to be distributed as a tempered action-value function with a (temperature) rationality parameter $\\lambda \\geq 0$, (ii) dynamic suggester type (that is characterized by discretized $\\lambda$) as a lazy random walk, and (iii) incorporating “ask action” option to request suggestion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method section 3 is easy to follow and the proposed contributions/components are introduced clearly with motivations. The paper also experiments in the setting where the proposed suggester model is misspecified (Section 5.4)."}, "weaknesses": {"value": "The contributions/components (i)–(iii) listed in the summary box are somewhat orthogonal, especially (i)–(ii) relative to (iii). Without comprehensive empirical experiments demonstrating a significant performance improvement over justified baselines, the overall contribution looks like a sum of incremental components. Further, proper ablation studies are critical in this case to understand the strengths and weaknesses of the individual components (maybe Tables 1–2 may touch on this, but it is difficult to discern without a clear narrative thread in the main text).\n\nExperimental Section 5 is insufficient: it lacks proper discussion of the hypothesis, baselines, and evaluation metrics. For this reason, it is difficult to judge (i) whether the proposed method preforms well overall, (ii) what are the components of the proposed method that contribute the most, i.e. ablation studies, and (iii) what is the trade-off between the increased computational complexity and the improvement in empirical performance.\n \n“Results summarized in Table 5 indicate that incorporating heuristic-based suggestions within our noisy rational modeling framework significantly improved agent performance compared to scenarios lacking suggestions.” Without proper discussion of the experimental setup conclusions like that are difficult to judge. \n\n“Numerous simulations were conducted to ensure statistical robustness.” This is too vague, etc."}, "questions": {"value": "As $\\lambda$ is continuous, why not treat is as such rather than discretize? Is there some other reason to keep it discrete than convenience?\n\n“…We address this through a two-stage approach: first solving the original POMDP without the ask action to derive state-action values, then using these values to parameterize the suggestion observation model…” Sounds computationally heavy, does it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ADbxEW8ZdN", "forum": "j8UGvohcpR", "replyto": "j8UGvohcpR", "signatures": ["ICLR.cc/2026/Conference/Submission22389/Reviewer_ZMv2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22389/Reviewer_ZMv2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902944232, "cdate": 1761902944232, "tmdate": 1762942195833, "mdate": 1762942195833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies autonomous agents operating in POMDPs who receive external action suggestions (e.g., from a human or another agent) whose reliability may vary over time. Prior work typically assumes fixed and known suggester reliability, which does not reflect real human behavior or real-world sensing systems. Main contributions are: \n- Model suggester reliability as a latent variable and infer it dynamically via Bayesian updates.\n- Introduce an explicit ask action, enabling strategic querying of suggestions under cost.\n- Demonstrate robust adaptation to varying suggestion quality and ability to avoid low-value queries.\n- Show empirical results across Tag and RockSample, with both rational and heuristic suggesters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Motivation: \n(1) Addresses a real and growing need in human-AI teaming: adapting trust to variable advice quality.\n(2) Aligned with the trend of interactive assistance and trust calibration.\n\n- Formulation:\nPOMDP and MOMDP are modeled in the scenarios: (1) Present a solid use of the MOMDP structure to efficiently manage the expanded state space introduced by modeling suggester reliability as a latent variable. (2) The Bayesian update mechanism for jointly inferring environment state and suggester quality is principled and well motivated.\n\n- Experiments:\n(1) The study covers a broad range of settings, including static, dynamic, and heuristic suggesters, as well as scenarios involving ask costs and limitations on querying. (2) It further provides informative ablations, such as fixed-λ models, discrete-type inference, and dynamic type transitions, helping isolate the contributions of each component. (3) Include relevant baseline comparisons: normal agents, naive fixed-λ agents, noisy-rational suggesters, and multi-type agents in both static and dynamic configurations"}, "weaknesses": {"value": "- Human study missing: For human-trust motivation, no human-in-the-loop experiments are conducted. Although this paper acknowledges this, it is still important for this paper.\n\n- Scalability: Tag and RockSample are standard but small. What if (1) the larger POMDP domains, (2) higher-dimensional latent human models, (3) multiple suggesters or groups of helpers.\n\n- Reliance on **known** Q-values: The ask suggestion model uses pre-solved Q values. What if (1) Q is inaccurate, (2) Q value needs to be learnt. Is it possible to apply to RL or online learning settings.\n\n\n- \"Does your AI agent get you? A personalizable framework for approximating human models from argumentation-based dialogue traces\". This paper seems also estimating the belief."}, "questions": {"value": "Your method discretizes suggester rationality (λ) into a small fixed set:\n- Q1: How sensitive is performance to the choice of λ grid values (e.g., {0,1,2,5,10})?\n\n- Q2: Would adaptive or continuous inference over λ (e.g., particle filtering or Bayesian regression) further improve performance?\n\n- Q3: If λ lies between grid points, how does belief estimation degrade?\n\n\nThe ask-action model assumes access to accurate Q-values from the solved base POMDP.\n\n- Q4: How robust is the ask mechanism when Q-values are approximate or learned online (e.g., under model mismatch or RL)?\n\n- Q5: Could errors in estimated Q(s,a) lead to biased belief updates about suggester reliability?\n\nExperiments: Tag and RockSample.\n\n- Q6: How does the computational cost scale with the number of suggester types and belief complexity?\n\n- Q7: Could the method scale to larger, continuous-state problems or multi-human settings?\n\nHuman-in-the-loop:\n- Q8: Do you anticipate additional challenges when suggesters are real humans? How would you integrate explicit human feedback or confidence signals"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tK1NET2TCE", "forum": "j8UGvohcpR", "replyto": "j8UGvohcpR", "signatures": ["ICLR.cc/2026/Conference/Submission22389/Reviewer_vhfw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22389/Reviewer_vhfw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926293789, "cdate": 1761926293789, "tmdate": 1762942195574, "mdate": 1762942195574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the human-AI interaction problem under a POMDP (or MOMDP) framework. Human, as the suggester, provides occasional suggestions to the AI (autonomous agent) in a sequential decision-making environment, and the AI can utilize the suggestions to refine its belief of the underlying state. The suggestion is also captured by a quality parameter to reflect different levels of confidence during the suggestion. The levels will also be used in belief updating and thus the decision-making of the AI."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to follow. The model is well explained and provided with nice intuitions."}, "weaknesses": {"value": "My main concern is the contribution of the paper:\n\nThe paper should be viewed more as a \"conceptual\" work. As noted above, the model is newly proposed and well-explained, but I find it hard to apply it in a real-world scenario. For the following reasons:\n- Solving such a model requires knowing a lot of parameters like the transition matrix, the noisy rational suggester model, etc. \n- Generally, the POMDP framework makes the model inapplicable to a real-world scenario with a moderate state space size.\n\nThe key model component is from (Asmar & Kochenderfer, 2022) and there is no algorithm specifically designed for the model. \n- Would there be algorithms that can utilize the model structure to solve the problem more efficiently?\n- Any theoretical guarantee for the case if the model is misspecified or the parameter wrongly estimated?"}, "questions": {"value": "See above,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IgX9lnQOaB", "forum": "j8UGvohcpR", "replyto": "j8UGvohcpR", "signatures": ["ICLR.cc/2026/Conference/Submission22389/Reviewer_b1yP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22389/Reviewer_b1yP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954574504, "cdate": 1761954574504, "tmdate": 1762942195346, "mdate": 1762942195346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}