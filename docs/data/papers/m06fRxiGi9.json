{"id": "m06fRxiGi9", "number": 24778, "cdate": 1758360268582, "mdate": 1759896749158, "content": {"title": "VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs", "abstract": "Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28\\%, while natural tasks yield better but still weak results with 45\\% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56\\% of failures arise from perception alone, 43\\% from both perception and reasoning, and only a mere 1\\% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.", "tldr": "", "keywords": ["Visual Reasoning", "Vision Language Models", "Benchmark", "IQ"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e65fa664e93389ed629df61aae20dd77acc393e3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces VRIQ (Visual Reasoning IQ), a novel benchmark designed to evaluate the visual reasoning ability of Vision-Language Models (VLMs). The benchmark consists of 440 expert-authored items across five IQ test categories (Sequence Completion, 3D Visualization, Figure Rotation, Odd One Out, Matrix Prediction), each with both abstract puzzle-style and natural-image reasoning tasks to enable direct cross-domain comparison. The authors adopt a three-tier diagnostic evaluation framework: Tier 1 measures end-to-end accuracy, Tier 2 uses perceptual probes (isolating visual attribute extraction) and reasoning probes (text-only logical inference), and Tier 3 classifies failures into perception-only, reasoning-only, or combined types. Experimental results show that VLMs perform poorly on abstract tasks (average ~28% accuracy, near random) and moderately better on natural tasks (~45% accuracy). Tool-augmented models like GPT-o3 yield notable improvements but remain limited. Diagnostic analysis reveals that ~56% of failures stem from perception alone, ~43% from both perception and reasoning, and only ~1% from reasoning alone. Fine-grained probes further identify high failure rates in perception categories like counting (73.8%), position (58.2%), and rotation/orientation (52.4%). The paper concludes that current VLMs’ visual reasoning limitations are mainly due to perceptual bottlenecks, providing a diagnostic basis for model improvement."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clarity: The paper has a logical flow—starting with the motivation of evaluating nonverbal reasoning in VLMs, followed by benchmark design, evaluation framework, experiments, and analysis. \n2. Originality: The paper fills a gap in existing visual reasoning benchmarks by constructing parallel abstract-natural task families with identical logical structures, enabling controlled comparison of VLMs’ performance across symbolic and semantics-grounded reasoning. The hierarchical diagnostic probe framework (perceptual vs. reasoning isolation) is innovative, moving beyond monolithic accuracy evaluation to precise failure attribution."}, "weaknesses": {"value": "- Limited benchmark scale: With only 440 total questions, the sample size is small for robust statistical analysis, especially when evaluating fine-grained perception categories. This may lead to unstable accuracy estimates, particularly for tasks with low model performance near random guess.\n- Inadequate model coverage: The evaluated models lack diversity in two aspects: (1) No large-scale open-source VLMs (e.g., InternVL3-26B, Qwen2.5-VL-14B) are included, making it hard to assess the impact of model scale on visual reasoning; (2) Key SOTA proprietary models like Gemini-2.5-Pro and Seed1.5-VL are missing, my preliminary tests (w. examples in figure 1) with these models show high accuracy, which may weaken the generalizability of the conclusion that VLMs are unreliable abstract reasoners.\n- Accuracy measurement vulnerability to randomness: Reporting raw accuracy for single-choice questions (with 4–5 options) is prone to randomness, especially when model performance is near the random baseline. Unlike benchmarks like MMBench [1] that use circular evaluation to mitigate this, the paper’s current metrics may overstate or understate true model capabilities, reducing conclusion credibility.\n- Insufficient illustrative details for tool-augmented reasoning: Section 6.1 highlights that tool-augmented models (GPT-o3) achieve significant improvements, but no example traces of the tool-use process are provided. This makes it difficult to understand how tools (e.g., cropping, rotation) address perception bottlenecks, limiting the insight for developing tool-augmented VLM systems.\n\n[1] MMBench: Is Your Multi-modal Model an All-around Player?"}, "questions": {"value": "- For Figure 1 (Sequence Completion - Natural Reasoning), the correct answer is unclear. Could you clarify the answer and the underlying logical rule (e.g., sequence of object attributes or transformations) to help verify task validity?\n- In Table 2, the random guess accuracy for 3D V (3D Visualization) is 20%, but Figure 1 shows a 3D V example with 4 choices (suggesting 25% random guess). Please explain the discrepancy—are 3D V questions designed with 5 options, or is there another reason for the 20% baseline?\n- In Line 420, you mention \"QwenB\" as one of the models for diagnostic analysis, but this model is not defined in Section 5 (Experimental Setup). Could you clarify whether this is a typo (e.g., Qwen2.5-VL-3B-Instruct) or an unmentioned model, and provide its details?\n\nPlease also help resolve concerns in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j3oYwowur3", "forum": "m06fRxiGi9", "replyto": "m06fRxiGi9", "signatures": ["ICLR.cc/2026/Conference/Submission24778/Reviewer_pJTp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24778/Reviewer_pJTp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380309953, "cdate": 1761380309953, "tmdate": 1762943195376, "mdate": 1762943195376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the evaluation of visual reasoning capabilities in Visual Language Models and proposes the diagnostic benchmark VRIQ. Through a parallel task design of \"abstract puzzles + natural image reasoning\" and a three-tier evaluation framework (end-to-end accuracy - diagnostic probes - error classification), it systematically analyzes the performance and shortcomings of current VLMs in visual reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and organized.\n2. The proposed three-tier evaluation framework decouples perception and reasoning as an integrated capability. It precisely attributes model failures to three categories: perception errors only, reasoning errors only and errors in both.\n3. It simultaneously evaluates open-source and proprietary models (such as Qwen, GPT-4o, etc.), making it quite comprehensive. The result analysis is in-depth, with an appropriate combination of quantitative and qualitative approaches."}, "weaknesses": {"value": "1. Although the authors claim that VRIQ is the first to conduct a parallel comparison in the abstract-natural dual domain, this setup is similar to MLLM IQ benchmarks such as MMIQ and MARVEL. It is hard to clearly demonstrate the unique advantages of VRIQ in terms of diagnostic accuracy or research inspiration.\n2. The VRIQ benchmark contains a total of 440 questions, and the sample size of some reasoning categories is quite small. Small samples may lead to contingency in evaluation results.\n3. For the tasks in the natural reasoning categories, if the scores are too high or relatively concentrated, this may mean that the benchmark lacks the ability to distinguish the capabilities between models.\n4. For open-source models, only smaller ones have been tested. Models of 72B or larger with stronger capabilities have not been tested. For closed-source models, Claude and Gemini have not been tested.\n5. Human reference performance should be provided to support the assertion that \"humans can easily solve, but models fail\"."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xSrjlPhree", "forum": "m06fRxiGi9", "replyto": "m06fRxiGi9", "signatures": ["ICLR.cc/2026/Conference/Submission24778/Reviewer_iAvs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24778/Reviewer_iAvs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536266657, "cdate": 1761536266657, "tmdate": 1762943195005, "mdate": 1762943195005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VRIQ, a diagnostic benchmark spanning five IQ-style categories across abstract and natural images, plus a two-tier probe methodology to attribute failures to perception vs. reasoning. Results suggest most errors are perceptual rather than reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The probe study is valuable. The perception-vs-reasoning decomposition and finer probe tags (count, position, rotation, 3D, etc.) are a useful analysis lens beyond aggregate accuracy."}, "weaknesses": {"value": "- Small sample size. The core benchmark (440 items) limits statistical power. In addition, category counts are unbalanced between abstract and nature samples.\n\n- The images are mix of open repositories and model-generated images, but sourcing, licenses are not described in the paper. Also how the images are generated using models are not described.  \n\n- The process for creating and validating data samples is not documented.\n\n- Claims about “thinking with images” and tool use lack a transparent harness: was this an API implementation, what visual tools were allowed, how calls were budgeted/capped, decoding params, etc."}, "questions": {"value": "- How the images are captured and how the final answers are validated? Please mention such important process in detail. \n\n- How the o3 baseline model is implemented? What are the allowed tools?\n\n- o3 is the only reasoning model being evaluated, it is worth to include other reasoning models, such as Gemini-2.5-pro, Claude-thinking modes, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ut7qK3T0Gh", "forum": "m06fRxiGi9", "replyto": "m06fRxiGi9", "signatures": ["ICLR.cc/2026/Conference/Submission24778/Reviewer_eq1H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24778/Reviewer_eq1H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674888578, "cdate": 1761674888578, "tmdate": 1762943194476, "mdate": 1762943194476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel benchmark named VRIQ (Visual Reasoning IQ), designed to assess the visual reasoning abilities of Vision Language Models (VLMs). A core design principle of this benchmark is its \"parallel domain construction,\" which includes both abstract, IQ-test-like puzzle tasks and natural image tasks that utilize the same reasoning logic (e.g., \"odd one out\" or \"sequence completion\"). The paper's main contribution is the introduction of a three-tier diagnostic framework that uses \"Perceptual Probes\" and \"Reasoning Probes\" to decouple the root causes of model failures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. As VLMs become more capable, it is crucial to discern whether they possess genuine reasoning abilities or merely rely on superficial statistical features from training data (shortcut learning). This paper directly addresses this core issue.\n\n2. The conclusion that \"the bottleneck is perception, not reasoning,\" while based on this specific dataset, is highly insightful. It points to a clear direction for future VLM improvements (i.e., strengthening foundational visual perception, especially spatial, counting, and 3D understanding)."}, "weaknesses": {"value": "1. The entire benchmark (VRIQ) contains only N=440 samples. In the fine-grained breakdown across 5 categories and 2 domains, aome sub-categories have an extremely small number of samples (e.g., Table 1 shows that each of the 5 \"natural\" domain categories has only 20 questions).\n\n2. Due to the small sample size, the statistical reliability of the results is low. On a test set with only 20 questions, getting one or two more questions right or wrong causes a large fluctuation in accuracy (5%-10%). Therefore, conclusions drawn from such a small dataset (especially comparisons between models) are not robust.\n\n3. The paper states VRIQ is \"expert-authored\" but provides no details on the creation process, quality control, or how data contamination was avoided (i.e., whether these IQ problems already exist online and might have been part of the VLMs' training data).\n\n4. The paper defines \"reasoning\" as the ability tested by \"text probes.\" However, this actually tests textual reasoning. A model might reason perfectly in text but fail to perform the same logical operation visually. Furthermore, some tasks classified as \"perception\" (like 3D visualization or counting) inherently involve complex reasoning. This binary split may oversimplify the problem."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gguqoW4IAH", "forum": "m06fRxiGi9", "replyto": "m06fRxiGi9", "signatures": ["ICLR.cc/2026/Conference/Submission24778/Reviewer_fxjD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24778/Reviewer_fxjD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911398825, "cdate": 1761911398825, "tmdate": 1762943193962, "mdate": 1762943193962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}