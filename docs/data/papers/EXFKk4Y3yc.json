{"id": "EXFKk4Y3yc", "number": 87, "cdate": 1756728629088, "mdate": 1759898276246, "content": {"title": "Spilled Energy in Large Language Models", "abstract": "We reinterpret the final softmax classifier over the vocabulary of Large Language Models (LLM) as an Energy-based Model (EBM). This allows us to decompose the chain of probabilities used in sequence-to-sequence modeling as multiple EBMs that interact together at inference time. Our decomposition offers a principled approach to measuring where the \"energy spills\" in LLM decoding, empirically showing that spilled energy correlates well with factual errors, inaccuracies, biases, and failures. Similar to Orgad et al. (2025), we localize the exact token associated with the answer, yet, unlike them, who need to train a classifier and ablate which activations to feed to it, we propose a method to detect hallucinations *completely training-free that naturally generalizes across tasks and LLMs* by using the output logits across subsequent generation steps. We propose two ways to detect hallucinations: the first one that measures the difference between two quantities that we call **spilled energy**, measuring the difference between energy values across two generation steps that mathematically should be equal; the other is **marginal energy**, which we can measure at a single step.\nUnlike prior work, our method is training-free, mathematically principled, and demonstrates strong cross-dataset generalization: we scale our analysis to state-of-the-art LLMs, including LLaMa-3, Mistral, and Qwen-3, evaluating on nine benchmarks and achieving competitive performance with robust results across datasets and different LLMs.", "tldr": "We recast the LLM softmax as an Energy-Based Model, introducing training-free energy measures to detect hallucinations. Our method pinpoints errors, generalizes across tasks, and shows robust results on nine benchmarks.", "keywords": ["LLM", "hallucination detection", "EBM"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7f4a295dde283e8da45345b35965fcf90a31fbf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method for detecting hallucinations in language models by interpreting the model's sequential predictions as quotients of two different energy-based models (EBMs) denoted $E_\\theta^{\\ell}$ and $E_\\theta^m$. The paper defines the spilled energy as the difference between the two energies: $\\Delta E_\\theta(\\mathbf{x}_{0:i}) = E_\\theta^\\ell(\\mathbf{x}_{0:i}) - E_\\theta^m(\\mathbf{x}_{0:i})$. \n\nThe paper then proposes several measures for hallucination detection, including:\n- marginal energy $E_\\theta^m$,\n- spilled energy $\\Delta E_\\theta$, and\n- scaled spilled energy $\\Delta E_s := |E_\\theta^m| \\Delta E_\\theta$.\n\nThese metrics are evaluated in two settings. First, a synthetic setup, involving the detection of increasingly smaller numerical errors introduced in arithmetic computations of 13-digit integers. Spilled energy ($\\Delta E_\\theta$) gets strictly superior ROC curves (better pointwise, not just in AUC) than baselines across Llama-3-8B (both instruct and non-instruct versions), Qwen-3-8B, and Mistral-7B-Instruct. \n\nSecond, the proposed methods are tested in detecting errors in reasoning tasks across several benchmarks. To do so, the *exact answer tokens* are detected, and the different methods are computed across them using different pooling methods. The different methods are compared against logit energy and [1] using AuROC scores.\n\n---\n\n[1] Orgad, H., Toker, M., Gekhman, Z., Reichart, R., Szpektor, I., Kotek, H. and Belinkov, Y., 2024. Llms know more than they show: On the intrinsic representation of llm hallucinations. arXiv preprint arXiv:2410.02707."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **(S1)** The proposed methods are efficient, simple, generic, easy to implement, can be applied to any language model given access to its logits (gray-box access), and extend previous hallucination detection methods.\n\n- **(S2)** The proposed methods are evaluated on a wide range of models and tasks, and show strong performance. The synthetic setup is well-designed, and the reasoning tasks are representative of the kind of tasks where hallucinations are likely to occur in real-world applications.\n\n- **(S3)** The entire derivation of spilled energy is presented in the main text and is not hidden in the appendix. The derivation is clear and easy to follow (if a bit cumbersome)."}, "weaknesses": {"value": "- **(W1)** The paper does not provide a theoretical justification for why spilled energy (or the other proposed methods) is a good indicator of hallucinations. It is just an empirical observation. Moreover, the intuitive explanation is limited to:\n  > ...the two energies, not interacting at the same step but at steps $i$ and $ i-1$, should be equal, but they are measured in the LLM at different generation steps and from different components.\n\n  and\n  > Since both terms on the right side should be equal to $E_\\theta (\\mathbf{x}_{i:1})$, delta values should always be zero when we are correctly modeling the energy at timestep $i$.\n\n  Which are unclear to me (see question 1).\n\n- **(W2)** While the authors mention that using the exact answer is critical for deployment, the paper is vague on the exact method used to detect these exact answer tokens. The only mention I found is in Section 4.2: \n  > We identify this span by prompting the LLM for a brief answer.\n\n- **(W3)** The derivation of spilled energy is more cumbersome and complicated than it should be. E.g., equations (1) and (2) are redundant (one of them suffices), and equations (3)-(5) can be united into one. This is a minor weakness."}, "questions": {"value": "- **(Q1)** Why do the authors claim that:\n>  ... delta values should always be zero when we are correctly modeling the energy at timestep $i$\n\n- **(Q2)** How exactly are the exact answer tokens detected?\n\n- **(Q3)** When comparing with [1] in Table 1, are the probes retrained on each dataset? If not, which datasets are they trained on?\n\nGiven a strong answer to **Q1**, addressing **W1**, I would be open to increasing the score. \n\n---\n\n[1] Orgad, H., Toker, M., Gekhman, Z., Reichart, R., Szpektor, I., Kotek, H. and Belinkov, Y., 2024. Llms know more than they show: On the intrinsic representation of llm hallucinations. arXiv preprint arXiv:2410.02707."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AAsINIAW18", "forum": "EXFKk4Y3yc", "replyto": "EXFKk4Y3yc", "signatures": ["ICLR.cc/2026/Conference/Submission87/Reviewer_LpMe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission87/Reviewer_LpMe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission87/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482978042, "cdate": 1761482978042, "tmdate": 1762915448325, "mdate": 1762915448325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free method for detecting hallucinations in LLM. The core idea is to reinterpret the final softmax layer of an LLM as an Energy-Based Model (EBM). The paper define a variable named spilled energy that quantifies the differences between two energy values across consecutive generation steps that should ideally be equal. The paper has a hypothesis that a big spilled energy is correlate with hallucination in model generation. The authors test this hypothesis on both synthetic task and real-world benchmarks. The results show that spilled energy is a strong signal for detecting hallucination. Their method outperforms baseline method, although there could still be false positive cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formulate the LLM's generation procedure as energy based model, which enable the following definition of \"spilled energy\" for hallucination detection. \n2. The method proposed in the paper is a training-free method which make it lightweight and applicable to most LLM for hallucination detection. Empirically, the paper show results on both sythentic and realworld dataset, which validate that the method may have generalization across different domains. While other works (non-training free) show worse performance under new datasets, the paper’s method show robustness in performance.\n3. The experiments in the paper are solide. The author provide results on sythentic dataset, which show the metric's ability to separate correct from incorrect answers. The results on realworld datasets shows the method's valid on a wide range of domains. The results are validated across model family as well.\n4. The ablation section also consolidate the findings. The authors demonstrate the critical importance of localizing the \"exact answer\" tokens, showing that doing so provides a good performance gain."}, "weaknesses": {"value": "1. The method needs to first identify the specific token span constituting the \"exact answer\". The paper implement this by \"prompting the LLM for a brief answer”. My concern is that what if the LLM's \"brief answer\" is itself a hallucinated? What if the “exact answer” is wrongly identified?\n2. The current evaluation focuses on tasks with answer that can be localized to a short range(words). My question is that how the method perform on more subtle hallucinations, such as a mutilple incorrect words/phrases in a long paragraph or complex factual error that is not contained in a single noun phrase.  \n3. In limitation, the author acknowledge that the method can produce false positives on tokens that are not semantically informative, such as punctuation. While this might be mitigated this with answer localization, it suggests the raw signal is noisy and may not be a pure measure of semantic or factual correctness. Can we do better to reduce the false positive? \n4. If energy gets “spilled”, it is a sign the model’s prediction doesn’t align well with its internal probability distribution. Can you give more explanations why we can use this to detect hallucination? I am not very convinced by current explaination in paper.\n5. Each time, we need to inference the model twice, the extra latency make the method impractical"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lr34cBV6eS", "forum": "EXFKk4Y3yc", "replyto": "EXFKk4Y3yc", "signatures": ["ICLR.cc/2026/Conference/Submission87/Reviewer_Xm2r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission87/Reviewer_Xm2r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission87/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595542372, "cdate": 1761595542372, "tmdate": 1762915448186, "mdate": 1762915448186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free method to detect hallucinations in Large Language Models (LLMs) by reinterpreting the final softmax classifier as an Energy-based Model (EBM). This reinterpretation allows the authors to decompose the sequence-to-sequence probability chain into multiple interacting EBMs.\n\nThe core contribution is the introduction of \"spilled energy\", which measures the discrepancy between two energy quantities that should be mathematically equal across consecutive generation steps, but differ in the LLM's implementation due to the way marginal and joint probabilities are computed. The authors also propose a complementary metric, marginal energy, which is measurable at a single step.\n\nThe paper empirically demonstrates that high spilled energy strongly correlates with LLM hallucinations. Crucially, their method is training-free and shows strong cross-dataset generalization across state-of-the-art open-sourced LLMs (LLaMa-3, Mistral, Qwen-3) and nine benchmarks including synthetic and real-world datasets. It localizes the signal to the \"exact answer tokens\" and outperforms the logit confidence baseline and prior probe classifier approaches, especially in cross-dataset settings where probe classifiers fail to generalize."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of reinterpreting the LLM's autoregressive sequence modeling as a chain of Energy-based Models (EBMs) is novel and very interesting.\n2. The implementation of the hallucination detector is simple and training free, but achieve a good generalization across synthetic and real-world datsets. \n3. The technical motivation and derivations are sound, and the method is well mathmatically-grounded.\n4. The paper is well-written and the presentation is clear."}, "weaknesses": {"value": "1. There might be some technical details make the detector hard to be applicable in the real-world tasks.\n    - The paper emphasizes that localizing the signal to the \"exact answer tokens\" is essential. However, correctly to find the exact answer tokens might be non-trivial. In the paper, authors identify this span by prompting the LLM for a brief answer seems to be a bit fragile to me. This is essentially a dependence on a pre-processing step that relies on LLM's outputs, which may introduce noise or additional complexity. \n   - Compared to logit confidence, spill energy requires calculation of the quantities using two-step predictions, which may introduce the additional cost. Marginal energy only needs the one-step calculation but the results for two energy forms are quite mixed.\n    - The analysis focuses on the \"exact answer\" tokens which can moslty be applied to QA or classification tasks. How well would the Spilled Energy perform on detecting other types of hallucinations common in LLMs, such as self-contradictions or source-attributable error that might occur earlier in a long-form generated answer or reasoning trace, outside of the specific final answer tokens?"}, "questions": {"value": "1. Why do we observe the cross-dataset results that for non-aligned models (Mistral), marginal energy sometimes slightly outperforms spilled energy? \n\n2. Is there any insight why marginal energy is a superior signal in the pre-trained model setting, or why instruction-tuning seems to amplify the margin for spilled energy? Maybe a more in-depth analysis into comparing two forms of energy is interesting for the pre-trained and instruction-tuned models.\n\nOther questions are listed in the bulletpoints of weakness as the justification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cWvcE7IMFT", "forum": "EXFKk4Y3yc", "replyto": "EXFKk4Y3yc", "signatures": ["ICLR.cc/2026/Conference/Submission87/Reviewer_zjL6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission87/Reviewer_zjL6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission87/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811025368, "cdate": 1761811025368, "tmdate": 1762915448032, "mdate": 1762915448032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reinterprets the final softmax layer of large language models (LLMs) as an energy-based model (EBM). From this perspective, the authors claim to identify a discrepancy between two energy formulations, termed \\emph{spilled energy}, which correlates strongly with hallucinations. Unlike probing classifiers, spilled energy requires no additional training. Experiments on LLaMa-3, Mistral, and Qwen-3 across nine benchmarks show that spilled energy outperforms logits and probing methods in detecting hallucinations in the cross dataset setup."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper considers a very timely and important problem of hallucination detection.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- In my view, the reliance on exact tokens is a significant limitation. I am familiar with the paper by Orgad et al., which was the first to demonstrate that identifying the right token can improve detection. However, I see this primarily as an interesting observation rather than a practical method for hallucination detection. The reason is that identifying such tokens typically requires external algorithms or the use of other LLMs, which leads to very high latency—making it impractical for real-world hallucination detection.\n- I find the concept of “spilled energy” somewhat unclear and potentially inconsistent. Could you please see my first question below for further context?\n- The work appears to lack some important baselines, and the experimantal setup is unclear. For example, in my view, it would be valuable to include comparisons with Ptrue and Semantic Entropy [1]. In addition, some key comparisons are not clearly presented—for instance, I did not see results benchamarked against the baselines (even those already considered in the paper) under the standard setup (e.g., Orgad et al. when trained and tested on the same dataset). I would appreciate clarification on this point. If such a comparison is not included (and only the “cross-dataset” case is considered), it would significantly weaken the strength of the paper.\n\n**References:**\n\n[1] Kuhn et al., Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation, 2023"}, "questions": {"value": "- I have a question regarding the following sentence (quoting from the paper): \n\"E^{\\ell}_{\\theta} and E^{m}_{\\theta} are computed from the output of the model, but with two key differences: E^{\\ell}_{\\theta} is obtained as a single logit extracted using the id of the sampled token, while E^{m}_{\\theta} is computed by marginalizing over all id's in the vocabulary.\"\n It seems that you are associating the logit of a single token with the energy function of all the preceding tokens as well. Could you clarify why this is considered correct? This association feels inconsistent: if E^{\\ell}_{\\theta} corresponds to a single token, then why does E^{m}_{\\theta} correspond to the normalization factor over the entire vocabulary? More concretely, I would have expected E^{m}_{\\theta} to represent the prediction for the specific token immediately preceding the one associated with E^{\\ell}_{\\theta}, rather than the marginalized quantity. Consequently, the idea of “spilled energy” seems problematic, as it appears to rely on an inconsistency in how energy is defined. I would be happy to get a clarification on that.\n- Could you clarify what is meant by “cross-dataset” in Table 1? This is not explained anywhere, and I find it difficult to understand the setup. Since your method does not require training, it is not clear to me how cross-dataset evaluation is applicable here. I do see how this makes sense in the context of Orgad et al.—since their probing method requires training—but then, what would be considered the standard experimental setup for them without cross-dataset evaluation (if its not Table 1)? Additionally, could you clarify how the cross-dataset setup in Table 1 is applied to Orgad et al.? Specifically, what is the exact setup—what was the model trained on, and what was it tested on?\n- Could you clarify what the superscripts m and l represent in Equation (4)?\n- In the Abstract, you write: “we propose a method to detect hallucinations completely training-free that naturally generalizes across tasks and LLMs.” Could you please point out where in the paper you demonstrate generalization across LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QPj759Tm7F", "forum": "EXFKk4Y3yc", "replyto": "EXFKk4Y3yc", "signatures": ["ICLR.cc/2026/Conference/Submission87/Reviewer_qt4T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission87/Reviewer_qt4T"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission87/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866190890, "cdate": 1761866190890, "tmdate": 1762915447776, "mdate": 1762915447776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}