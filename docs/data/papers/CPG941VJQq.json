{"id": "CPG941VJQq", "number": 18207, "cdate": 1758285148291, "mdate": 1759897119602, "content": {"title": "RankGen: A Statistically Robust Framework for Ranking Generative Models Using Classifier-Based Metrics", "abstract": "Standard metrics for evaluating generative models are brittle, easy to game, and often ignore task relevance. We introduce RankGen, a unified evaluation framework built on four metrics: Quality, Utility, Indistinguishability, and Similarity; each designed to capture a distinct failure mode and supported by PAC-style generalization bounds. RankGen follows a two-stage process: models that violate bounds are discarded, while the rest are ranked using robust, quantile-based summaries. The resulting composite score, Exchangeability, captures both fidelity and task relevance. By exposing hidden pathologies such as memorization, RankGen provides a principled foundation for safer model selection and deployment.", "tldr": "RankGen is a statistically robust framework for evaluating generative models via four classifier-based metrics with PAC-style guarantees, yielding interpretable rankings and exposing failure modes such as memorization that standard metrics overlook.", "keywords": ["Generative models", "Model evaluation", "PAC-style bounds", "Classifier-based probes", "Evaluation Metrics"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bf6e76ae503051efc9ed8b8b78068e3b4336f15.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces RankGen, a unified and statistically robust framework for evaluating and ranking generative models, designed to overcome the brittleness and shortcomings of standard, easy-to-game evaluation metrics. RankGen utilizes four PAC-style generalization-bound classifier-based metrics—Quality, Utility, Indistinguishability, and Similarity—to capture distinct failure modes like low fidelity, memorization, distribution shift, and mode collapse."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. a new method different with the previous FID, IS, Precision&Recall\n2. give the new definition for generative quality"}, "weaknesses": {"value": "1. The paper must clearly define the operational and conceptual distinction between Quality, Similarity, and Utility. . All are to discuss the diversity situation.\n2. The paper uses \"Quality\" to assess distribution coverage (fit) rather than the standard meaning of sample fidelity (realism). This non-standard usage must be explicitly stated early on to avoid confusion with existing generative metrics (e.g., FID).\n3. The discussion on the metric's diagnostic role, particularly concerning mode collapse (Sec 3.3, 3.6), needs significant expansion. This is a critical phenomenon that requires a deeper analysis, including differentiating between fidelity collapse and diversity collapse. The diagnostic role should be systematically extended to other failure modes (e.g., over-generalization, memorization).\n4. similar to 3, the other Diagnostic role from sec 3.5 and 3.6  should also discuss more. \n5. In the paper, the reliance on two specific training sets, ($train_1$ and $train_2$), severely limits applicability.\nUnlabeled Data: The authors must propose a methodology to guarantee the distinguishability of  $train_1$ and $train_2$  when working with unlabeled datasets.\nConditional Models: The current Indistinguishability task may be inappropriate for labeled/conditional generative models. The authors must state this limitation or provide an adaptation for such models.\n6. Figure 2 is not clear. and figure 3 should give the score for the different models (more clear to see the rank)"}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G05Rp0vE6c", "forum": "CPG941VJQq", "replyto": "CPG941VJQq", "signatures": ["ICLR.cc/2026/Conference/Submission18207/Reviewer_kCSF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18207/Reviewer_kCSF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760699197389, "cdate": 1760699197389, "tmdate": 1762927952253, "mdate": 1762927952253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes four metrics to evaluate generative model performance on datasets including a classification label. The first measures the quality of generated data comparing the accuracies of classifiers trained on real and synthetic data. The second compares how much adding generated data to real data improves the accuracy of the classifier, compared to adding more real data. The third is how hard it is for a discriminator to distinguish real and generated data. The fourth measures how locally similar real and generated points are. The metrics are computed with multiple train-test splits to estimate uncertainty, and generators are ranked with a Monte-Carlo procedure taking the uncertainty into account. The proposed metrics are evaluated with some sanity checks with Gaussian features + binary label. The paper also evaluates several image and molecule generators with the proposed metrics."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper studies an important problem: generative model evaluation metrics are hard to interpret, and estimating their uncertainty is important."}, "weaknesses": {"value": "The writing of the paper shows signs of heavy LLM use. Some examples:\n- Line 159: the definition of $f^{(i)}$ does not make sense, since $i$ appears to be indexing over multiple train-test splits.\n- Line 159: not clear why $D^y_{train}$ is a parameter of $f^{(i)}$.\n- Symbols for the 4 metrics change in Section 3.7 from what was previously used.\n- The paper states that many parameters like underlying classifier and dataset size are swept in the sanity check (Section 4.1), but the results in Table 3 are not given over the whole sweeps, and there is no indication that the numbers in Table 3 are aggregated over results from the sweeps.\n- The numbers in Table 3 do not support the stated conclusions in lines 350-356.\n- Utility PAC-bound in Section 3.4 does not match the bound that is proven in eq. A.1. The numerators are different.\n- The proof of the similarity PAC-bound in Appendix 10 concludes with a different inequality than the one that is supposed to be proven (which appears before the concluding inequality).\n- Line 272: formulas for quantile-to-moment conversion rules do not appear in Appendix 5 as stated.\n- The last sentence of the paper is \"The duplicate metric tables appearing in prior drafts have been **removed** to avoid redundancy.\" (emphasis from paper).\n\nBesides, the evaluation of the proposed metrics is limited. The only actual evaluation of them is 5 sanity checks of perturbing data with Gaussian features. The rest of the experiments evaluate generative models with the new metrics, but these do not provide any evidence that the proposed metrics are useful since it is not possible to know what values a good metric would have. There are also no comparisons with previous metrics.\n\nIn addition, many important details are unclear:\n- Line 235: not clear what \"same domain\" means, which makes the whole definition of the similarity metric impossible to understand.\n- PAC bounds for similarity and indistinguishability are stated as a difference between the finite sample value and infinite sample value. It is not clear how one could compute the infinite-sample value to check that the bound is satisfied.\n- Classifiers behind the metrics for molecule and image evaluations are not specified.\n- Not clear how Rademacher complexities and VC-dimensions for the PAC-bounds are computed for actual classifiers.\n- The \"mode collapse\" test (Appendix 12) doesn't really test mode collapse since replacing generated points with similar ones from real data preserves model. Testing mode collapse with unimodal Gaussian features is not possible in any case.\n- The paper states that means and standard deviations of the metric values are not always reliable, and uses medians and interquartile ranges for this reason. But they are immediately converted to means and variances for the Monte-Carlo ranking procedure for some reason.\n- Line 158: $D^y\\_{train2}$ not defined.\n- Line 225: $\\mathrm{indist}^*$ is not defined until the Appendix.\n- Line 235: $D\\_{mix}$ not defined."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hGFiL8INoG", "forum": "CPG941VJQq", "replyto": "CPG941VJQq", "signatures": ["ICLR.cc/2026/Conference/Submission18207/Reviewer_Mvww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18207/Reviewer_Mvww"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819892393, "cdate": 1761819892393, "tmdate": 1762927951826, "mdate": 1762927951826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces four classifier‑based metrics—Quality, Utility, Indistinguishability, and Similarity—each capturing a distinct failure mode. Quality measures how well classifiers trained on synthetic data generalize compared to real data; it is defined as the normalized ratio of classification performance and comes with a PAC‑style lower bound. Utility measures how much synthetic data improves downstream performance beyond real data. Indistinguishability asks how difficult it is for a discriminator to tell real from generated samples. Similarity assesses whether real and synthetic samples share local neighborhoods via entropy of k‑NN domains. These metrics together probe fidelity gaps, redundancy, distributional shifts, and local mixing."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation: The paper points out that existing scalar heuristics (e.g., FID, IS) are brittle, lack statistical guarantees, and conflate fidelity with diversity. It argues convincingly that evaluation should be multi‑dimensional and diagnostic rather than a single score.\n\n- Each metric is accompanied by PAC‑style generalization bounds derived in the appendices. For instance, the quality score bound depends on Rademacher complexity, and the indistinguishability bound depends on VC dimension. This gives the framework a principled way to decide if an empirical score is statistically valid.\n\n- Robust Ranking Procedure: RankGen uses quartiles (median and interquartile range) to summarize heavy‑tailed metric distributions and Monte‑Carlo sampling with pairwise dominance counts to produce uncertainty‑aware rankings. Models that fail PAC bounds are filtered out, and surviving models are compared using robust summaries.\n\n- Diagnostic Interpretation: Instead of just ranking, RankGen explains why a model fails: e.g., a high similarity but low utility score signals memorization, while low indistinguishability reveals distribution shifts. This diagnostic approach can guide safer deployment."}, "weaknesses": {"value": "- All four metrics rely on a downstream classification task; they require labelled data and a predefined classifier architecture. In many generative settings (e.g., open‑domain image generation, text, audio) labels may be unavailable or the “task” may not be classification. The quality and utility scores hinge on the choice of classifier and evaluation metric, potentially biasing evaluation.\n\n- Computational complexity: RankGen entails multiple stratified splits, training at least five classifiers per generator (for quality, utility, indistinguishability, similarity), computing k‑NN neighborhoods, and Monte‑Carlo sampling for rankings. The method may be computationally heavy, particularly for high‑dimensional data.\n\n- Sensitivity to hyperparameters: Similarity requires selecting k (10–50); the ranking procedure samples from a truncated Gaussian with a ridge variance; the number of splits and δ allocations must be chosen. The paper does not explore sensitivity to these hyperparameters.\n\n- The composite score Emin takes the minimum of the predictive and alignment blocks, which can harshly penalize models that excel in one aspect while slightly underperforming in another. This may discard generators that are strong but specialized (e.g., high‑fidelity but low utility) even if they could be useful for certain applications. Similarly, filtering by PAC bounds may eliminate models that are slightly below threshold despite being practically useful.\n\n- Limited modalities and generators: The experiments focus on relatively small datasets (MNIST‑like) and small‑sized models (e.g., StyleGAN2‑lite, DCGAN). Large‑scale diffusion models (e.g., SDXL, Flux), autoregressive text models, or audio generators are not evaluated, leaving the generality of RankGen uncertain.\n\n- Classifier Dependence: The quality and utility metrics depend on the chosen classifier architecture and metric (accuracy, AUC, etc.), and similarity uses k‑NN on raw features rather than learned embeddings. Different choices could alter results; the paper does not examine robustness to these choices.\n\n- Presentation: The main paper is dense; key derivations, algorithm details, and hyperparameters are relegated to numerous appendices, which may hinder readability. The method introduces many moving parts, which can be daunting for practitioners seeking a simple evaluation protocol."}, "questions": {"value": "- How does RankGen handle unconditional generative models or generative tasks without obvious classification labels (e.g., open‑domain text generation, image captioning)? Could one use self‑supervised or regression tasks? Are there plans to extend the framework beyond classification?\n\n- Have you investigated the effect of varying the number of resampling splits, the k for similarity, or the δ allocations in the bounds? How should a practitioner choose these values?\n\n- What is the computational cost and generalization of RankGen on large datasets or high‑resolution images like ImageNet 512x512 or text-to-image dataset?\n\n- Did any generators fail the PAC bounds but still perform well empirically? Conversely, did any pass but exhibit poor generalization? An empirical study of bound accuracy would strengthen the theoretical claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9ZtHUWrUZW", "forum": "CPG941VJQq", "replyto": "CPG941VJQq", "signatures": ["ICLR.cc/2026/Conference/Submission18207/Reviewer_8HJX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18207/Reviewer_8HJX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867170466, "cdate": 1761867170466, "tmdate": 1762927951470, "mdate": 1762927951470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RankGen, a statistically grounded text-to-text model for ranking generated text. It reformulates text ranking as a conditional generation probability problem and applies statistical calibration to reduce bias from sequence length and distribution shift. Experiments across multiple NLG tasks show that RankGen aligns more closely with human judgments than existing automatic metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and easy to follow, with clear organization and presentation.\n- The results are interpretable and provide meaningful insights into the model’s behavior.\n- The research problem is interesting and relevant to the text generation and evaluation community."}, "weaknesses": {"value": "- The novelty is relatively limited, as the work mainly reformulates probabilistic ranking rather than introducing a new model architecture.\n\n- The paper lacks stronger comparisons with recent large model–based scoring or preference models, such as GPT-judge or reward models.\n\n- Reproducibility is limited since neither the code nor model weights are released.\n\n- The generalization ability remains uncertain, as the method has not been demonstrated on open-domain generation tasks such as dialogue"}, "questions": {"value": "See in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "r5h4tp7bYx", "forum": "CPG941VJQq", "replyto": "CPG941VJQq", "signatures": ["ICLR.cc/2026/Conference/Submission18207/Reviewer_EnTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18207/Reviewer_EnTj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974463942, "cdate": 1761974463942, "tmdate": 1762927951154, "mdate": 1762927951154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper defines and tests some measures that are meant to distinguish real from generated data. \n\nQuality is meant to measure the difference between accuracy when trained on real versus generated data. Utility is the same but only when part of the data is replaced.\n\nIndistinguishability is meant to measure the ability to tell apart real from generated data. Similarity tests the data with respect to a specific distinguisher (neighborhoods of a given sample).\n\nThe main contribution are these definitions. Numerous experiments calculate the requisite statistics on several datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Trying to make sense of the differences between real and generated data is a well-motivated question. This paper fleshes out and test some specific measures for this purpose."}, "weaknesses": {"value": "There appears to be a conceptual misunderstanding. If the output of a generative model is *indistinguishable* from the training data then no (efficient) test can tell the two apart. Given sufficient data, it is impossible that the quality measure is high but the two are indistinguishable because measuring the quality is a particular way to distinguish between the real and generated data.\n\nIt is therefore not sensible that the \"indistinguishability rank\" can be low but any of the other ones (like quality or utility) are high.  This is merely an indication that the discriminator you use to ascertain indistinguishability is not strong enough to emulate the quality or utility test."}, "questions": {"value": "In fact in most of the experiments you report the ranks are similar. There are few exceptions. In line 445 you write:\n\n\"StyleGAN2-lite delivers high Quality but almost no Utility and weak Similarity, mirroring the synthetic mode-collapse profile: crisp yet\nnarrow samples. DCGAN lands near chance in Utility while keeping Indistinguishability high, signalling shallow realism that fails to expand the task dataset.\"\n\nCan you explain what \"synthetic mode-collapse profile\", \"crisp but narrow samples\", and \"shallow realism\" mean and how they are captured by your measures? Some concrete examples (possibly on synthetic data) could go a long way towards justifying the sensibility of your definitions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rienssufpU", "forum": "CPG941VJQq", "replyto": "CPG941VJQq", "signatures": ["ICLR.cc/2026/Conference/Submission18207/Reviewer_WAWk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18207/Reviewer_WAWk"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18207/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123769483, "cdate": 1762123769483, "tmdate": 1762927950807, "mdate": 1762927950807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}