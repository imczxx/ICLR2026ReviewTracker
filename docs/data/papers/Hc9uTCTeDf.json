{"id": "Hc9uTCTeDf", "number": 16949, "cdate": 1758270505054, "mdate": 1759897208414, "content": {"title": "Cluster-Masked Scanning and Pretraining for Enhanced xLSTM Vision Performance", "abstract": "While modern recurrent architectures like xLSTM show promise for vision tasks, their potential has been hindered by the challenge of effectively applying autoregressive pretraining---a cornerstone of NLP success---to 2D image data. This paper introduces MAL, a framework that unlocks autoregressive learning for vision-oriented xLSTMs. Our core innovation is a cluster-masked pretraining strategy, which reorganizes an image into a sequence of semantically meaningful local clusters. This approach creates a more structured input sequence uniquely suited to xLSTM's memory mechanisms. By combining this with our novel cluster scanning strategy which defines an optimal processing order, MAL effectively learns powerful visual representations by predicting entire image regions autoregressively. Our experiments show that this novel pretraining scheme allows MAL to significantly outperform traditional supervised models, fully leveraging the scaling potential of xLSTM and setting a new performance benchmark.", "tldr": "MAL, a novel framework that enhances xLSTM's performance in visual representation learning through innovative cluster-masked masking and scanning strategies, significantly outperforms traditional supervised models in various visual tasks.", "keywords": ["LSTM", "Cluster-Masked", "Autoregressive Pretraining", "Visual Tasks", "Visual Pretraining"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96a77c5fe7ea620fa4382d5a34874dc5ab286af5.pdf", "supplementary_material": "/attachment/aa00eb9cd193010b081726cbc8fd4f4e503c3b2c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a cluster-masked pretraining strategy for autoregressive prediction of image regions (MAL). \nMAL reorganizes an image into a sequence of local clusters by forming groups of image patches and varying the scanning and prediction order.\nMAL is accompanied by a two stage training procedure and an encoder decoder architecture, where the encoder is an xLSTM and the decoder an attention model. In the first stage the encoder-decoder architecture is used for pretraining to build strong features in the encoder. In the second stage the encoder is finetuned on respective downstream tasks.\nOn standard image benchmarks MAL shows strong performance, outperforming DeiT, Vision-Mamba and Vision-LSTM baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Extensive experiments on different backbone architectures like Mamba and Vision Transformers with attention.\n- Strong performance of MAL compared to the baselines."}, "weaknesses": {"value": "- While the experiments demonstrate superior performance of MAL, intuitively it is not fundamentally different from standard vision transformer based patching. Both masking and scanning techniques are not informed about the content of the images, and rather convert the image into sequences by patching and then arranging these patches into a sequence. MAL seems to be a more complex strategy for creating this sequence, but intuitively it is not clear why this results in better performance nor why the sequence is “semantically more meaningful” ?\nCould the authors elaborate on the intuition why MAL shows better performance ?\n- As described in the paper, MAL is complemented by a two stage process that trains an encoder-decoder architecture in the first stage and then finetunes  the encoder with linear heads in the second stage.\nA natural question to ask is whether the cluster-masked pretraining strategy would also help direct ViT pretraining or whether the combination with this 2 stage training procedure and the encoder-decoder architecture is strictly necessary. Ablations on this would further strengthen the paper."}, "questions": {"value": "- Have you also experimented with xLSTM or Mamba blocks in the Decoder?\n- Did you ensure that the compute budget (e.g. in number of epochs) in the overall pre-training and finetuning is comparable to the budget of the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rLylhZlsNU", "forum": "Hc9uTCTeDf", "replyto": "Hc9uTCTeDf", "signatures": ["ICLR.cc/2026/Conference/Submission16949/Reviewer_Cir2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16949/Reviewer_Cir2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846217711, "cdate": 1761846217711, "tmdate": 1762926968883, "mdate": 1762926968883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a modified autoregressive neural network on image patches built on x-LSTM, to learn image representations through a self-supervised reconstruction loss. Different from original x-LSTM, it employs a two-level patch-modeling strategy. The image is first divided into large patches, and the large patches are further divided into smaller patches. The LSTM forward pass first go through all smaller patches and then go across large patches. It also uses a masking strategy to enhance the model similar to masked auto-encoder (MAE). It achieves better performance than existing methods including transformer based (DeiT) and autoregressive models (Mamba, VMamba, VRWKV) on imagenet classification, detection and instance segmentation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe overall idea is executed well. The model performance is strong among similar approaches.\n-\tThe proposed cluster-masked strategy achieves better performance than the original MAE strategy (table 4)"}, "weaknesses": {"value": "-\tThe overall novelty of the proposed method is limited. The difference from original x-LSTM is the two-level scanning order, which cannot be deemed as a major contribution. Similar idea has also been used by existing literature such as: Autoregressive Pretraining with Mamba in Vision. arXiv 2024. The training /finetuning details are very similar to those of self-supervised learning literature.\n-\tThe author has some misleading description about the formulation. The two-level patch formation is referred to as “cluster-based”, which is problematic since it does not involve any clustering algorithm. It could just be \"patches\" and \"grouped patches\"."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vOnD3bTtjB", "forum": "Hc9uTCTeDf", "replyto": "Hc9uTCTeDf", "signatures": ["ICLR.cc/2026/Conference/Submission16949/Reviewer_pWEU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16949/Reviewer_pWEU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931130072, "cdate": 1761931130072, "tmdate": 1762926968481, "mdate": 1762926968481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MAL proposes to enable effective autoregressive pretraining for vision xLSTMs by grouping adjacent patches into semantically coherent clusters, serializing these clusters via a proposed cluster-scanning order, and training xLSTM encoders to autoregressively predict cluster-level image regions; the method claims improved visual representations and downstream performance by leveraging cluster units to both reduce sequence length and better match xLSTM’s memory mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of cluster-based serialization provides a clear intuition: grouping local regions can preserve spatial relationships better than flat patch sequences.\n- The proposed framework is well-motivated and technically consistent, showing that autoregressive modeling can be made practical for xLSTM-based vision systems.\n- The empirical results are solid, with comprehensive experiments and reasonable ablations that validate the design choices."}, "weaknesses": {"value": "- The conceptual novelty of the cluster-masked strategy appears somewhat limited compared to existing masked or region-based pretraining methods.\n- The cluster scanning order lacks theoretical grounding or strong empirical justification; the claimed “optimal” ordering seems heuristic."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cn08yOT7Wy", "forum": "Hc9uTCTeDf", "replyto": "Hc9uTCTeDf", "signatures": ["ICLR.cc/2026/Conference/Submission16949/Reviewer_Q38j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16949/Reviewer_Q38j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001281834, "cdate": 1762001281834, "tmdate": 1762926968209, "mdate": 1762926968209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}