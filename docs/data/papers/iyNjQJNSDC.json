{"id": "iyNjQJNSDC", "number": 10420, "cdate": 1758170878272, "mdate": 1763091796261, "content": {"title": "Revisiting Incremental Object Detection with Pre-Trained Vision-Language  Models", "abstract": "Pre-trained Vision-Language Models (VLMs) have recently been applied to Incremental Object Detection (IOD), achieving notable progress. However, existing researches often oversimplify real-world scenarios by assuming the incremental tasks come from a single general domain. To better investigate VLMs under IOD, it is necessary to explore more generalized scenarios that encompass both novel categories and domains. To this end, we propose Cross-Domain Incremental Object Detection (CDIOD), a new benchmark that assesses the ability to continuously adapt to diverse object detection tasks across domains. CDIOD reveals that existing methods struggle to balance between adaptivity and stability under substantial domain shifts. To tackle this challenge, we propose $\\textbf{D$^3$}$, a novel framework that possesses $\\textbf{D}$ynamic grouping to promote knowledge sharing and prevent task collisions; $\\textbf{D}$ynamic adapter assignment to effectively adapt to new tasks while controlling model scale; and $\\textbf{D}$ynamic training pipeline to ensure a proper stability-adaptivity balance. D$^3$ enables VLMs to effectively handle task streams of various distribution shifts. Extensive experiments demonstrate that D$^3$ achieves state-of-the-art results across three benchmarks, highlighting its versatility and robustness in diverse incremental learning scenarios.", "tldr": "", "keywords": ["Object Detection", "Incremental Learning", "Vision Language Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/71d2bc9a9e6aa12217cc49b6b90963c02c0aacb9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Cross-Domain Incremental Object Detection (CDIOD), a new benchmark for evaluating Vision-Language Models (VLMs) in scenarios involving both novel categories and domain shifts. To address the challenge of balancing adaptivity and stability under such conditions, the authors propose D^3, a dynamic framework integrating Dynamic Task Grouping, Dynamic Adapters Assignment, and a Dynamic Training Pipeline. Experiments on CDIOD and other benchmarks show that D^3 achieves state-of-the-art performance with minimal parameter overhead, demonstrating strong adaptability and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear Architecture Illustration. The framework diagram effectively presents the overall structure and workflow of D^3, helping readers grasp the method intuitively.\n\n2. Well-Explained Method Section. Section 4 clearly describes each component of D^3, improving readability and easing understanding of the proposed approach.\n\n3. Comprehensive Ablation Studies. The ablation experiments are detailed and informative; for example, Figure 5 provides valuable insights and reference for future research."}, "weaknesses": {"value": "1. The proposed method essentially follows the routing-based architectural extension paradigm, similar to earlier approaches such as L2P and MD-DETR. The overall design philosophy, introducing modular routing for incremental adaptation, has been well established in prior works, and this paper does not introduce substantial methodological innovation beyond these existing frameworks.\n\n2. The strategy for mitigating forgetting of pre-trained knowledge is overly simplistic. The method primarily discards or deactivates previously introduced parameters, which is conceptually similar to techniques employed in most previous methods. In contrast, the design of ZiRa provides a more elegant and principled mechanism for preserving pre-trained knowledge while maintaining adaptability. The proposed solution here appears less sophisticated and lacks theoretical depth.\n\n3. Tables 1 and 2 fail to include evaluations that directly measure the retention of pre-trained knowledge, which is crucial for validating the claimed effectiveness of the approach. Moreover, in Table 3, the performance improvement over ZiRa is marginal. This limited gain suggests that the proposed design, despite its complexity and lengthy formulation, may contribute only minor practical benefits.\n\n4. Although the paper introduces a new benchmark termed CDIOD, its differences from existing continual learning setups are minor. Essentially, the benchmark extends conventional multi-dataset incremental learning by adding a few more incremental steps within a single dataset rather than truly capturing the dynamics of realistic, evolving domains. As a result, it does not meaningfully advance the field toward more practical or real-world incremental learning scenarios."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JFVqCz3BNU", "forum": "iyNjQJNSDC", "replyto": "iyNjQJNSDC", "signatures": ["ICLR.cc/2026/Conference/Submission10420/Reviewer_nf99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10420/Reviewer_nf99"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761392826829, "cdate": 1761392826829, "tmdate": 1762921729654, "mdate": 1762921729654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "2qXSiWa9WM", "forum": "iyNjQJNSDC", "replyto": "iyNjQJNSDC", "signatures": ["ICLR.cc/2026/Conference/Submission10420/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10420/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763091795003, "cdate": 1763091795003, "tmdate": 1763091795003, "mdate": 1763091795003, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on incremental object detection (IOD) using pre-trained visual language models (VLMs), particularly in more complex and realistic scenarios where tasks originate from multiple domains (cross-domain) rather than a single domain. The authors propose a new benchmark set, CDIOD, and a novel framework, D3, which combines dynamic task grouping, dynamic adapter assignment, and a dynamic training pipeline. Experimental results demonstrate that D3 outperforms state-of-the-art methods in handling domain transformation and incremental learning challenges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The discovery on performance droping when switch domains during training is interesting.\n* The key idea of the proposed D3 framework, using dynamic grouping and adapter assignment, is reasonable.\n* The experiments show the effectiveness of the proposed method."}, "weaknesses": {"value": "* The motivation and methodology are mismatched. The paper's motivation emphasizes that existing work may face significant performance degradation when switching domains due to domain bias. However, the method design does not seem to have any special design for cross-domain issues, appearing no different from existing ICD methods.\n* Following 1., the paper also lacks research on some existing cross-domain and PEFT related work, such as [1][2].\n* As can be seen from the second row of Figure 7, the baseline model has a very strong zero-shot capability (maintaining high performance even in untrained scenarios). Therefore, the paper needs to demonstrate the performance improvement of the proposed method relative to the baseline model to prove the effectiveness of the method.\n\n[1] Learning domain-aware detection head with prompt tuning. NeurIPS, 2023 \n\n[2] SEEN-DA: SEmantic ENtropy guided Domain-aware Attention for Domain Adaptive Object Detection. CVPR, 2025"}, "questions": {"value": "* The text in Figure 1-3 is too small and hard to read.\n* I'm curious why the training is strictly based on domain switching(e.g., DIOR(2phases)→ PascalVOC(2)→RUOD(1).). I believe a good measure of cross-domain performance should randomly shuffle all classes and domains.\n* Please refer to Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X7vyZEWKSE", "forum": "iyNjQJNSDC", "replyto": "iyNjQJNSDC", "signatures": ["ICLR.cc/2026/Conference/Submission10420/Reviewer_UAo2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10420/Reviewer_UAo2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791012198, "cdate": 1761791012198, "tmdate": 1762921729215, "mdate": 1762921729215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors introduce Cross-Domain Incremental Object Detection (CDIOD) to evaluate incremental adaptation of VLMs under diverse domain shifts. They proposed D^3, a dynamic architecture with task grouping, adapter assignment, and training scheduling to balance adaptivity and stability. Experiments on multiple benchmarks demonstrate significant gains over IOD and IVLOD baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Good motivation: Current IOD methods assume incremental phases stay in-distribution, which is not how models are deployed in practice. CDIOD is well-motivated and reflects the real-world situation.\n\nNovel idea: The idea of grouping related tasks and assigning group adapters to let them share capacity looks sound. The routing-at-inference also looks plausible for scaling to many domains without blowing up parameters.\n\nGood performance: The proposed method achieves strong performance with only 1.2% parameter overhead and this looks attractive.\n\nThorough experimental analysis: The authors distinguish “intra-domain forgetting”, “cross-domain stability” and “adaptivity” and quantify them. This seems better than just reporting mAP after each step."}, "weaknesses": {"value": "The method depends heavily on a VLM detector backbone. It is not guaranteed that the proposed method could be applied to other types of backbones."}, "questions": {"value": "How distribution similarity computed? It would be better to have more explanation on this.\n\nWhat happens if tasks come from many small related domains and the number of adapters may explode?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YDmabp0ltV", "forum": "iyNjQJNSDC", "replyto": "iyNjQJNSDC", "signatures": ["ICLR.cc/2026/Conference/Submission10420/Reviewer_x2uv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10420/Reviewer_x2uv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932981892, "cdate": 1761932981892, "tmdate": 1762921728428, "mdate": 1762921728428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a significant limitation in the current evaluation of Incremental Object Detection (IOD) for Vision-Language Models (VLMs). The authors argue that existing benchmarks are oversimplified, assuming incremental tasks come from a single, general domain, which does not reflect the real-world deployment of VLMs that are designed to operate across diverse domains. To address this, they propose a new benchmark, Cross-Domain Incremental Object Detection (CDIOD), which sequences tasks from three distinct domains: natural scenes (Pascal VOC), remote sensing (DIOR), and underwater (RUOD)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The introduction of CDIOD is a significant contribution. It moves beyond the oversimplified single-domain assumption and presents a more realistic and challenging evaluation protocol for VLM-based incremental learning. This benchmark is well-constructed, uses publicly available datasets from diverse domains, and is likely to become a valuable tool for the community.\n2. The paper effectively critiques the current state of IOD research and convincingly argues why a cross-domain benchmark is necessary, especially in the context of modern, general-purpose VLMs. The analysis in Section 3.2 clearly shows the shortcomings of existing methods.\n3. The method is highly parameter-efficient, adding only 1.2% trainable parameters and activating 0.4% at inference, making it very attractive for real-world applications where model size and inference cost are critical."}, "weaknesses": {"value": "1. The authors correctly identify a key limitation: DTG's performance relies on accurate task distribution estimation, which can be unreliable with very small amounts of data (e.g., few-shot settings). While this is acknowledged, it remains a potential weakness for certain application scenarios. The robustness of DTG to very noisy distribution estimates could be explored further.\n2. While comparisons to SOTA IOD methods are comprehensive, the paper could be strengthened by comparing against a broader set of general continual learning methods that also use dynamic architecture/parameter-isolation strategies, even if they are not specifically designed for object detection or VLMs. This would better situate the dynamic grouping concept within the wider field. \n3. The expansion threshold τ for DTG is a critical hyperparameter. While the value of 150 is provided, an ablation study on its sensitivity and its impact on the final number of groups and performance would have been informative."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YuvlneOQWc", "forum": "iyNjQJNSDC", "replyto": "iyNjQJNSDC", "signatures": ["ICLR.cc/2026/Conference/Submission10420/Reviewer_mdU1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10420/Reviewer_mdU1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983849035, "cdate": 1761983849035, "tmdate": 1762921728023, "mdate": 1762921728023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}