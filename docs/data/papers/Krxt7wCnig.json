{"id": "Krxt7wCnig", "number": 8571, "cdate": 1758091255206, "mdate": 1759897775889, "content": {"title": "Long-range Modeling and Processing of Multimodal Event Sequences", "abstract": "Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. \nWe employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.", "tldr": "", "keywords": ["Temporal Point Process", "Multimodal LLM"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad54b76a39963074f2061ad07d9effc17ed95004.pdf", "supplementary_material": "/attachment/4817e95816d3ccd4860a4646e397f22bfc009cd8.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a unified encoding paradigm that integrates time, event type, text, and image to achieve multimodal TPP modeling. It introduces an adaptive temporal compression mechanism, where adjacent events with time differences below a defined threshold are replaced by a special token—representing temporally similar events), enabling contextual compression and enhancing the model’s long-range sequence modeling capability."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The target task has high practical value and represents an important direction in the field of long-term reasoning and multimodal generation.\n\n2. The proposed method in this paper is concise and effective. The proposed time-similarity-based compression mechanism can significantly reduce sequence length.\n\n3. The experimental design of the paper is very comprehensive, with particularly thorough ablation studies."}, "weaknesses": {"value": "The temporal compression approach proposed in the paper may have certain limitations in some scenarios, as it only addresses the compression of the number of events but not the compression within individual events."}, "questions": {"value": "1. When events are compressed, will the compression be applied consecutively? For example, if there are three events A, B, and C, after compressing ⟨A, B⟩, will it further compress to ⟨A, B, C⟩? If so, this should be clarified in the paper.\n\n2. When a particular moment is controversial — for instance, when some comments (danmaku) are positive while others are negative — does the current method only consider the earliest event? This could indicate a potential limitation.\n\n3. I suggest adding a comparative experiment that randomly drops some events, representing the most naive form of compression, and comparing it against the proposed adaptive compression method. This would make the results more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FWfAK87BJD", "forum": "Krxt7wCnig", "replyto": "Krxt7wCnig", "signatures": ["ICLR.cc/2026/Conference/Submission8571/Reviewer_r5oN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8571/Reviewer_r5oN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660359112, "cdate": 1761660359112, "tmdate": 1762920424655, "mdate": 1762920424655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework, multimodal temporal point processes (MM-TPP), that extends Language-TPP to predict event time and type while generating rich text conditioned on visual, textual, and temporal inputs. The authors further introduce a compression mechanism to reduce sequence length. Finally, they introduce a benchmark TAXI_PRO to evaluate their proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized.  \n\n2. The topic of using LLMs for multimodal event sequences is worth exploring in the research community. This paper targets this important problem.\n\n3. The paper provides the code to help reviewers better understand of the proposed method."}, "weaknesses": {"value": "There are some concerns and questions about this paper:\n\n1.\tIn Section 4.3, the authors mention that temporal similarity between events can be calculated to reduce sequence length. However, how is this similarity calculated? The authors do not seem to mention the calculation method.\n\n2.\tIn video understanding, I learned that only 64 frames are needed to help the model understand what is happening in the video. So, for the video input mentioned in the paper, how many video frames are actually input into the model?\n\n3.\tTo reduce sequence length, did the authors try keeping the input sequence length constant while using token merging or pruning methods? How does this method perform compared to the methods mentioned in the paper?\n\n4.\tAmong the comparison methods listed in Table 1, only Language-TPP was published in 2025, while the others were published in 2022 or earlier. Could the authors update the comparison methods or include more recent methods published in 2024 or 2025 for a fairer comparison?\n\n**Please note that I am not very familiar with this research area. I would suggest that the ACs place greater weight on the comments from the other reviewers.**"}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4UWZtLTjvv", "forum": "Krxt7wCnig", "replyto": "Krxt7wCnig", "signatures": ["ICLR.cc/2026/Conference/Submission8571/Reviewer_dt4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8571/Reviewer_dt4W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816732668, "cdate": 1761816732668, "tmdate": 1762920424230, "mdate": 1762920424230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MM-TPP, a multi-modal, generative framework for temporal point processes that models multi-modal event data and predicts the next event’s time/type while generating text. To cope with very long sequences induced by multimodal tokenization, the authors introduce an adaptive temporal-similarity compression that replaces runs of events with similar inter-arrival times by a special `<|similar_event|>` token, enabling longer effective context within a fixed window. The system is built on Qwen2.5-VL-3B with a two-stage training recipe including continued pre-training and SFT. Experiments on DanmakuTPP and a newly proposed TAXI-PRO dataset show improved performance over strong TPP baselines and Language-TPP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **First multi-modal TPP dataset.** This article proposed TAXI-PRO, which may be a useful testbed for future work.\n2. **Simplicity and effectiveness.** The `<|similar_event|>` token is easy to implement and effectively increases the total event counts in a single context window."}, "weaknesses": {"value": "1. **Evaluation breadth.** Only DanmakuTPP is chosen as existing benchmarks to be evaluated, which could be biased.\n2. **Lack to efficient MLLM baselines.** The author should include token pruning baselines as baseline methods (such as [1]), as they are closely related to the addressed topic.\n\n[1] An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models (ECCV 2024)"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "hNmCYg0PjM", "forum": "Krxt7wCnig", "replyto": "Krxt7wCnig", "signatures": ["ICLR.cc/2026/Conference/Submission8571/Reviewer_X9AS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8571/Reviewer_X9AS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911177618, "cdate": 1761911177618, "tmdate": 1762920423518, "mdate": 1762920423518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends TPPs to the visual domain for modeling event sequences, avoiding traditional issues in modeling multimodal data via TPPs such as long sequence length. The authors implement this extension by introducing an adaptive sequence compression mechanism that retains key patterns, and pretrains on compressed sequences and then fine-tunes for specific tasks. The authors evaluate their method on the DanmakuTPPQA benchmark."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The extensive documentation of details of the experiment setup are appreciated.\n\n- The experiments are extensive. The authors compare against a range of TPP approaches, and select fair experiment details for an appropriate comparison. Selected metrics seem appropriate.\n\n- Standard deviations for experiment results are presented in Table 1 alongside key metrics.\n\n- The paper is well-written and very detailed. I do not have many questions after reading through the experiments section.\n\n- The ablation study included in Sec. 5.5 is very helpful for understanding details of the proposed method and how it relates to other approaches."}, "weaknesses": {"value": "- Given that only two benchmarks were selected for evaluation, it would be helpful to add a sentence or two explaining why these datasets were selected and why more are not needed for a fair evaluation.\n\n- Figure 1 is difficult to read due to the font choices and text size.\n\n- It would be interesting to consider other backbone architectures beyond Qwen-2.5 and its different sizes."}, "questions": {"value": "See weaknesses. Generally speaking, the paper is very clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sIuHTK19AT", "forum": "Krxt7wCnig", "replyto": "Krxt7wCnig", "signatures": ["ICLR.cc/2026/Conference/Submission8571/Reviewer_F8w3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8571/Reviewer_F8w3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962563347, "cdate": 1761962563347, "tmdate": 1762920423071, "mdate": 1762920423071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}