{"id": "ofJYlbzoqn", "number": 24544, "cdate": 1758357820112, "mdate": 1763553959676, "content": {"title": "Chronological Thinking in Full-Duplex Spoken Dialogue Language Models", "abstract": "Recent advances in spoken dialogue language models (SDLMs) reflect growing interest in shifting from turn-based to full-duplex systems, where the models continuously perceive user speech streams while generating responses. This simultaneous listening and speaking design enables real-time interaction and the agent can handle dynamic conversational behaviors like user barge-in. However, during the listening phase, existing systems keep the agent idle by repeatedly predicting the silence token, which departs from human behavior: we usually engage in lightweight thinking during conversation rather than remaining absent-minded. Inspired by this, we propose Chronological Thinking, a on-the-fly conversational thinking mechanism that aims to improve response quality in full-duplex SDLMs. Specifically, chronological thinking presents a paradigm shift from conventional LLM thinking approaches, such as Chain-of-Thought, purpose-built for streaming acoustic input. (1) Strictly causal: the agent reasons incrementally while listening, updating internal hypotheses only from past audio with no lookahead. (2) No additional latency: reasoning is amortized during the listening window; once the user stops speaking, the agent halts thinking and begins speaking without further delay. Experiments demonstrate the effectiveness of chronological thinking through both objective metrics and human evaluations show consistent improvements in response quality. Furthermore, chronological thinking robustly handles conversational dynamics and attains competitive performance on full‑duplex interaction metrics.", "tldr": "", "keywords": ["duplex", "speech-to-speech", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/433830ead31634e77bab535554a93ee29beb8070.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a chronological thinking mechanism, incorporated within the full-duplex pipeline of spoken dialogue systems. The core motivation is to prevent the agent idle when listening to user's utterances, which is not aligned with cognitive human interactions. There are five nodes consisting of the chronological thinking, and they are implemented on top of SALM-duplex model. Experiments are based on multi-turn benchmarks (SpokenWOZ, MtBenchEval) and factual QA benchmarks (Llama Questions, Web Questions)."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The motivation is strong. Agent's idle state generation is a critical issue regarding performance and efficiency in duplex systems. This concurrent speech/reasoning processing is aligns with recent trends in this field.\n\n* The overall writing flow is easy to read."}, "weaknesses": {"value": "1. **Evaluation metric**: \n\n    1. It is widely acknowledged that the evaluation performance of LLMs is highly sensitive to the choice of input prompt. However, the manuscript does not provide sufficient details regarding the prompts or the specific evaluation features used in experiments.\n\n    1. For multi-turn dialogues, controlling conversation flow is inherently challenging. The manuscript does not clarify whether the evaluation is conducted at the turn level or at the dialogue level. For example, is the model provided with the entire dialogue history and evaluated solely on its final response?\n\n    1. Using syntactic or semantic similarity metrics such as BLEU and Sentence-BERT is not well-suited for task-oriented dialogue systems, as these metrics do not assess task completion or goal success. Metrics that measure task success rates or completion performance would be more appropriate for this evaluation scenario.\n\n1. **Analysis on the proposed method**: Although the authors emphasize the motivation rooted in the ACT-R cognitive framework, the manuscript lacks further analysis or ablation studies to support the claim. For instance, the five node types described in Table 1 rely entirely on LLM-generated content, yet no analysis is provided to verify whether the generated content aligns with the authors' intended definitions or how each node type contributes to performance across different conditions.\n\n1. **Actual performance benefit of the proposed thinking mechanism**: The performance improvements between the variants with and without the proposed thinking mechanism are marginal. In particular, in Table 4, the claim that chronological thinking outperforms SALM-Duplex appears overstated. The reported gains may instead stem from (1) predicting audio tokens only and (2) the addition of a Transformer decoder module, rather than from the thinking mechanism itself.\n\n1. **# of parameters in Table 1**: Although the proposed architecture introduces an additional Transformer decoder relative to SALM-Duplex, Table 1 does not clearly report the total number of parameters. The manuscript should provide explicit details regarding the parameter count and the architectural additions.\n\n1. **Concerns on the figure**: Figure 2 appears visually similar to figures used in the SALM-Duplex paper, which may raise concerns about originality. It is recommended that the figure highlight this paper's distinctive contribution - specifically, the chronological thinking mechanism.\n\n1. **Subjective results**: Information on evaluator recruitment, sample size, and evaluation guidelines is insufficient. These details are necessary to ensure reproducibility and to support the credibility of the subjective evaluation results.\n\n1. **Some editorial suggeestions**:\n    1. Terms \"SpokenWOZ\" after the line 312 need to be described as \"SpokenWOZ-G\" to prevent confusion?\n    1. MtBenchEval in line 356 should be properly cited.\n    1. Typo: \"Impatiet\" in Table 5."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0sOkBxrbXF", "forum": "ofJYlbzoqn", "replyto": "ofJYlbzoqn", "signatures": ["ICLR.cc/2026/Conference/Submission24544/Reviewer_hQuT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24544/Reviewer_hQuT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949277851, "cdate": 1761949277851, "tmdate": 1762943118037, "mdate": 1762943118037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to all reviewers"}, "comment": {"value": "We sincerely appreciate the reviewers for their dedicated efforts. We upload a revised version of this paper that incorporates several updates: 1. an explanation regarding response latency settings; 2. enhanced emphasis in Figure 2 on the improvements of our method compared to existing approaches; 3. a supplementary explanation of the subjective experiment setup; 4. error corrections and writing clarifications. We address specific concerns in detail within our individual responses to each reviewer."}}, "id": "iZWATqnGlr", "forum": "ofJYlbzoqn", "replyto": "ofJYlbzoqn", "signatures": ["ICLR.cc/2026/Conference/Submission24544/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24544/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24544/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763551069154, "cdate": 1763551069154, "tmdate": 1763551069154, "mdate": 1763551069154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript explores the integration of deliberative reasoning capabilities into end-to-end spoken dialogue models, representing an interesting and nascent direction in the audio domain. The core technique involves a \"Think-While-Listen\" paradigm: specific output tokens emitted by the Language Model during the user's speaking time are substituted with corresponding textual reasoning tokens. This concurrent approach is claimed to introduce zero latency for the reasoning process. The experimental section includes basic conversational QA tests and latency measurements, reporting decent performance gains over weak, foundational baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work is one of the first attempts to systematically explore and implement a deliberative (or \"thinking\") mode within the architecture of end-to-end spoken dialogue models.\n\n2. The proposed method, while structurally simple, is conceptually sound. Leveraging the user's silence or speaking time for concurrent processing is a valid and pragmatic approach to integrating complex reasoning without incurring additional latency."}, "weaknesses": {"value": "1. The exploration of the reasoning mechanism is overly simplistic, relying merely on substituting special LLM output tokens with scratchpad text. Further architectural and procedural investigation is warranted. Potential avenues for future exploration include: Expanding beyond Think-While-Listen to Think-While-Speak (concurrent reasoning during both input and output phases). Introducing a dedicated reasoning output head, separate from the main dialogue response head, purely for inference and planning. Investigating whether fine-tuning the reasoning component using Reinforcement Learning (RL) post-pre-training could enhance the quality of the deliberation. Analyzing the reasoning capability's sensitivity to diverse training data distributions.\n2. Many conversational scenarios do not necessitate complex reasoning. The training data generation phase should be critically optimized to focus on high-level logical inference dialogues (e.g., those found in benchmarks like BigBenchAudio in GPT-realtime). Furthermore, the examples provided in Appendix A1 appear heavily biased toward tool-use/Agent functionality (which often involves a search path). It is crucial to include more generalized conversational scenarios and provide a clearer manifestation of the ACT-R theory (or similar cognitive architectures) in these common dialogue contexts.\n3. The current experimental results are limited by the selection of relatively weak baselines (e.g., GLM-4-Voice). The evaluation must be expanded to include comparisons against state-of-the-art competitive models such as Qwen3-Omni and Kimi-Audio to properly benchmark the proposed technique's efficacy.\n4. The thinking paradigm is not inherently constrained to the spoken dialogue model architecture. Validation should be extended to a wider array of models and tasks, such as non-interruptive Spoken QA, and training should ideally be conducted on larger language models to confirm scalability.\n5. The ablation study (Table 4) shows relatively marginal gains, suggesting that the current data and scenario design may not sufficiently highlight the benefits of the reasoning process. Further optimization of the data and evaluation scenarios is highly recommended, potentially utilizing broader benchmarks like VoiceBench.\n6. The authors should include a more thorough discussion and comparison with concurrent related works [1] and [2].\n\n[1]. Can SpeechLLMs Think while Listening?\n\n[2]. STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yFucHfY1t0", "forum": "ofJYlbzoqn", "replyto": "ofJYlbzoqn", "signatures": ["ICLR.cc/2026/Conference/Submission24544/Reviewer_gefN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24544/Reviewer_gefN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990299118, "cdate": 1761990299118, "tmdate": 1762943117838, "mdate": 1762943117838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Chronological Thinking, a strictly causal, on-the-fly reasoning mechanism for full-duplex Spoken Dialogue Language Models (SDLMs) that allows them to \"think while listening\" instead of outputting repeated silence tokens. The system replaces silence tokens during the listening phase with structured reasoning nodes inspired by ACT-R cognitive architecture, enabling better real-time conversational reasoning without added latency. The contributions include (a) the CT-SDLM architecture integrating chronological thinking, (b) a synthetic multi-speaker dialogue dataset generation pipeline, (c) objective and subjective evaluations showing improved reasoning quality, and (d) comparisons against LLM and SDLM baselines such as SALM-Duplex and GT-LM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a novel and practical causal reasoning mechanism tailored for full-duplex spoken dialogue, addressing a well-defined gap in how agents utilize the listening phase.  \n2. Demonstrates improvements in reasoning-heavy dialogue tasks without cost to latency or conversational dynamics.  \n3. Evaluation includes comparisons to baselines, across multiple metrics (GPT score, BLEU, Sentence-BERT, factual QA accuracy, turn-taking/barge-in), offering a thorough empirical picture."}, "weaknesses": {"value": "1. Heavy reliance on synthetic datasets may limit conclusions about real-world conversational robustness and generalization.  \n2 The proposed improvement in factual QA performance is negligible, indicating the method’s gains may be task-specific.  \n3. Lack of ablation isolating the benefit of ACT-R-inspired node types versus more naive streaming reasoning approaches beyond brief mentions.  \n4. Architecture depends on a specific multi-component pipeline (speech encoder, LLM backbone, transformer decoder) which may reduce reproducibility or applicability for researchers without comparable resources."}, "questions": {"value": "- How does the chronological thinking mechanism perform on purely real, noisy speech data and spontaneous interruptions, beyond synthetic datasets?  \n- Could you provide detailed ablation results comparing ACT-R node structuring versus unstructured incremental reasoning to confirm the design choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6wlQCMGAax", "forum": "ofJYlbzoqn", "replyto": "ofJYlbzoqn", "signatures": ["ICLR.cc/2026/Conference/Submission24544/Reviewer_12Gj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24544/Reviewer_12Gj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998240386, "cdate": 1761998240386, "tmdate": 1762943117647, "mdate": 1762943117647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Chronological Thinking (CT) for full-duplex spoken dialogue LLMs: while the user is speaking, the model fills the silence/idle slots with a causal, interruptible 5-node \"thinking\" chain (Entity / Intent / Action / Knowledge / Logic), so that when speech input ends, the model can answer with little extra latency. This directly targets the \"inner monologue but unused\" issue in Moshi and the \"asynchronous thinking\" idea in SALMONN-omni. On synthetic duplex dialogue sets, CT improves text/speech quality over its own duplex baseline, while keeping turn-taking roughly in the same range as prior work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear target & mechanism. Reusing silence tokens for structured, causal planning is simple and fits current full-duplex pipelines. \n\n- Aligned with 2025 trend. Very close to SCoT, \"Can Speech LLMs Think while Listening?\", and SHANKS, all of which seek reasoning-while-listening in streaming setups. CT is a reasonable variant in this space. \n\n- Empirical uplift on its own data. Within their synthetic setup, \"with CT\" beats \"no CT\", so the idea is at least self-consistent."}, "weaknesses": {"value": "- Novelty over Moshi/SALMONN-omni/SCoT is modest. All of these already run an inner or asynchronous text stream during duplex; CT mostly adds a fixed 5-slot structure and a replacement rule. The paper should say why this is better. \n\n- Comparisons miss the closest 2025 baselines (SCoT, SHANKS, \"Can Speech LLMs Think while Listening?\"), so it’s hard to attribute the gains to CT itself. \n\n- Latency claim is fragile. Many duplex systems are at ~200–400 ms E2E; any 200–300 ms overhead from longer thinking streams is user-visible, but the paper does not give distributional or hardware-normalized numbers. (Contrast Moshi’s 160–200 ms.) \n\n- Data realism. Most evidence is on TTS/synthetic duplex; no noisy/overlapped human speech to show the CT trigger is robust. This is exactly where recent SALMONN-omni reports strength."}, "questions": {"value": "- Can you run CT on one public recipe (Moshi streaming eval, SALMONN-omni duplex tasks, or SCoT’s streaming CoT benchmark) to show cross-setup gains? \n\n- Do we really need all 5 node types? Please give ablations (2-3 nodes, or a single \"thinking token\").\n\n- How is CT triggered online (VAD, ASR partial, fixed frame)? What happens if the user talks unusually fast/slow?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RbtzEkJ0SL", "forum": "ofJYlbzoqn", "replyto": "ofJYlbzoqn", "signatures": ["ICLR.cc/2026/Conference/Submission24544/Reviewer_pEmW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24544/Reviewer_pEmW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24544/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066161858, "cdate": 1762066161858, "tmdate": 1762943117415, "mdate": 1762943117415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}