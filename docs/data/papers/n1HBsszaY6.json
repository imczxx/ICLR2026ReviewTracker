{"id": "n1HBsszaY6", "number": 6401, "cdate": 1757980280385, "mdate": 1763672982235, "content": {"title": "SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning", "abstract": "Federated Prompt Learning has emerged as a communication-efficient and privacy-preserving paradigm for adapting large vision-language models like CLIP across decentralized clients. However, the security implications of this setup remain underexplored. In this work, we present the first study of backdoor attacks in Federated Prompt Learning. We show that when malicious clients inject visually imperceptible, learnable noise triggers into input images, the global prompt learner becomes vulnerable to targeted misclassification while still maintaining high accuracy on clean inputs. Motivated by this vulnerability, we propose SABRE-FL, a lightweight, modular defense that filters poisoned prompt updates using an embedding-space anomaly detector trained offline on out-of-distribution data. SABRE-FL requires no access to raw client data or labels and generalizes across diverse datasets. We show, both theoretically and empirically, that malicious clients can be reliably identified and filtered using an embedding-based detector. Across five diverse datasets and four baseline defenses, SABRE-FL outperforms all baselines by significantly reducing backdoor accuracy while preserving clean accuracy, demonstrating strong empirical performance and underscoring the need for robust prompt learning in future federated systems.", "tldr": "", "keywords": ["Federated Learning", "Poisoning Attacks", "Multimodal Learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82fe07b2ef48b31a585e60d65bf22e1888d642ca.pdf", "supplementary_material": "/attachment/e98e493940a835c3b4886c5ed7a411708e8225ba.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the security of Federated Prompt Learning (FPL) and presents SABRE-FL, a defense framework that detects backdoored client updates using embedding-space anomaly detection. The authors first demonstrate that prompt-based federated systems, despite having a smaller attack surface than full-model FL, are highly vulnerable to imperceptible noise-trigger attacks. SABRE-FL then leverages CLIP embedding deviations to filter malicious updates without accessing raw client data or labels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and timely problem. The problem of backdoor attacks in federated prompt learning is novel and interesting. The paper fills an important gap in understanding the security risks of adapting foundation models via prompt learning in a decentralized setting.\n2. Clear intuition and methodology. The core idea is intuitive and well-motivated. The defense operates in embedding space, aligning with the privacy constraints of FL.\n3. Strong empirical evaluation. Extensive experiments across five datasets and four defense baselines. SABRE-FL achieves the lowest backdoor accuracy while maintaining high clean accuracy."}, "weaknesses": {"value": "1. Limited threat model. Only data poisoning is considered. Model poisoning or adaptive strategies are not analyzed. It is unclear whether an attacker aware of SABRE-FL could evade embedding-space detection. The assumption that the attacker controls 25% of all clients is generally considered to be high compared with existing literatures.\n2. Detector training assumptions. The detector is trained using poisoned embeddings generated on an auxiliary dataset. It is not obvious how a real-world server would obtain such poisoned examples to train the detector. The paper should justify the practicality of this pre-training phase.\n3. More explanation on comparison to FLAME and other baselines. FLAME also uses embedding-based filtering. The paper needs a clearer conceptual distinction and justification of why SABRE-FL is fundamentally stronger."}, "questions": {"value": "1. Could authors provide the effectiveness of the method against adaptive attackers?\n2. Could authors justify the practicality of the pretraining phase?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0CLDWq4Ruo", "forum": "n1HBsszaY6", "replyto": "n1HBsszaY6", "signatures": ["ICLR.cc/2026/Conference/Submission6401/Reviewer_r8mg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6401/Reviewer_r8mg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761582127320, "cdate": 1761582127320, "tmdate": 1762918805458, "mdate": 1762918805458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his paper proposes a method named SABRE-FL to defend Federated Prompt Learning (FPL) against backdoor attacks. It takes the form of a server-side defense mechanism that uses a lightweight detector to filter malicious client updates. To achieve this, they employ an anomaly detector that operates in the CLIP embedding space, which is trained offline on out-of-distribution (OOD) data. The paper outlines the vulnerability of FPL to malicious clients injecting learnable noise triggers and proposes a solution to mitigate these issues. Their main contribution is SABRE-FL, a lightweight and modular defense framework, which leverages the consistent deviation produced by poisoned samples in the embedding space to identify and filter poisoned prompt updates. They conducted an empirical study of the attack across five diverse datasets, concluding that FPL is highly vulnerable to backdoor attacks while still maintaining high clean data accuracy. Finally, an empirical study of the defense was conducted, concluding that SABRE-FL demonstrates superior performance compared to four other baseline defenses, as it can reduce backdoor accuracy while preserving clean accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This work presents the first systematic study of backdoor attack vulnerabilities within the Federated Prompt Learning (FPL) paradigm. This exploratory contribution is significant as it illuminates a critical and previously unexamined attack dimension.\n2.The paper proposes SABRE-FL, a novel server-side defense mechanism. The core of this mechanism involves using a lightweight MLP, trained offline on an out-of-distribution (OOD) dataset, to detect embedding-space anomalies.\n3.A key advantage of this defense is its generalizability. The experiments demonstrate that the detector, trained on a single auxiliary dataset, can effectively generalize and be applied across five other distinct task datasets."}, "weaknesses": {"value": "1.SABRE-FL is essentially an anomaly detector. In heterogeneous (Non-IID) FL scenarios, natural shifts in data distribution are an inherent characteristic. The paper provides no evidence that the detector D can distinguish between malicious offsets caused by the attack and benign shifts arising from this data heterogeneity. This casts serious doubt on the method's effectiveness in realistic FL settings.\n2.The defense mechanism relies on an assumption that is difficult to satisfy in practice: the server must know the exact number of malicious clients m,a prior in each round to filter out the clients with the top-m highest scores. This assumption is unrealistic for most real-world scenarios.\n3.SABRE-FL (according to Algorithm 1) requires clients to upload the embeddings for all their local data in each round. This is likely to incur substantial communication overhead.\n4.The paper only provides a small-scale experiment (32 clients) in Appendix E.3 and lacks an evaluation on larger-scale federated networks."}, "questions": {"value": "1.If m is unknown or mis-specified (e.g., if the server underestimates the number of attackers), how does the defense performance of SABRE-FL (both CA and BA) degrade?\n2.I would like to see some discussion regarding the communication overhead introduced by SABRE-FL.\n3.Could the authors provide more insight into the results on the FGVC Aircraft dataset (Figure 8d)? In the 'no defense' setting, this dataset exhibits two extremes: the lowest Clean Accuracy (CA) and the highest Backdoor Accuracy (BA), and these results differ significantly from those of the other datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f0V6qiIBwM", "forum": "n1HBsszaY6", "replyto": "n1HBsszaY6", "signatures": ["ICLR.cc/2026/Conference/Submission6401/Reviewer_4QUt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6401/Reviewer_4QUt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829046382, "cdate": 1761829046382, "tmdate": 1762918804825, "mdate": 1762918804825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the security vulnerabilities of Federated Prompt Learning (FPL) under backdoor attacks. The authors show that FPL models can be compromised when malicious clients inject visually imperceptible, learnable noise triggers into images, leading to targeted misclassification while maintaining high clean accuracy. To counter this, they propose SABRE-FL, a modular and lightweight server-side defense that employs an embedding-space anomaly detector—trained offline on out-of-distribution (OOD) data—to identify and filter poisoned prompt updates without accessing raw client data or labels. Experiments demonstrate that SABRE-FL significantly reduces backdoor accuracy while preserving clean performance across various datasets and FPL settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Pioneers the study of backdoor threats in the emerging FPL paradigm.\n- Introduces a well-motivated and FPL-specific backdoor mechanism based on learnable, imperceptible noise triggers.\n- Clear writing and strong organization, aided by effective visual explanations of both attack and defense designs."}, "weaknesses": {"value": "1. The paper claims the noise triggers are *visually imperceptible*, but lacks direct image comparisons. Including visual examples (original vs. triggered) or a qualitative study would strengthen this claim.\n2. SABRE-FL removes the top-*m* suspicious clients, assuming *m* is known. An analysis of sensitivity to inaccurate estimates of *m* would clarify robustness in real-world settings.\n3. The distinction between the proposed attack and a federated adaptation of BadCLIP should be elaborated—what novel properties or mechanisms are introduced?\n4. The effect of data heterogeneity (Non-IID settings) on SABRE-FL’s detection performance is not well-studied; benign diversity may confound the embedding-based detector.\n5. SABRE-FL’s effectiveness depends on the defender’s ability to model known trigger behaviors when training its detector offline. However, this reliance makes it vulnerable to novel or unconventional backdoor types—such as semantic, geometric, or other model-poisoning attacks—that fall outside the learned embedding distribution.\n6. Additional comparisons to recent strong baselines such as **Deepsight** [1] or **BackdoorIndicator** [2] would better contextualize SABRE-FL’s improvements.\n\n[1]. Rieger, Phillip, et al. \"Deepsight: Mitigating backdoor attacks in federated learning through deep model inspection.\" arXiv preprint arXiv:2201.00763 (2022).\n\n[2]. Li, Songze, and Yanbo Dai. \"{BackdoorIndicator}: Leveraging {OOD} Data for Proactive Backdoor Detection in Federated Learning.\" 33rd USENIX Security Symposium (USENIX Security 24). 2024."}, "questions": {"value": "Please address the aforementioned weaknesses, particularly by including qualitative visualizations, sensitivity analyses for m, comparisons to stronger defenses, and discussions of detector robustness under Non-IID and adaptive attack scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HqCMym9s9B", "forum": "n1HBsszaY6", "replyto": "n1HBsszaY6", "signatures": ["ICLR.cc/2026/Conference/Submission6401/Reviewer_jAGJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6401/Reviewer_jAGJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964951904, "cdate": 1761964951904, "tmdate": 1762918804362, "mdate": 1762918804362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies backdoor attacks in Federated Prompt Learning (FPL) and introduces SABRE-FL, a defense framework that detects and filters poisoned client updates using representation-space anomaly detection. The key idea is to train a binary detector on CLIP embeddings from an auxiliary dataset which includes clean and triggered/poisoned samples, so the model can identify abnormal embedding patterns corresponding to malicious clients' updates during FL aggregation. \n\nThe method is evaluated on five datasets, which are Flowers, Pets, DTD, FGVC Aircraft, and Food101 under varying malicious client ratios, showing that SABRE-FL achieves low backdoor success rates while preserving clean accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper studies backdoor attacks in federated prompt learning (FPL), where only prompt parameters, not full model weights, are shared; this is timely and relevant as CLIP-style adaptations in FL are increasingly used.\n2. The method shows potential generalizability: a detector trained on Caltech-101 transfers to datasets not seen during training.\n3. The paper provides clear motivation and visualization that support the defense’s intuition; the results look promising in tackling the backdoor attack tested."}, "weaknesses": {"value": "1. The methodology section lacks important details.\n(i) The paper describes the trigger as a learnable noise pattern but does not explain how it is optimized, what loss function or parameters are used, and how it interacts with the local prompt updates (e.g., whether it uses SGD, PGD, or another generator).\n(ii) The defense critically depends on the parameter $m$—the number of clients excluded from aggregation each round—but there is no principled method or empirical guideline for setting this value.\n(iii) The detector is trained on an auxiliary dataset (Caltech-101) with synthetically poisoned samples, yet the paper provides little justification that this dataset captures the diversity of real backdoor triggers or resembles the adversary’s trigger design. The assumption that embedding deviations generalize across datasets and trigger types is unverified.\n\n2. Several important ablation studies are missing, such as varying $m$, testing under different non-IID settings, changing the auxiliary dataset, and exploring the effects of trigger strength, magnitude, and optimization steps.\n\n3. The paper does not visualize or analyze the learned trigger patterns that drive the backdoor. Showing how the trigger alters image embeddings or prediction confidence would make the mechanism more transparent.\n\n4. The evaluation focuses on a single type of learnable additive-noise trigger inspired by BadCLIP. It does not test against adaptive or structurally different triggers (e.g., patch-based, frequency-domain, sample-specific, or model-poisoning attacks). As a result, the claimed generality of SABRE-FL across attack mechanisms is not sufficiently demonstrated."}, "questions": {"value": "Please revise the methodology section based on the weaknesses mentioned above. Also, adding some more important experiments should strengthen the contribution of this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9R78BQHQw1", "forum": "n1HBsszaY6", "replyto": "n1HBsszaY6", "signatures": ["ICLR.cc/2026/Conference/Submission6401/Reviewer_D5Ed"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6401/Reviewer_D5Ed"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052255537, "cdate": 1762052255537, "tmdate": 1762918804008, "mdate": 1762918804008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Rebuttal Summary by Authors"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful feedback and constructive comments. We are encouraged by the recognition of the paper’s novelty and relevance, and we have addressed each reviewer’s concerns in detail. Several reviewers highlighted important aspects regarding evaluation breadth, practical deployment, and robustness to adaptive settings. We have carefully considered these suggestions and conducted a number of new experiments to clarify our contributions and limitations.\n\nBelow, we summarize the key concerns raised and our responses:\n\n* **Non-IID Data and Heterogeneity Effects:** A recurring concern was SABRE-FL’s robustness under federated data heterogeneity. We now include a comprehensive Dirichlet-based study (Appendix F, Table 4), varying $\\alpha$ from 0.9 (mild) to 0.1 (severe). Our results show SABRE-FL remains effective across most settings, with minor performance drops only at extreme heterogeneity levels.\n\n* **Sensitivity to m (Number of Malicious Clients):** As raised by multiple reviewers, we evaluated SABRE-FL under both underestimation (failing to remove attackers) and overestimation (removing benign clients). The defense is fairly robust under both scenarios (Appendix F, Tables 7 & 8).\n\n* **Qualitative Visualization:** We added visual examples of clean, backdoored, and triggered images across all datasets (Appendix F, Figure 9), showing the imperceptibility of triggers and supporting the paper’s claims.\n\n* **Trigger Strength and Optimization:** We conducted ablations varying both the trigger magnitude ($\\epsilon$) and the optimization duration. These show a clear trend: stronger triggers result in higher attack success, but SABRE-FL consistently mitigates the backdoor (Appendix F, Tables 5 & 6).\n\n* **Generalization Across Datasets:** To verify that SABRE-FL’s embedding-space detector does not overfit to a particular auxiliary dataset, we trained the classifier on a different OOD dataset (Flowers-102 instead of Caltech-101) and re-ran our main experiments. The detector generalized well across five target datasets, reaffirming that it captures dataset-agnostic trigger signatures (Appendix F, Table 9).\n\n* **Practicality of the Detector Pretraining Phase:** Multiple reviewers questioned how the server obtains poisoned examples for training the anomaly detector. In response, we clarified that poisoned embeddings can be synthetically generated offline using public OOD data and a generic trigger. This setting mirrors realistic FL defenses where the server simulates threat scenarios during calibration.\n\n* **Comparison with FLAME and Related Work:** We clarified conceptual differences with FLAME and related defenses. SABRE-FL leverages a supervised embedding-space detector, offering precise client-level filtering, while being tailored to prompt-based FL systems.\n\n* **Adaptive Attackers:** While full adaptive attack analysis is left to future work, we note that the robustness under misestimated $m$ indirectly simulates such scenarios, where a smart attacker evades detection. Our results suggest SABRE-FL still limits backdoor success in such cases.\n\nWe believe the additional results and clarifications significantly strengthen the submission. All new results are included in Appendix F. We will integrate key insights and references in the revised paper and will be grateful for the opportunity to update the paper for acceptance at ICLR'26. Thank you again for your detailed reviews."}}, "id": "dpYgFzeyoX", "forum": "n1HBsszaY6", "replyto": "n1HBsszaY6", "signatures": ["ICLR.cc/2026/Conference/Submission6401/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6401/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission6401/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763675106747, "cdate": 1763675106747, "tmdate": 1763675106747, "mdate": 1763675106747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}