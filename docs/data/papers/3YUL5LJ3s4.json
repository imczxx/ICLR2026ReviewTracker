{"id": "3YUL5LJ3s4", "number": 5051, "cdate": 1757839290197, "mdate": 1763011318562, "content": {"title": "DC-LLM: HARDWARE-FRIENDLY LLM WEIGHT COMPRESSION VIA DYNAMIC LINEAR COMBINATION", "abstract": "The progressive scaling of large language models (LLMs) has consistently en-\nhanced multimodal understanding and advanced reasoning capabilities, but has\nsubstantially increased computational and hardware execution overhead. In this\npaper, we present DC-LLM, a novel post-method that compresses only model\nweights. We partition each weight tensor into fixed-size blocks and assign a sin-\ngle seed to each block. The seed drives a hardware-friendly Linear Feedback\nShift Register (LFSR) generator that dynamically produces multiple basis ma-\ntrices. Each block is then reconstructed as a linear combination of these basis\nmatrices, with block-specific coefficients, which substantially reduces the amount\nof stored data, increases the data-transfer efficiency between memory and com-\npute units, and consequently speeds up memory-bound inference for large lan-\nguage models. Experimental results on different LLM models ranging\nfrom 7B–70B parameters show that DC-LLM attains state-of-the-art performance\nwhen weights are compressed to approximately 3-bit or 4-bit. We also design a\ndedicated ASIC accelerator that achieves a 4× speed-up for memory-bound LLM\ninference.", "tldr": "", "keywords": ["LLM; Compression"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7b82e765cedf59380468adcc57ff07f9db11c949.pdf", "supplementary_material": "/attachment/c0ebb5d52be9c912ec5671fb086020d87af8003a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a weight-only compression method for large language models, dynamically constructing each weight block from a seed generated using Linear Feedback Shift Register (LFSR), which is then combined with other generated weight blocks as an approximation of the original model weight. This method increases data-transfer efficiency between memory and compute units by reducing memory footprint. Experiment shows that this method can effectively boost performance on different LLMs of different sizes. Moreover, the paper proposes a dedicated hardware accelerator to speed up the inference process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a novel compression method, DC-LLM,  that utilizes LFSR to efficiently generate dense seeds, and develops a calibration-free method to reconstruct weight blocks from these seeds.\n\n2. The paper formulates the process of finding the optimal parameters as a design space exploration (DSE) problem to find parameters that achieve pareto balance.\n\n3.  The paper proposes a seed-compression-aware accelerator design to accelerate inference at hardware level."}, "weaknesses": {"value": "1. Time and memory complexity lacks theoretical analysis. For the reconstruction process, the paper doesn’t provide theoretical analysis that explore the memory and time complexity of the reconstruction process during model inference, making the performance boost less convincing. For the parameter search and weight compression process, the paper also lacks in theoretical analysis, leaving readers to wonder the resource consumption to compress models from scrach.\n\n2. Models used for experiments lack of variety. All experiments conducted in this paper is based on the models of the Llama family, which raises doubts about the effectiveness of the proposed method on other mainstream model families, such as Mistral and Qwen.\n\n3. Experiment setting is vague. The experiment section doesn’t provide readers with the initial settings of the search parameters $T=<B, S, G, R_{th}>$, and the result in table 1 only provides the equivalent quantization level (i.e. 3.8 bit and 2.7 bit). It would be beneficial to provide the initial parameters, as well as the search results of the formulated DSE problem, since it’s now unclear how many set of parameters that reach the pareto balance can be used for different compression rate, which hinders the practicability of the proposed method."}, "questions": {"value": "1. How are the initial hyperparameters $T=<B, S, G, R_{th}>$ defined? Will different initial settings affect the time comsumption of the search process? If so, how do we leverage this problem?\n2. Is it possible to implement the hardware design on actual hardwares instead of simulation? If not applicable, what are the obstacles? \n3. Can users customize the compression rate easily? In Table 1, the quantization bits of the proposed method  are 3.8 bit and 2.7 bit. Is this value customizable? If so, to what extent of flexibility and granularity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P7sN0uX1E2", "forum": "3YUL5LJ3s4", "replyto": "3YUL5LJ3s4", "signatures": ["ICLR.cc/2026/Conference/Submission5051/Reviewer_jMYx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5051/Reviewer_jMYx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914696420, "cdate": 1761914696420, "tmdate": 1762917844331, "mdate": 1762917844331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "aKL9d15piY", "forum": "3YUL5LJ3s4", "replyto": "3YUL5LJ3s4", "signatures": ["ICLR.cc/2026/Conference/Submission5051/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5051/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763011317738, "cdate": 1763011317738, "tmdate": 1763011317738, "mdate": 1763011317738, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DC-LLM, a new post-training, weight-only compression method for LLMs. The primary goal is to address the computational and hardware overhead of LLMs , which is mainly caused by the high memory bandwidth required to load model weights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Hardware-friendly:\n The proposed algorithm is designed to be efficient on hardware. The choice of a Linear Feedback Shift Register (LFSR) is key, as it's extremely cheap in hardware (just shift and XOR operations)\n\n2. Calibration-Free: The method is \"post-training\" and does not require any calibration data.\n\n3. Robust Hyperparameter Tuning: The authors treat the compression setup (block size, seed length, etc.) as a formal multi-objective optimization problem. Using Bayesian Optimization to find the best configuration is a standard/robust way to navigate the complex trade-offs between accuracy and storage"}, "weaknesses": {"value": "1. Negligible performance gain compared to SeeLM (most important baseline): In performance analysis, authors compared their method with other baselines to show that DC-LLM achieves better performance compared to other baselines, but the performance gain compared to SeedLM which is the most direct related work is negligible. \n\n2. Ablation Study: The paper misses a detailed ablation study, most importantly, authors do not discuss the post-training time complexity and efficiency, given the heavy tuning and calculations, I believe this could be very huge bottleneck/limitation.\n\n3. Custom Hardware is Required for Speedup: most significant barrier to adoption. The paper explicitly states that \"conventional GPUs... cannot efficiently reconstruct weights from compact seeds\". The 4x speedup is only demonstrated on their custom-designed ASIC accelerator which limits the scalability of the proposed method."}, "questions": {"value": "1. How much improvement can we see in term of Latency compared to other baselines (the ones that you've compared DC-LLM with for performance)?\n2. How much time does it take to run the compression? \n3. What are the actual numbers for latency if you implement this method and simulate it on CPU instead of GPU?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VoRFWnir9d", "forum": "3YUL5LJ3s4", "replyto": "3YUL5LJ3s4", "signatures": ["ICLR.cc/2026/Conference/Submission5051/Reviewer_uyW7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5051/Reviewer_uyW7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963705581, "cdate": 1761963705581, "tmdate": 1762917843948, "mdate": 1762917843948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DC-LLM, a post-training, weight-only compression approach for LLMs. The method reconstructs model weights from compact seeds using an LFSR-based generator that produces pseudo-random basis matrices. Each weight block is expressed as a linear combination of these generated bases, with coefficients chosen adaptively to satisfy an explained-energy threshold. The goal is to reduce storage and memory bandwidth demands while preserving model accuracy. The authors also present a SystemVerilog ASIC design demonstrating up to 4× inference speedup and strong results on LLaMA models compressed to 3–4 bits.\nHowever, the paper’s central idea and much of its formulation are highly similar to SeedLM."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The method addresses a key bottleneck in LLM inference.\n- Adaptive basis selection and Bayesian optimization are well-motivated."}, "weaknesses": {"value": "- There is significant conceptual and textual overlap with SeedLM, to the point where many sections appear directly paraphrased from it.\n- The novelty claim is weak: DC-LLM’s core idea, representing weight blocks via pseudo-random generator seeds and coefficients, is essentially the same as SeedLM, with only minor algorithmic variations.\n- The writing and structure mirror SeedLM closely, suggesting inadequate originality in presentation and contribution framing.\n- Empirical results also do not clearly demonstrate substantial improvement over SeedLM to justify a new publication."}, "questions": {"value": "A future version should reflect the authors’ own understanding and narrative framing of the problem, even if the core idea remains related."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "There appears to be substantial textual and structural overlap with the prior work SeedLM. Many sections appear paraphrased or minimally altered, raising potential derivative-submission concerns."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AvRETFIahW", "forum": "3YUL5LJ3s4", "replyto": "3YUL5LJ3s4", "signatures": ["ICLR.cc/2026/Conference/Submission5051/Reviewer_U3q9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5051/Reviewer_U3q9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241998297, "cdate": 1762241998297, "tmdate": 1762917843743, "mdate": 1762917843743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DC-LLM, a hardware-friendly weight-only compression technique for large language models. The core idea is to use a Linear Feedback Shift Register to generate pseudo-random basis tensors from a seed, and reconstruct each weight block as a linear combination of these bases. \nExperiments on LLaMA 2 and 3 (7B–70B) show competitive results with 3–4 bit compression, and the authors further provide a SystemVerilog hardware implementation showing about 4× inference speedup."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1.  **Interesting hardware design**: The use of the explained-energy ratio to automatically decide how many bases each weight block needs is elegant. It avoids wasting capacity on easy blocks while keeping difficult ones accurate. The visualization in Fig. 2 nicely illustrates this behavior.\n2. **Well-thought system design**:\nFraming the compression setup (block size, seed length, threshold, etc.) as a multi-objective problem and solving it via Bayesian optimization is a solid, principled choice. It gives the method a data-driven way to balance accuracy and storage.\n3. **Solid Experiments**:\nThe paper tests on both LLaMA-2 and LLaMA-3 (7B–70B) models with many benchmarks. It’s good to see comparisons against strong baselines (SeedLM, AWQ, OmniQuant, QuIP#) under consistent settings and without fine-tuning or calibration."}, "weaknesses": {"value": "**1. Too hardware-oriented for ICLR:**\nThe main contributions are focused on implementation efficiency and hardware acceleration. While the work is technically impressive, it feels more suitable for a systems or hardware-focused venue (e.g., MLSys, DAC, ISCA) than for ICLR. \n\n**2. Should add clearer Ablation Study:**\nThe paper would be stronger with clearer ablations isolating the effects of the adaptive basis selection, the explained-energy ratio threshold, and the Bayesian optimization step. Without them, it’s difficult to assess which components matter most.\n\n**3. Restricted evaluation scope:**\nAlthough the paper includes an extensive set of experiments across multiple LLaMA model sizes (7B–70B) and tasks, all evaluations are confined to the LLaMA family. It remains unclear how well DC-LLM generalizes to other architectures or modalities.\n\n**4. Writing should be polished:**\nThe paper tries to cover both algorithmic design and hardware implementation, and as a result the main story feels a bit diluted. It’s clear what the authors have achieved, but it takes some effort for the reader to connect all parts together."}, "questions": {"value": "1. Could you add an ablation or some sensitivity analysis on the number of bases or seed length?\n\n2. Do you have any theoretical or empirical insight into why LFSR-generated bases work well for real model weights?\n\n3. How much computation overhead does the reconstruction add in practice, compared to standard quantization?\n\n4. Since the main contribution and validation are heavily hardware-oriented, have you considered submitting or extending this work for a systems or hardware-focused venue (e.g., MLSys, DAC, or ISCA), where it might reach a more appropriate audience and receive stronger technical feedback?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "I9udN31ZDr", "forum": "3YUL5LJ3s4", "replyto": "3YUL5LJ3s4", "signatures": ["ICLR.cc/2026/Conference/Submission5051/Reviewer_oqtn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5051/Reviewer_oqtn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762290102267, "cdate": 1762290102267, "tmdate": 1763009122034, "mdate": 1763009122034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}