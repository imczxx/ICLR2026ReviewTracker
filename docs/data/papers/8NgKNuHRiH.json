{"id": "8NgKNuHRiH", "number": 14792, "cdate": 1758243817721, "mdate": 1763698737532, "content": {"title": "CooperTrim: Adaptive Data Selection for Uncertainty-Aware Cooperative Perception", "abstract": "Cooperative perception enables autonomous agents to share encoded representations over wireless communication to enhance each other’s live situational awareness. However, the tension between the limited communication bandwidth and the rich sensor information\nhinders its practical deployment. Recent studies have explored selection strategies that share only a subset of features per frame while striving to keep the performance on par. Nevertheless, the bandwidth requirement still stresses current wireless technologies. To\nfundamentally ease the tension, we take a proactive approach, exploiting the temporal continuity to identify features that capture environment dynamics, while avoiding repetitive and redundant transmission of static information. By incorporating temporal awareness,\nagents are empowered to dynamically adapt the sharing quantity according to environment complexity. We instantiate this intuition into an adaptive selection framework, COOPERTRIM, which introduces a novel conformal temporal uncertainty metric to gauge feature\nrelevance, and a data-driven mechanism to dynamically determine the sharing quantity. To evaluate COOPERTRIM, we take semantic segmentation as an example task. Across multiple open-source cooperative segmentation models, COOPERTRIM achieves up to 80.28% bandwidth reduction while maintaining a comparable accuracy. Relative to other selection strategies, COOPERTRIM also improves IoU by as much as 45.54% with up to 72% less bandwidth. Qualitative results show COOPERTRIM gracefully adapt to environmental dynamics, demonstrating the flexibility and paving the way towards real-world deployment.", "tldr": "This paper introduces CooperTrim, a temporal uncertainty-guided adaptive feature selection mechanism to balance network overhead and task performance in Cooperative Perception..", "keywords": ["Cooperative Perception", "Intermediate Fusion", "Uncertainty Estimation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b4a118041c7d8a244df5155e539426799a32aba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "# What the paper does\nCOOPERTRIM is an adaptive data-selection framework for cooperative perception that uses temporal uncertainty to decide what features to share and how much to share under bandwidth limits.\n# Key idea\n* Compute a conformal, temporal uncertainty signal by comparing current encoded features (F_t) with the previous fused features (F_{t-1}^{\\text{fused}}); uncertainty indicates where collaboration helps most.\n* Use an uncertainty-guided attention to score relevance per channel/region and apply adaptive thresholds to (i) select features and (ii) determine sharing quantity frame-by-frame.\n* Train with an ε-greedy–inspired regimen to balance exploration/exploitation so the model learns robust selection under bandwidth constraints."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The temporally driven, uncertainty-aware communication scheme is conceptually clear and well structured: it measures discrepancies between the previous fused representation and the current features, applies conformal quantile thresholding to select candidates, and then uses attention with an adaptive mask cutoff to decide both what to transmit and how much—focusing bandwidth on high-value regions.\n2. The $\\epsilon$-greedy training schedule provides a practical stabilizer under bandwidth constraints: intermittent full-feature updates interleaved with predominantly masked updates smooth optimization and reduce variance, yielding stronger performance than standard-deviation–only uncertainty baselines and curriculum-style fine-tuning.\n3. The method is readily portable: as a drop-in component for cooperative semantic segmentation backbones (e.g., CoBEVT, AttFuse, DiscoNet), it delivers consistent improvements at equal or lower communication budgets."}, "weaknesses": {"value": "1.\tLack of comparison with asynchrony-robust methods (e.g., CoBEVFlow). While the task settings may differ, CoBEVFlow demonstrates that estimating BEV flow and propagating prior features can effectively counter temporal variation; this capability should be considered—either as a baseline or as a complementary design—when claiming advantages in time-varying scenes and realistic, asynchronous communications.\n2.\tSingle-benchmark evaluation. Experiments are confined to OPV2V, which limits external validity. Broader evidence across datasets (e.g., DAIR-V2X, V2X-Sim, OPV2V-Async) and tasks beyond semantic segmentation would strengthen generality claims.\n3.\t“Conformal” is used primarily as quantile gating rather than as standard conformal prediction with finite-sample coverage guarantees. The paper lacks formal coverage analyses or mismatch bounds, so the terminology risks overstating the method’s theoretical assurances.\n4.\tLimited system- and communication-layer characterization. Reported metrics focus on bandwidth ratios/Mbps and IoU, with no measurements of end-to-end latency, packet loss/retransmissions, congestion behavior, or the computation/runtime overhead introduced by attention and masking under different hardware budgets. Deployment-level compute-communication trade-offs thus remain underexplored."}, "questions": {"value": "1.\tBenchmark against asynchrony-robust methods and/or integrate BEV flow.\nCan you compare to CoBEVFlow or prepend a BEV-flow pre-alignment module, reporting IoU–bandwidth trade-offs under controlled time offsets (e.g., ±50/100/200 ms) on OPV2V/OPV2V-Async? Does your two-threshold policy still add gains beyond flow alone?\n2.\tReport system- and network-level metrics under realistic conditions.\nMeasure end-to-end latency (encode→select→transmit→align→fuse→decode), packet loss/retransmissions, and congestion behavior across link budgets (e.g., 3/6/12 Mbps) and loss rates (0–10%). Plot IoU–bandwidth–latency curves and characterize degradation/fallback under losses.\n3.\tQuantify computational overhead and deployment feasibility.\nDetail added FLOPs/memory and per-frame latency from attention/masking on embedded automotive hardware (e.g., Jetson/SoC) and desktop GPUs. Compare compute-communication trade-offs against feature-compression/distillation baselines at equal accuracy.\n4.\tEstablish cross-dataset and cross-task generalization.\nEvaluate beyond OPV2V (e.g., DAIR-V2X, V2X-Sim, OPV2V-Async) and beyond semantic segmentation (detection/occupancy/tracking). Include fine-tuned and zero-shot transfers, reporting full IoU–bandwidth curves to substantiate external validity.\n\nI would support acceptance provided the authors satisfactorily resolve all identified concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0yKFfD3MQ8", "forum": "8NgKNuHRiH", "replyto": "8NgKNuHRiH", "signatures": ["ICLR.cc/2026/Conference/Submission14792/Reviewer_Cjxj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14792/Reviewer_Cjxj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731599232, "cdate": 1761731599232, "tmdate": 1762925143177, "mdate": 1762925143177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "**General Response I:**\n\nWe thank all reviewers for the constructive feedback. We are encouraged by the positive assessments. We are addressing concerns shared across reviewers (S1-S3) in this general response, and specific concerns with more details in individual responses.\n\n**S1: Evaluation of CooperTrim on a task other than segmentation and on a dataset other than OPV2V. (Reviewers- All)**\n\n**Response:** We extend our evaluation of CooperTrim to 3D detection task and V2V4REAL dataset, to demonstrate the robustness and generalizability of our approach. We also clarify why semantic segmentation task evaluation was constrained to specific datasets. The following results have been attached below and added to the paper: ***Section (4) Experiments- “Trimming” Existing Cooperative Perception Baselines linked to Figure 2(d) and line 301***.\n\n**Limitations on Semantic Segmentation Across Datasets:** Beyond OPV2V dataset, unfortunately, several other prominent datasets in the vehicle-to-vehicle (V2V) and vehicle-to-everything (V2X) domains, such as DAIR-V2X, V2XSeq, and V2V4Real, do not include semantic segmentation groundtruth. Specifically:\nDAIR-V2X focuses primarily on detection tasks and lacks semantic segmentation annotations, making it unsuitable for evaluating segmentation performance.\nV2XSeq supports tracking and detection tasks but similarly does not provide semantic segmentation GT data.\nV2V4Real includes 3D vector maps and supports detection tasks, but it lacks semantic segmentation annotations. \nAdditionally, prior work such as CoBEVT has demonstrated segmentation performance on datasets like nuScenes and OPV2V. NuScenes is a single agent dataset and hence not considered for our work. \n\n**Extention to 3D Detection Task on both OPV2V and V2V4REAL:** To address concerns on CooperTrim compatibilityty and dataset diversity, we extended our evaluation to 3D detection task on both OPV2V and V2V4Real as shown below.  We compare CooperTrim against the baseline CoBEVT across two IoU thresholds (0.5 and 0.7).For now, we provide evaluation on a subset of data for detection. We expect the trends to be the same on the whole dataset and produce the final results by December 2. We also report the bandwidth reduction achieved by each method.\n\n3D Detection Results\n| Dataset   | CoBEVT       | CooperTrim   | CoBEVT      | CooperTrim | Bandwidth Use |\n|-----------|--------------|--------------|-------------|------------|---------------|\n|           | AP@IoU 0.5   | AP@IoU 0.5   | AP@IoU 0.7  | AP@IoU 0.7 | wrt CoBEVT    |\n|-----------|--------------|--------------|-------------|------------|---------------|\n| OPV2V     | 0.88         | 0.80         | 0.78        | 0.69       | 56.84%        |\n| V2V4Real  | 0.61         | 0.53         | 0.42        | 0.31       | 26.83%        |\n\n\n\nConsistent with the trends shown by the semantic segmentation task on OPV2V, the detection results show that CooperTrim achieves performance comparable to the baseline, while offering a significant average bandwidth reduction of 42.36%. These findings underscore the practical applicability of our method in real-world V2V scenarios in V2V4Real dataset, balancing performance and efficiency effectively in both - lidar based detection and camera based segmentation tasks. We hope this multi-sensor, multi-task and multi-dataset evaluation strengthens the validity of our approach and addresses the concerns."}}, "id": "NlRnmyOWp2", "forum": "8NgKNuHRiH", "replyto": "8NgKNuHRiH", "signatures": ["ICLR.cc/2026/Conference/Submission14792/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14792/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14792/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763694345981, "cdate": 1763694345981, "tmdate": 1763701995049, "mdate": 1763701995049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an adaptive data selection framework called COOPERTRIM for cooperative perception in autonomous agents. The main idea is to exploit the temporal continuity of the environment to identify relevant features and avoid transmitting redundant or static information. This reduces the communication bandwidth required while maintaining comparable accuracy to existing selection strategies. The proposed framework uses a conformal temporal uncertainty metric to measure feature relevance and a data-driven mechanism to determine the amount of data shared. The evaluation shows significant bandwidth reduction and improved IoU compared to other selection strategies. However, there are some limitations, such as no ablation study on choosing optimal thresholds, only one simulated dataset was used, and the method has not been evaluated on real-world datasets. Additionally, the threshold-based method may not be robust in scenarios where only minor changes occur in the scene. The method is currently only evaluated on segmentation tasks and further evaluation on detection tasks would demonstrate its generalizability. Finally, the impact of the computation cost after introducing this data selection to collaboration perception models needs to be evaluated. Overall, the idea of using temporal uncertainty for data selection is interesting and the theoretical proof is sound, but further research is needed to address the limitations mentioned above."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of data selection by using temporal uncertainty is interesting.\n2. The theoretical proof is sound.\n3. The reduction in communication bandwidth consumption in segmentation tasks is obvious."}, "weaknesses": {"value": "1. No ablation studies on how to choose the optimal thresholds.\n2. Only one simulated dataset is used; No real-world dataset is evaluated.\n3. What is the mathematical expression of the distance function? What is the deep reason for using this distance function?\n4. How robust is the threshold-based method? For example, in a certain scenario, maybe the overall scene doesn't change much, only a small object (e.g., a new pedestrian emerges), probably leading to a small temporal uncertainty, and how will the system take actions to this?\n5. Although the method is advantageous for segmentation tasks, it should be better evaluated on detection tasks as well to demonstrate its generalizability. \n6. Apart from the bandwidth consumption, one important factor is how the computation costs change after introducing this data selection to the collaboration perception models. What is the processing speed (evaluated by FPS)? Can this method be used in a real-time driving system (Typically more than 100 FPS)?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vV1BJOGuiq", "forum": "8NgKNuHRiH", "replyto": "8NgKNuHRiH", "signatures": ["ICLR.cc/2026/Conference/Submission14792/Reviewer_ypSd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14792/Reviewer_ypSd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792164514, "cdate": 1761792164514, "tmdate": 1762925142764, "mdate": 1762925142764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "**General Response II:**\n\n**S2: Evaluation of System Overhead of CooperTrim compared to Baselines. (Reviewers- ypSd Weakness 6 and Cjxj Question 2 and 3 )**\n\n**Response:** We thank Reviewers for the kind reminder on the computation costs and processing speed, for which we conduct the following evaluation and have updated it in ***Section 4.1 (System Overhead) linked to Table 3 and line 441*** of the paper. \nWe present the processing speed evaluated in Frames Per Second (FPS) for both the baseline CoBEVT and CooperTrim under static and dynamic settings. End-to-end latency per frame was measured across the entire pipeline (encode → select → transmit → align → fuse → decode) for both CooperTrim and the baseline CoBEVT under dynamic and static configurations. CooperTrim adds a minor latency of ~14ms per frame, reducing FPS from 10.69 to 8.67 in dynamic settings.\n\n| Method       | Configuration | FPS   | Time(ms) per Frame (end-to-end) |\n|--------------|---------------|-------|---------------------------------|\n| CoBEVT       | Dynamic       | 10.69 | 92.9                            |\n| CooperTrim   | Dynamic       | 8.67  | 94.7                            |\n| CoBEVT       | Static        | 10.59 | 93.1                            |\n| CooperTrim   | Static        | 8.58  | 119.5                           |\n\nThis indicates a modest reduction in processing speed due to the added computational steps of our method. However, this trade-off is justified by the significant bandwidth reduction achieved, which is the primary focus of our work. Regarding the suitability for real-time driving systems, CooperTrim is focused on perception rather than control frame rate. For reference, Waymo Open dataset which is collected with real-world vehicles has a frame rate of 10 FPS (e.g. LiDAR frame rate). \n\n**S3: Comparison of CooperTrim with Compression-based Methods (Reviewers- Yo96 Minor Weakness 2 and Cjxj Question 3 )**\n\nWe have added comparative results with network-efficient compression-based methods and updated ***Section (4) Experiments - Compression-based Method Comparison  linked to Figure 4 and line 317***, where we had compared CooperTrim with SwissCheese and Where2Comm, the closest related works in feature selection and communication efficiency. While other feature selection methods like Unisense exist, their source code is unavailable for direct comparison. Compression-based approaches, such as those used in CoBEVT and AttFuse, are compatible with CooperTrim. We show further bandwidth reduction using compression after feature selection. \nCooperTrim significantly outperforms compression-only methods after applying post-selection compression, while maintaining superior perception accuracy. Below are the detailed results for IoU performance and bandwidth usage, with CooperTrim achieving a bandwidth usage as low as 1.46% of total data in high-compression settings.\nTable 1: IoU Performance Comparison after Compression\n| Scenario   | CooperTrim IoU | CoBEVT IoU | AttFuse IoU |\n|------------|----------------|------------|-------------|\n| 1x Dyn     | 54.03          | 50.23      | 32.20       |\n| 1x Lane    | 24.45          | 23.79      | 24.14       |\n| 1x Road    | 44.38          | 45.28      | 34.86       |\n| 8x Dyn     | 53.99          | 50.19      | 32.21       |\n| 8x Lane    | 24.52          | 23.78      | 24.13       |\n| 8x Road    | 44.43          | 45.27      | 34.86       |\n| 32x Dyn    | 50.32          | 49.63      | 32.19       |\n| 32x Lane   | 24.62          | 23.77      | 24.14       |\n| 32x Road   | 45.05          | 45.27      | 34.86       |\n\n\nTable 2: Bandwidth Usage Comparison after Compression\n| Scenario   | CooperTrim BW | CoBEVT Compressed BW | AttFuse BW |\n|------------|---------------|----------------------|------------|\n| 1x Dyn     | 32.04%        | 100%                 | 100%       |\n| 1x Lane    | 22.77%        | 100%                 | 100%       |\n| 1x Road    | 22.77%        | 100%                 | 100%       |\n| 8x Dyn     | 2.67%         | 5.98%                | 14.3%      |\n| 8x Lane    | 9.25%         | 14.28%               | 17.42%     |\n| 8x Road    | 9.25%         | 14.28%               | 17.42%     |\n| 32x Dyn    | 1.46%         | 3.76%                | 10.62%     |\n| 32x Lane   | 6.06%         | 9.55%                | 13.71%     |\n| 32x Road   | 6.06%         | 9.55%                | 13.71%     |"}}, "id": "ponpnj4vef", "forum": "8NgKNuHRiH", "replyto": "8NgKNuHRiH", "signatures": ["ICLR.cc/2026/Conference/Submission14792/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14792/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14792/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763694647388, "cdate": 1763694647388, "tmdate": 1763700745668, "mdate": 1763700745668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CooperTrim, an adaptive, uncertainty-aware feature selection framework for cooperative perception. The framework leverages conformal prediction to estimate temporal uncertainty for feature relevance and employs a data-driven adaptive mechanism to select an appropriate quantity of shared features based on environmental complexity. CooperTrim is plugged in and extensively evaluated on semantic segmentation using several co-perception methods on the OPV2V dataset, demonstrating significant reductions in bandwidth usage without sacrificing accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Compelling Motivation and Scope:** The paper focuses on the bandwidth-accuracy trade-offs in cooperative perception, arguing for temporally and contextually adaptive feature selection that is not covered by static or threshold-based approaches. \n\n**Effectiveness of Sub-modules:** CooperTrim's integration of conformal temporal uncertainty estimation with a cross-attention-based selection mechanism is well described, addressing both feature relevance and adaptivity. Detailed elaboration on training strategies provides hints on reproduction.\n\n**Concrete Adaptivity Insights:** The claim of scene adaptivity is validated by the qualitative results given in Fig. 4."}, "weaknesses": {"value": "**Major Weaknesses:**\n1. Although more components are incorporated, the proposed method remains a threshold masking mechanism. The adaptivity claim needs more quantitative validation. For example, in Fig. 4 (left), a convincing result would be to show that the IoU curve is relatively stable, or at least more stable than the BW curve (\"complexity\" curve). The current result cannot prove that the adaptivity benefits the final results.\n2. Robustness against localization error and latency is not discussed.\n\n**Minor Weaknesses:**\n1. The method is only tested on OPV2V, which is a relatively simple simulated dataset.\n2. More network-efficient baselines are expected to be included. Presenting results for the object detection task can also help enhance comparisons with previous methods."}, "questions": {"value": "1. Can the method adjust the bandwidth requirement during operation? Or a new model (a newly learned threshold generator) is needed to cope with a new network condition?\n2. There will be a potential delayed response to a new traffic pattern as history information is used to generate the threshold. Are there any results on the influence of the temporal window size?\n3. How well would CooperTrim's adaptivity generalize to other perception tasks (detection, tracking, etc.)? In those tasks, the ROI is much sparser.\n\nSee the weaknesses section for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7WtiIXW9nh", "forum": "8NgKNuHRiH", "replyto": "8NgKNuHRiH", "signatures": ["ICLR.cc/2026/Conference/Submission14792/Reviewer_Yo96"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14792/Reviewer_Yo96"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964992213, "cdate": 1761964992213, "tmdate": 1762925142383, "mdate": 1762925142383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}