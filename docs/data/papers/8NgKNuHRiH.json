{"id": "8NgKNuHRiH", "number": 14792, "cdate": 1758243817721, "mdate": 1759897349143, "content": {"title": "CooperTrim: Adaptive Data Selection for Uncertainty-Aware Cooperative Perception", "abstract": "Cooperative perception enables autonomous agents to share encoded representations over wireless communication to enhance each other’s live situational awareness. However, the tension between the limited communication bandwidth and the rich sensor information\nhinders its practical deployment. Recent studies have explored selection strategies that share only a subset of features per frame while striving to keep the performance on par. Nevertheless, the bandwidth requirement still stresses current wireless technologies. To\nfundamentally ease the tension, we take a proactive approach, exploiting the temporal continuity to identify features that capture environment dynamics, while avoiding repetitive and redundant transmission of static information. By incorporating temporal awareness,\nagents are empowered to dynamically adapt the sharing quantity according to environment complexity. We instantiate this intuition into an adaptive selection framework, COOPERTRIM, which introduces a novel conformal temporal uncertainty metric to gauge feature\nrelevance, and a data-driven mechanism to dynamically determine the sharing quantity. To evaluate COOPERTRIM, we take semantic segmentation as an example task. Across multiple open-source cooperative segmentation models, COOPERTRIM achieves up to 80.28% bandwidth reduction while maintaining a comparable accuracy. Relative to other selection strategies, COOPERTRIM also improves IoU by as much as 45.54% with up to 72% less bandwidth. Qualitative results show COOPERTRIM gracefully adapt to environmental dynamics, demonstrating the flexibility and paving the way towards real-world deployment.", "tldr": "This paper introduces CooperTrim, a temporal uncertainty-guided adaptive feature selection mechanism to balance network overhead and task performance in Cooperative Perception..", "keywords": ["Cooperative Perception", "Intermediate Fusion", "Uncertainty Estimation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7fe93d389027bc5c72d6cdbc2a153ac5c0918bbd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "# What the paper does\nCOOPERTRIM is an adaptive data-selection framework for cooperative perception that uses temporal uncertainty to decide what features to share and how much to share under bandwidth limits.\n# Key idea\n* Compute a conformal, temporal uncertainty signal by comparing current encoded features (F_t) with the previous fused features (F_{t-1}^{\\text{fused}}); uncertainty indicates where collaboration helps most.\n* Use an uncertainty-guided attention to score relevance per channel/region and apply adaptive thresholds to (i) select features and (ii) determine sharing quantity frame-by-frame.\n* Train with an ε-greedy–inspired regimen to balance exploration/exploitation so the model learns robust selection under bandwidth constraints."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The temporally driven, uncertainty-aware communication scheme is conceptually clear and well structured: it measures discrepancies between the previous fused representation and the current features, applies conformal quantile thresholding to select candidates, and then uses attention with an adaptive mask cutoff to decide both what to transmit and how much—focusing bandwidth on high-value regions.\n2. The $\\epsilon$-greedy training schedule provides a practical stabilizer under bandwidth constraints: intermittent full-feature updates interleaved with predominantly masked updates smooth optimization and reduce variance, yielding stronger performance than standard-deviation–only uncertainty baselines and curriculum-style fine-tuning.\n3. The method is readily portable: as a drop-in component for cooperative semantic segmentation backbones (e.g., CoBEVT, AttFuse, DiscoNet), it delivers consistent improvements at equal or lower communication budgets."}, "weaknesses": {"value": "1.\tLack of comparison with asynchrony-robust methods (e.g., CoBEVFlow). While the task settings may differ, CoBEVFlow demonstrates that estimating BEV flow and propagating prior features can effectively counter temporal variation; this capability should be considered—either as a baseline or as a complementary design—when claiming advantages in time-varying scenes and realistic, asynchronous communications.\n2.\tSingle-benchmark evaluation. Experiments are confined to OPV2V, which limits external validity. Broader evidence across datasets (e.g., DAIR-V2X, V2X-Sim, OPV2V-Async) and tasks beyond semantic segmentation would strengthen generality claims.\n3.\t“Conformal” is used primarily as quantile gating rather than as standard conformal prediction with finite-sample coverage guarantees. The paper lacks formal coverage analyses or mismatch bounds, so the terminology risks overstating the method’s theoretical assurances.\n4.\tLimited system- and communication-layer characterization. Reported metrics focus on bandwidth ratios/Mbps and IoU, with no measurements of end-to-end latency, packet loss/retransmissions, congestion behavior, or the computation/runtime overhead introduced by attention and masking under different hardware budgets. Deployment-level compute-communication trade-offs thus remain underexplored."}, "questions": {"value": "1.\tBenchmark against asynchrony-robust methods and/or integrate BEV flow.\nCan you compare to CoBEVFlow or prepend a BEV-flow pre-alignment module, reporting IoU–bandwidth trade-offs under controlled time offsets (e.g., ±50/100/200 ms) on OPV2V/OPV2V-Async? Does your two-threshold policy still add gains beyond flow alone?\n2.\tReport system- and network-level metrics under realistic conditions.\nMeasure end-to-end latency (encode→select→transmit→align→fuse→decode), packet loss/retransmissions, and congestion behavior across link budgets (e.g., 3/6/12 Mbps) and loss rates (0–10%). Plot IoU–bandwidth–latency curves and characterize degradation/fallback under losses.\n3.\tQuantify computational overhead and deployment feasibility.\nDetail added FLOPs/memory and per-frame latency from attention/masking on embedded automotive hardware (e.g., Jetson/SoC) and desktop GPUs. Compare compute-communication trade-offs against feature-compression/distillation baselines at equal accuracy.\n4.\tEstablish cross-dataset and cross-task generalization.\nEvaluate beyond OPV2V (e.g., DAIR-V2X, V2X-Sim, OPV2V-Async) and beyond semantic segmentation (detection/occupancy/tracking). Include fine-tuned and zero-shot transfers, reporting full IoU–bandwidth curves to substantiate external validity.\n\nI would support acceptance provided the authors satisfactorily resolve all identified concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0yKFfD3MQ8", "forum": "8NgKNuHRiH", "replyto": "8NgKNuHRiH", "signatures": ["ICLR.cc/2026/Conference/Submission14792/Reviewer_Cjxj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14792/Reviewer_Cjxj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731599232, "cdate": 1761731599232, "tmdate": 1762925143177, "mdate": 1762925143177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an adaptive data selection framework called COOPERTRIM for cooperative perception in autonomous agents. The main idea is to exploit the temporal continuity of the environment to identify relevant features and avoid transmitting redundant or static information. This reduces the communication bandwidth required while maintaining comparable accuracy to existing selection strategies. The proposed framework uses a conformal temporal uncertainty metric to measure feature relevance and a data-driven mechanism to determine the amount of data shared. The evaluation shows significant bandwidth reduction and improved IoU compared to other selection strategies. However, there are some limitations, such as no ablation study on choosing optimal thresholds, only one simulated dataset was used, and the method has not been evaluated on real-world datasets. Additionally, the threshold-based method may not be robust in scenarios where only minor changes occur in the scene. The method is currently only evaluated on segmentation tasks and further evaluation on detection tasks would demonstrate its generalizability. Finally, the impact of the computation cost after introducing this data selection to collaboration perception models needs to be evaluated. Overall, the idea of using temporal uncertainty for data selection is interesting and the theoretical proof is sound, but further research is needed to address the limitations mentioned above."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of data selection by using temporal uncertainty is interesting.\n2. The theoretical proof is sound.\n3. The reduction in communication bandwidth consumption in segmentation tasks is obvious."}, "weaknesses": {"value": "1. No ablation studies on how to choose the optimal thresholds.\n2. Only one simulated dataset is used; No real-world dataset is evaluated.\n3. What is the mathematical expression of the distance function? What is the deep reason for using this distance function?\n4. How robust is the threshold-based method? For example, in a certain scenario, maybe the overall scene doesn't change much, only a small object (e.g., a new pedestrian emerges), probably leading to a small temporal uncertainty, and how will the system take actions to this?\n5. Although the method is advantageous for segmentation tasks, it should be better evaluated on detection tasks as well to demonstrate its generalizability. \n6. Apart from the bandwidth consumption, one important factor is how the computation costs change after introducing this data selection to the collaboration perception models. What is the processing speed (evaluated by FPS)? Can this method be used in a real-time driving system (Typically more than 100 FPS)?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vV1BJOGuiq", "forum": "8NgKNuHRiH", "replyto": "8NgKNuHRiH", "signatures": ["ICLR.cc/2026/Conference/Submission14792/Reviewer_ypSd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14792/Reviewer_ypSd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792164514, "cdate": 1761792164514, "tmdate": 1762925142764, "mdate": 1762925142764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CooperTrim, an adaptive, uncertainty-aware feature selection framework for cooperative perception. The framework leverages conformal prediction to estimate temporal uncertainty for feature relevance and employs a data-driven adaptive mechanism to select an appropriate quantity of shared features based on environmental complexity. CooperTrim is plugged in and extensively evaluated on semantic segmentation using several co-perception methods on the OPV2V dataset, demonstrating significant reductions in bandwidth usage without sacrificing accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Compelling Motivation and Scope:** The paper focuses on the bandwidth-accuracy trade-offs in cooperative perception, arguing for temporally and contextually adaptive feature selection that is not covered by static or threshold-based approaches. \n\n**Effectiveness of Sub-modules:** CooperTrim's integration of conformal temporal uncertainty estimation with a cross-attention-based selection mechanism is well described, addressing both feature relevance and adaptivity. Detailed elaboration on training strategies provides hints on reproduction.\n\n**Concrete Adaptivity Insights:** The claim of scene adaptivity is validated by the qualitative results given in Fig. 4."}, "weaknesses": {"value": "**Major Weaknesses:**\n1. Although more components are incorporated, the proposed method remains a threshold masking mechanism. The adaptivity claim needs more quantitative validation. For example, in Fig. 4 (left), a convincing result would be to show that the IoU curve is relatively stable, or at least more stable than the BW curve (\"complexity\" curve). The current result cannot prove that the adaptivity benefits the final results.\n2. Robustness against localization error and latency is not discussed.\n\n**Minor Weaknesses:**\n1. The method is only tested on OPV2V, which is a relatively simple simulated dataset.\n2. More network-efficient baselines are expected to be included. Presenting results for the object detection task can also help enhance comparisons with previous methods."}, "questions": {"value": "1. Can the method adjust the bandwidth requirement during operation? Or a new model (a newly learned threshold generator) is needed to cope with a new network condition?\n2. There will be a potential delayed response to a new traffic pattern as history information is used to generate the threshold. Are there any results on the influence of the temporal window size?\n3. How well would CooperTrim's adaptivity generalize to other perception tasks (detection, tracking, etc.)? In those tasks, the ROI is much sparser.\n\nSee the weaknesses section for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7WtiIXW9nh", "forum": "8NgKNuHRiH", "replyto": "8NgKNuHRiH", "signatures": ["ICLR.cc/2026/Conference/Submission14792/Reviewer_Yo96"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14792/Reviewer_Yo96"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964992213, "cdate": 1761964992213, "tmdate": 1762925142383, "mdate": 1762925142383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}