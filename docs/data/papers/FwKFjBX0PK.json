{"id": "FwKFjBX0PK", "number": 275, "cdate": 1756733138818, "mdate": 1763267874055, "content": {"title": "Federated Graph-Level Clustering Network with Dual Knowledge Separation", "abstract": "Federated Graph-level Clustering (FGC) offers a promising framework for analyzing distributed graph data while ensuring privacy protection.\nHowever, existing methods fail to simultaneously consider knowledge heterogeneity across intra- and inter-client, and still attempt to share as much knowledge as possible, resulting in consensus failure in the server.\nTo solve these issues, we propose a novel **F**ederated **G**raph-level **C**lustering **N**etwork with **D**ual **K**nowledge **S**eparation (FGCN-DKS). \nThe core idea is to decouple differentiated subgraph patterns and optimize them separately on the client, and then leverage cluster-oriented patterns to guide personalized knowledge aggregation on the server.\nSpecifically, on the client, we separate personalized variant subgraphs and cluster-oriented invariant subgraphs for each graph. Then the former are retained locally for further refinement of the clustering process, while pattern digests are extracted from the latter for uploading to the server.\nOn the server, we calculate the relation of inter-cluster patterns to adaptively aggregate cluster-oriented prototypes and parameters. Finally, the server generates personalized guidance signals for each cluster of clients, which are then fed back to local clients to enhance overall clustering performance.\nExtensive experiments on multiple graph benchmark datasets have proven the superiority of the proposed FGCN-DKS over the SOTA methods.", "tldr": "", "keywords": ["Clustering", "Deep Graph Learning", "Unsupervised Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8cb6a680e70ad841cc8c86033c99b5e01c6a18bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method for federated graph-level clustering. Compared with existing federated graph-level clustering methods, this method solves the heterogeneity problem of multi-source data through dual knowledge separation. The article is clear in expression, motivation is clear, and the experimental design is complete."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The experimental setup is comprehensive, with comparisons made against both supervised and unsupervised methods. Ablation analysis demonstrates the superiority of the proposed approach.\n2. As the novel method for federated graph-level clustering, this approach significantly enhances performance by separating different subgraphs and utilizing cluster-guided prototype aggregation.\n3. Research on federated graph-level clustering remains scarce, and this study effectively fills that gap."}, "weaknesses": {"value": "1. The authors should further elaborated The non-IID setting in the appendix for additional clarification.\n2. The authors should include experiments on federated communication costs.\n3. Some minor errors should be further corrected, such as changing \"leverage\" to \"leverages.\""}, "questions": {"value": "1. The paper mentions using a graph kernel method to measure network heterogeneity. To facilitate understanding and replication, could the authors please specify which specific graph kernel or types of kernels they used? Also, could they explain the rationale for choosing this particular kernel function to quantify heterogeneity?\n2. The authors need to clarify how and to what extent label information is used in the unsupervised training phase and the downstream task evaluation phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rrWtemwohx", "forum": "FwKFjBX0PK", "replyto": "FwKFjBX0PK", "signatures": ["ICLR.cc/2026/Conference/Submission275/Reviewer_zKZU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission275/Reviewer_zKZU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761399551536, "cdate": 1761399551536, "tmdate": 1762915483303, "mdate": 1762915483303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenging issue of Federated Graph-level Clustering, where heterogeneity across clients, both intra- and inter-client, can cause difficulties in achieving a meaningful consensus on the server side. The authors propose a novel framework called FGCN-DKS. The core idea involves two levels of decoupling: 1) On the client side, each graph is split into an \"invariant\" subgraph and a \"variant\" subgraph. Only the digest of the invariant part is shared. 2) On the server side, a Common Knowledge Sharing Strategy is employed to perform a personalized aggregation of client models, guided by the similarity between clients' cluster pattern digests. A two-stage clustering process on the client leverages both shared and local representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1.The paper effectively identifies the key issue of heterogeneity in federated graph clustering, distinguishing it from the more common challenges seen in node-level federated learning.\n\n2.The comprehensive experiments across multiple datasets and non-IID settings lend strong support to the method's robustness and generalizability."}, "weaknesses": {"value": "1.The framework introduces multiple new components and hyperparameters, making it complex and difficult to analyze or tune effectively. It is unclear which specific component contributes most to the performance gains, making it challenging to pinpoint areas for improvement or fine-tuning.\n\n2.The many moving parts in the system make it difficult to attribute performance improvements to specific components. \n\n3.While the dual knowledge separation and two-stage clustering approach is novel, the system’s complexity may limit its practicality and ease of adoption. Simplifying some aspects could make it more accessible."}, "questions": {"value": "1.Given the complexity of the framework, could the authors provide a more detailed ablation study to isolate the contribution of each key component? \n\n2.Could the authors quantify this communication cost and compare it to standard federated learning approaches like FedAvg, which transmit model parameters?\n\n3.How can the \"quality\" of this separation be directly evaluated, beyond just its impact on downstream clustering accuracy? Is there a way to ensure that the invariant subgraphs capture truly common, cross-client patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gtpyM1gPFw", "forum": "FwKFjBX0PK", "replyto": "FwKFjBX0PK", "signatures": ["ICLR.cc/2026/Conference/Submission275/Reviewer_q64Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission275/Reviewer_q64Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761407069915, "cdate": 1761407069915, "tmdate": 1762915483183, "mdate": 1762915483183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenging issue of Federated Graph-level Clustering, where heterogeneity across clients, both intra- and inter-client, can cause difficulties in achieving a meaningful consensus on the server side. The authors propose a novel framework called FGCN-DKS. The core idea involves two levels of decoupling: 1) On the client side, each graph is split into an \"invariant\" subgraph and a \"variant\" subgraph. Only the digest of the invariant part is shared. 2) On the server side, a Common Knowledge Sharing Strategy is employed to perform a personalized aggregation of client models, guided by the similarity between clients' cluster pattern digests. A two-stage clustering process on the client leverages both shared and local representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1.The paper effectively identifies the key issue of heterogeneity in federated graph clustering, distinguishing it from the more common challenges seen in node-level federated learning.\n\n2.The comprehensive experiments across multiple datasets and non-IID settings lend strong support to the method's robustness and generalizability."}, "weaknesses": {"value": "1.The framework introduces multiple new components and hyperparameters, making it complex and difficult to analyze or tune effectively. It is unclear which specific component contributes most to the performance gains, making it challenging to pinpoint areas for improvement or fine-tuning.\n\n2.The many moving parts in the system make it difficult to attribute performance improvements to specific components. \n\n3.While the dual knowledge separation and two-stage clustering approach is novel, the system’s complexity may limit its practicality and ease of adoption. Simplifying some aspects could make it more accessible."}, "questions": {"value": "1.Given the complexity of the framework, could the authors provide a more detailed ablation study to isolate the contribution of each key component? \n\n2.Could the authors quantify this communication cost and compare it to standard federated learning approaches like FedAvg, which transmit model parameters?\n\n3.How can the \"quality\" of this separation be directly evaluated, beyond just its impact on downstream clustering accuracy? Is there a way to ensure that the invariant subgraphs capture truly common, cross-client patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gtpyM1gPFw", "forum": "FwKFjBX0PK", "replyto": "FwKFjBX0PK", "signatures": ["ICLR.cc/2026/Conference/Submission275/Reviewer_q64Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission275/Reviewer_q64Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761407069915, "cdate": 1761407069915, "tmdate": 1763293058429, "mdate": 1763293058429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenging issue of Federated Graph-level Clustering, where heterogeneity across clients, both intra- and inter-client, can cause difficulties in achieving a meaningful consensus on the server side. The authors propose a novel framework called FGCN-DKS. The core idea involves two levels of decoupling: 1) On the client side, each graph is split into an \"invariant\" subgraph and a \"variant\" subgraph. Only the digest of the invariant part is shared. 2) On the server side, a Common Knowledge Sharing Strategy is employed to perform a personalized aggregation of client models, guided by the similarity between clients' cluster pattern digests. A two-stage clustering process on the client leverages both shared and local representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper effectively identifies the key issue of heterogeneity in federated graph clustering, distinguishing it from the more common challenges seen in node-level federated learning.\n\n2.The comprehensive experiments across multiple datasets and non-IID settings lend strong support to the method's robustness and generalizability."}, "weaknesses": {"value": "1.The framework introduces multiple new components and hyperparameters, making it complex and difficult to analyze or tune effectively. It is unclear which specific component contributes most to the performance gains, making it challenging to pinpoint areas for improvement or fine-tuning.\n\n2.The many moving parts in the system make it difficult to attribute performance improvements to specific components. \n\n3.While the dual knowledge separation and two-stage clustering approach is novel, the system’s complexity may limit its practicality and ease of adoption. Simplifying some aspects could make it more accessible."}, "questions": {"value": "1.Given the complexity of the framework, could the authors provide a more detailed ablation study to isolate the contribution of each key component? \n\n2.Could the authors quantify this communication cost and compare it to standard federated learning approaches like FedAvg, which transmit model parameters?\n\n3.How can the \"quality\" of this separation be directly evaluated, beyond just its impact on downstream clustering accuracy? Is there a way to ensure that the invariant subgraphs capture truly common, cross-client patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gtpyM1gPFw", "forum": "FwKFjBX0PK", "replyto": "FwKFjBX0PK", "signatures": ["ICLR.cc/2026/Conference/Submission275/Reviewer_q64Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission275/Reviewer_q64Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761407069915, "cdate": 1761407069915, "tmdate": 1763653704510, "mdate": 1763653704510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces a novel framework for federated clustering, centered around the idea of decoupling knowledge both within and across clients to enhance clustering performance. This manuscript is well-written, logically structured, clearly articulated, and experimentally robust, addressing an innovative problem. The experiments demonstrate significant improvements compared to advanced methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed method is novel, as it introduces for the first time the dual decomposition of knowledge into cluster-oriented and client-oriented dimensions.\n2. The experimental setup is robust and reliable and thoroughly demonstrates the superiority of the proposed framework.\n3. The authors cleverly integrate invariant learning into the federated graph-level clustering framework, which I find very insightful. It suggests that a similar strategy could potentially be used to improve federated graph classification as well."}, "weaknesses": {"value": "1. The adaptation of other methods, such as federated graph learning and federated anomaly detection, to federated graph clustering should be further explained.\n2. Communication overhead experiments should be included to further validate the superiority of the method.\n3. The fonts in the architecture diagram are consistent. It is recommended to further unify them to New Times Roman or Microsoft YaHei."}, "questions": {"value": "1. The calculation of the heterogeneity of intra-client graphs is unclear. Is the author referring to the heterogeneity of all graphs within the client, or the heterogeneity of graphs within different clusters on the client?\n2. Does this method have the capability to address the issue of inconsistent numbers of clusters across different clients?\n3. Is this federated client configuration self-defined, or does it follow the setup used in prior work?\n4. What exactly does the prototype refer to? I hope the author can explain it further."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns detected."}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eFhJ2wIhX2", "forum": "FwKFjBX0PK", "replyto": "FwKFjBX0PK", "signatures": ["ICLR.cc/2026/Conference/Submission275/Reviewer_YqnG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission275/Reviewer_YqnG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926279915, "cdate": 1761926279915, "tmdate": 1762915483018, "mdate": 1762915483018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel federated graph-level clustering algorithm. The algorithm decouple knowledge at both the local and server levels, thereby improving the quality of global consensus. The motivation is clearly presented, the overall presentation is of high quality, and the experimental results demonstrate superior performance compared with existing methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1. Unlike traditional approaches that enhance consensus by increasing shared data, this framework adopts an alternative strategy by reducing the amount of data involved in consensus, which not only facilitates more efficient agreement but also contributes to better privacy protection.\n\nS2. The article demonstrates high quality in writing, presentation, and overall organization, and it is easy to follow and understand.\n\nS3. Compared with other methods, this approach is better suited for real-world federated scenarios. Theoretical analysis is further provided to demonstrate the convergence of the proposed method."}, "weaknesses": {"value": "W1. The convergence analysis should include the loss variation during training.\n\nW2. Performance comparisons should be performed across all clients in a non-iid setting to demonstrate that the federated approach can improve overall performance without sacrificing the performance of individual clients.\n\nW3. Comparing it to supervised methods seems less meaningful; replacing it with the experiments described above better demonstrates the advantages of the method."}, "questions": {"value": "Q1. The convergence analysis mentions the FedPKA method, but it is missing from the comparative experiments. Although this method was not published in a top-tier conference, could it be that the authors accidentally omitted it?\n\nQ2. Why was FedSage compared in the experiment? This method seems to be applicable to node-based tasks rather than graph-level tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v0md4JdWCV", "forum": "FwKFjBX0PK", "replyto": "FwKFjBX0PK", "signatures": ["ICLR.cc/2026/Conference/Submission275/Reviewer_5r3P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission275/Reviewer_5r3P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934496491, "cdate": 1761934496491, "tmdate": 1762915482876, "mdate": 1762915482876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}